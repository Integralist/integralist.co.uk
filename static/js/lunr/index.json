[{"title":"Home","href":"/","content":" Be patient kind and generous with your time Me "},{"title":"About","href":"/about","content":" Hello my name is Mark and I work BuzzFeedhttpswwwbuzzfeedcom as a Staff Software Engineer formerly Principal Engineer BBCNewshttpwwwbbccouknews keybaseiointegralisthttpskeybaseiointegralist Im a published author of five books three through Apress and two selfpublished via Leanpub Pro VimhttpwwwamazoncoukProVimMarkMcDonnelldp1484202511refsr11 Tmux TasterhttpwwwamazoncouktmuxTasterMarkMcDonnellebookdpB00OPTU7LYrefsr11 Quick Clojure Effective Functional Programminghttpwwwapresscom9781484229514 Programming in Clojurehttpsleanpubcomprogrammingclojure Python for Programmershttpsleanpubcompythonforprogrammers For more details on my work history please refer to my resumehttpwwwintegralistcoukresume "},{"title":"Resume","href":"/resume","content":" Hellohello Working Togetherworkingtogether Connectconnect Brief Historybriefhistory Impactimpact Talkstalks Interviewsinterviews Publishedpublished Popular articlespopulararticles OpenSourceopensource Tools Languages and Techtoolslanguagesandtech Summarysummary Hello Hello my name is Mark and I work BuzzFeedhttpswwwbuzzfeedcom as a Staff Software Engineer formerly Principal Engineer BBCNewshttpwwwbbccouknews Im also a published author with Apress and LeanPub Apress Quick Clojure Effective Functional Programminghttpwwwapresscom9781484229514 Pro Vimhttpwwwapresscom9781484202517 Tmux Tasterhttpwwwapresscomgbbook9781484207765 LeanPub Python for Programmershttpsleanpubcompythonforprogrammers Programming in Clojurehttpsleanpubcomprogrammingclojure Working Together as smart as developers are they are not always good at explaining things in a way that makes human sense not you you are an exception you are A I this feedback It came from someone I was mentoring who worked in Product Support at BuzzFeed She was interested in getting a better understanding of how software systems were designedarchitected and also how to know what types of questions she should ask when investigating technical incidents Her feedback also hints at something bigger which I strive for to help others to do their best work and to pushpromote the good work other engineers do especially those from either a diverse background or minority I care a lot about the people I work with and aim to build meaningful relationships with people across the company In doing so I hope to ensure that we collectively are able to work as a cohesive unit and thus provide great value to our users Connect You can find me online at the following locations integralistcoukhttpwwwintegralistcouk githubcomintegralisthttpsgithubcomintegralist twittercomintegralisthttpwwwtwittercomintegralist keybaseiointegralisthttpskeybaseiointegralist linkedincommarkmcdonnellhttpswwwlinkedincominmarkmcdonnell08800565 Brief History BuzzFeed June 2016 present The journey has just begun BUT the story so far is that I joined as a Senior Software Engineer as part of a new core UK dev team being pushed to take on some important new projects for the company Alongside that we were tasked with decommissioning a 10yr legacy Perl monolithic application stack over to various Python and Go services Ive worked primarily within BuzzFeeds OOINFRA group which sits somewhere inbetween traditional infrastructureoperation teams and engineering teams building user facing products Our motivations were to make the lives of our fellow engineers easier by building tools services and abstractions that enabled them to work more quickly and efficiently January 2018 I was promoted to Staff Software Engineer after helping to designarchitect develop and maintain some of BuzzFeeds key infrastructure and software CDN caching strategies routing behaviours and securityauthentication related concerns January 2019 I moved over to the Core Infrastructure team which is split up into multiple sub teams Im initially part of the Infra Edge team responsible for strengthening and securing our Edge The Edge is the surface area of our infrastructure that is in direct contact with the public internet It includes DNS and the various services that accept ingress traffic from the Big Bad Internet It is essential to our business that our Edge be a resilient and securityfocused layer of our infrastructure Each year I would also participate in the various working groups and mentoring programs and become part of the oncall rota and handle interactions with the Hackerone program Note Im a remote worker and currently my team is based in New York so good communication focus and work ethic is essential BBC Jan 2013 June 2016 I joined BBC Newshttpwwwbbccouknews as a clientsidemobile specialist within their Core News team Within the year I had moved into a senior engineering role The then Technical Lead for the BBC News Frameworks team requested I join them in order to help the organisation transition from its current platform over to one built upon the AWS platform I started in the Frameworks team building and designing backend architecture for different microservices hosted upon the AWS platform and we developed these services primarily using JRuby In October 2014 I was offered the role of Technical Lead Near the end of 2015 I decided to change roles to Principal Software Engineer as my previous role required more of my time to be spent in meetings and handling line manager duties whereas I wanted to focus my time more on helping my team solve technical problems Storm Creative Feb 2001 Dec 2012 I started working at the agency Storm Creativehttpwwwstormcreativecouk straight out of college I was always focused on learning and improving my skill set both technical and communication skills the latter helped me communicate better with both clients and other stakeholderscolleagues I progressed upwards through the organisation moving from initially being a clientside web developer this included doing animations utilising ActionScript 30 to becoming a serverside developer ASPNET PHP and Ruby then onto becoming a Technical Lead across all projects and finally becoming the Digital Media Manager responsible for my own team of four engineers and overseeing all aspects of our projects Impact Id like to share the various things Ive worked on over the years and the impactvalue this work has provided Note the large majority of my impact has been as a remote worker My hope is that the following list demonstrates how Ive successfully made a positive impact both as an IC and as a Team Lead while also being 100 remote 2019 What Documented specific definitions for SLI SLO SLAhttpsgistgithubcomb9aa8e225ade0f78fcb57e1852627785 Why To help people understand what these terms mean and why they are useful Impact Engaged more teams in defining SLAs for their services along with their own internal SLISLOs What Planned and designed project to migrate VCL logic from CDN layer to a new perimeter service Why Reduce the amount of custom routing and behavioural logic within our CDN which is a black box for most engineers Impact Enabled BuzzFeeds ability to failover to a different CDN provider thus avoiding vendor lockin and improving our CDN resilience What Implemented CLI command for authenticated purging of URLs in Python Why To enable BuzzFeed staff inc tech support to purge a given URL Impact Reduced the number of blessed users given direct access to our CDN control panel thus reducing the scope of potential user mistakes for a critical piece of our infrastructure What Locked down purging of individual CDN URLs to authentication by API key Why Avoid bad actors engineering a distributed attack of our cache nodes Impact Increased platform resilience to cache dropping exploits What Moved to Core Infra specifically the Infra Edge team Why To increase the security and resilience of our Edge Impact Everyone in Core Infra was very excited to have me onboard Seems my reputation preceded me 2018 What I replaced BuzzFeeds use of NGINX a very expensive commercial product that was being used as part of a critical piece of BuzzFeeds infrastructure with the opensource equivalent Why This was a hack week projectpdfshackweek2018nginxpdf It took one day to implement the changes one day to test and verify behaviours in a staging environment followed by a quick rollout to production Impact This saved the organization 60000 a year in licensing fees What Designed and coimplemented new authentication system built in Python on top of AWS Cognito Why Decommission our legacy authentication system which was tightly coupled to a 10yr monolithic Perl application Impact Enabled more services to offer authentication thus allowing more community driven features across our products What Built a Python package that wraps scrypt Why Provide a consistent interface when requiring a hashing function Impact Engineers unfamiliar with the particulars of various security protocols eg the various hashing mechanisms or the difference to encryption could safely utilise hashing without having to understand the implementation What Helped promote the benefits of Kim Scotts Radical Candor Marshall Rosenbergs Nonviolent Communication and Fred Kofmans Authentic Communication to various teams across BuzzFeed Why Effective clear and compassionate communication benefits everyone Impact Teams were becoming more productive as the confidence to give the most appropriate and direct feedback necessary to catch both interpersonal issues and team concerns happened much more quickly What Introduced Wednesday lunch videospresentations Why To motivate and inspire our development teams Impact People had fun listening to interesting topics not all tech related and having a source of conversation and discussion beyond the lunch hour and in some cases helped to inspire changes that fed back into the company What Designed and implemented a Python Tornado web handler decorator responsible for acquiringcachingrevalidating an asymmetrical public key Why To protect services from unauthenticated access the public key was used to sign JWTs provided by an authentication proxy we had built previously in Go Impact Helped engineers to quickly integrate with our custom built authentication service and provide a consistent experience across the platform What Codesigned and coimplemented a Go based reverse proxy acting as an authentication layer in front of BuzzFeed services Why Part of a plan to decommission our legacy authentication system Impact The use of JWTs helped to implement a stateless system for providing authenticated access to services thus making the system easier to reason about and enabled teams to decouple themselves from our legacy Perl stack 2017 What Implemented README validator in Python Why As part of BuzzFeeds Better Docs initiative of which I was a core member of its Working Group Impact This helped BuzzFeed to track the success of its new Doc Day event which supports staff across the org in reviewing and improving software documentation What Led the effort to document improve and educate others on the state of BuzzFeeds monitoring Why Our monitoring system was very noisy which made going oncall a much more stressful proposition Impact I also wrote a community blog postpostsmonitoringbestpractices sharing and explaining a lot of what we did along with sharing a template Runbookhttpsdocsgooglecomdocumentd1eaT9KMam5zq7lT5OVz9T91RJQUxqx2q6WnKSvxCUedituspsharing for operational safety What Core member of the BuzzFeed BetterDocs Working Group Why We aim to improve documentation and its discoverability for BuzzFeed Tech Impact We standardized the majority of doc formats the creation and maintenance of doc tooling and continued to educate ourselves and the BF Tech community about the importance of good documentation What Tech Lead for the Site Infra Resilience team Why Necessary to help improve the stablity and resilience of BuzzFeeds existing services while helping to educate development teams on the various best practices Impact We designed a disaster recovery strategy specific for BuzzFeeds needs called Plan Z which helped to facilitate multiple failure scenarios and failovers for many of our service providers alongside that primary task we helped improve the resilience for many BuzzFeed services What Built an operations Slackbot in Go Why This was implemented as part of BuzzFeeds Hack Week but was quickly put into general rotation as is still used for all service incidents Impact Enabled all BuzzFeed staff whether technical or not to quickly spin up either a public or private incident channel in Slack while allowing interested parties to be autoinvited based upon an emoji reaction implementation The tool also allowed people to search for operational runbooks stored within our organizations Google Drive What Designed and implemented a roundrobin multicloud provider nginx solution for serving static assets Why To help provide greater resilience when serving clientside static assets such as images or scripts Impact The tooling we built around this implementation helped to make the process of deploying and serving static assets efficiently much easier What Technical Lead and architect for a dynamic video player service Why To enable asynchronous editor workflows Impact Enabled flexible video selection for end users while helping to promote BuzzFeeds own brand of video content outside of YouTube which would otherwise require us to lose potential profit What Designed and implemented a Go CLI tool for deploying Fastly VCL changeshttpsgithubcomintegralistgofastlycli Why The existing process for deploying Fastly VCL was manual and time consuming and prone to mistakes Impact Helped unblock engineers who needed a more efficient way to rollout changes while allowing them to diff and validate changes locally without having to signin to Fastlys otherwise confusing UI What Refactored existing HTTP Cache Client Python package Why Original design was a facade around a multitiered cache abstraction over a Python HTTP client This proved to be too limiting for engineers Impact Utilized an Adapter pattern internally in order to provide a unified interface thus making it easier for various HTTP clients to be provided instead of locking the caller down to a single client type What Implemented GitHub hook mechanism for detecting API changes and generating updated documentation Why Documentation would often go stale because engineers would make changes but not rerun the rendering tools to generate new docs Impact Enabled engineers to make changes without having to think about generating new documentation or having to know how to use the various tools for generating documentation What Refactored legacy VCL code and spent time building necessary abstractions Why Original code was difficult to understand and meant only a blessed few engineers understood how it all worked Impact Opened up the CDN to more engineers and helped to provide abstractions such as for logging to make working with VCL easier for those new to the language 2016 What Migrated Fastlys version of varnishvcl 2x to standard 41 Why Support switching to an alternative backup CDN Impact Strengthened our relationship with Site Reliability while also building confidence in a failover CDN What Designed and implemented generic GitHub Pull Request template file Why Consistency and standardization of how pull requests are structured The final format was based loosely on an old blog postpostsgithubpullrequestformatting I wrote back before GitHub offered their template feature Impact Clearer problemsolution descriptions that enabled engineers not familiar with the services to understand the changes being proposed What Implemented a smoke test scheduler service in Python Why Catch regressions with BuzzFeeds primary routing service Impact Helped engineers to identify integration problems where routing changes would have adverse unexpected effects What Led development across a mostly US based team and the rollout of a new critical routing service Why The routing behaviours for BuzzFeed were locked down to those blessed few who understood the CDN and VCL Impact Enabled the entire engineering department to make routing changes based on complex sets of dynamic input and requirements via a simple config driven workflow What Porting of Perl services over to Python BFFhttpsamnewmaniopatternsarchitecturalbff services Why Decommission of 10yr monolithic Perl application Impact Increased BuzzFeeds recruitment opportunities by expanding the amount of services written in Python compared to hiring Perl developers as well as improving the code quality of those services migrated What Proposed usage of specific Python linters and related tooling Why Code consistency and easier debugging of code Impact Improved overall code quality What Defined The Perfect Developer QualitieshttpsgistgithubcomIntegralist3f8089345a1236b374a7a5b8a13591a1 Why To inspire and motivate my colleagues Impact Engineers from across the organization reached out to me to share their thoughts feedback and general appreciation for the time and consideration as well as the obvious past experience that led to this ideal list of character traits What Released the opensource project goelasticachehttpsgithubcomIntegralistgoelasticache Why Share useful tools that would benefit others Impact Improved the developer experience when working with AWSs ElastiCache service What Led performance testing analysis and resolution of scaling issues for the BBCs internal Mozart platform written in Ruby Why Network bottlenecks were causing issues during load testing Impact Helped to identify specific service within the overall architecture that resulted in it being rewritten in Go and thus resolving the scaling performance issues What Implemented simple yet performant URL monitoring system in Bash called Bash Watchtowerpostsbashwatchtower Why Previous version was a complicated and over engineered Node application it was a colleagues pet project and no one in the organization used Node at the time It was also laden with NPM packages which made installing and running a very slow process Impact Improved deployment speed helped other engineers understand the code base by using a language they were more familiar with and simplified the overall code What Created and led BBC News Coding and Architecture working group Why We were charged with ensuring best practices were adhered to Impact Improved the overall quality of new services being developed and helped us to communicate with a wider range of the organization What Codesigned and coimplemented the BBC News Mozart platform Why Simplify the ability to build up dynamic page composition Impact Enabled teams to more easily build up complex pages of individual components It also helped path the way for the organization to move away from internal hosted system to the AWS platform while enabling developers to utilize easier languages and tools 2015 What Represented BBC at AWS week long reInvent technical conference in Las Vegas Why To learn more about the new AWS services that could benefit the organization Impact Networking with lots of different companies and helping to promote the work that the BBC does specifically the engineering arm of the organization What Codesigned and coimplemented a Go based CLI tool called Apollo Why Abstract away certificate based authentication to internal APIs Impact Enabled teams to more easily deploy services to the AWS platform What Team Lead for BBC News Frameworks team Why To help my team grow and to learn Impact Helped to promote a large segment of my team into senior position roles What Won Connecting the News Hack Day event Why Event for different news organizations to come together around a shared data source provided by the BBC and to see what interesting tools and services can enhance that data Impact Networking with engineering teams across different news platforms helped to inform potential ideas for our own services Showcased BBC News as a great place to work What Released BBC Newsbeat v2 Why First fully AWS product from BBC News Impact Started the movement of services from using an internal hosting platform onto the AWS platform What Tech Lead for General Elections Why The General Elections was a big event for BBC News Impact Successful build deploy and monitoring of election reporting platform What Rebuilt and migrated BBCs Market Data to AWS using the BBCs opensource Alephant framework of which I was a coauthor Why Fix an old and unmaintained yet critical data service Impact Modernized and improved this essential financial market service for its stakeholders and enabled further extension by other engineering teams 2014 What Designed and implemented Jello which was an internal synchronization service between Trello and Jira Why Teams preferred to use Trello while the rest of the organization was using a very old version of Jira Impact Enabled teams to benefit from the speed and feature set of Trello without having to manually track tasks back into Jira for the rest of the organizations visibility What Won Most innovative use of Technology BBC News Award Docker CI Why Legacy Jenkins CI was locked down to centralized operations team Impact Enabled teams to build and deploy software using any langage or platform supported by Docker What Won Best Public Relations of the Year BBC News Award Pro Vim Why I like writing and sharing information that helps people be more proficient with the tools they use Impact Book was well received and opened the Vim editor to wider range of engineers What Codesigned and coimplemented cloud based distributed load testing tool Why Existing solutions werent able to scale with our platform Impact Enabled engineers to easily load test their services at scale and identify performance bottlenecks What Organized public speaking event with Sandi Metzhttpwwwsandimetzcom Why To build an engineering network event for the London tech community Impact London tech community got to see an otherwise often unseen internal look at BBC engineering talent and were able to discuss topics of interest 2013 What Voted Developer of the Year at the BBC News awards Why I had made sure to reach out and affect in a positive way every single aspect of the business and to make a real difference to the developer community within the BBC Impact A genuine sense of pride that I was able to achieve what I set out to do make a difference What Led development of the BBC News responsive navigation redesign Why Part of the new BBC UX rebranding Impact Resulted in communication with product design and engineering teams across the entire breadth of the BBC platform Leading to a new responsive nagivation that was able to successfully accommodate all perspectives and requirements What Invited to speak at Mozilla offices in Parishttpsspeakerdeckcomintegralistbbcnewsresponsiveimages Why To discuss the BBC News responsive images technique to browser vendors such as Apple Microsoft Opera Mozilla and Google Impact I was able to establish myself as a person of interest to this organizations and an expert in the field when it came to clientside development What Implemented new BBC UX framework Why The BBC brand was undergoing a organization wide redesign Impact This was a very long and deliberate implementation and rollout process that helps reestablish BBC News as a leader in the responsive mobile development space and helped showcase BBC News engineering talents What Implemented new BBC responsive images solutionhttpsgithubcomBBCNewsImagerjs Why Scalable and responsive images was not widely supported by browsers with native APIs meaning custom solutions needed to be implemented Impact Public BBC News posthttpresponsivenewscoukpost58244240772imagerjs proposed our solution to the then difficult problem of how best to serve images in a scalable way to browsers and mobile devices What Introduced the use of GruntJShttpgruntjscom Why Ruby and Rake was being used although majority of engineers were unfamiliar with the language and were afraid to make changes or to build new tasks Impact Improved the ability of engineers to automate project tasks using JavaScript What Member of the BBCs GEL Responsive Working Grouphttpwwwbbccoukgel Why To help ensure engineers perspective on how best to implement new UX designs were accounted for Impact Simplified specific aspects of GELs design Talks Site Router Videohttpswwwyoutubecomwatchvmd4de3RyN8 80 minute presentation on BuzzFeed HTTP routing service abstraction BBC Talks Slideshttpsslidescommarkmcdonnell various presentations I gave while at the BBC Imagerjs Slideshttpsspeakerdeckcomintegralistbbcnewsresponsiveimages Talk I gave at the Mozilla offices in Paris which included speakers from Google Apple Microsoft Adobe Opera W3C and Akamai Interviews InfoQ How BuzzFeed Migrated from a Perl Monolith to Go and Python Microserviceshttpswwwinfoqcomarticlesbuzzfeedmicroservicesmigration Published Im a print published and selfpublished author Im also a tech reviewer and am a published author for many popular online organisations youll find many technical articles on my own website as well Apress Pro Vimhttpwwwapresscom9781484202517 Nov 2014 Tmux Tasterhttpwwwapresscomgbbook9781484207765 Nov 2014 Quick Clojure Effective Functional Programminghttpwwwapresscom9781484229514 August 2017 Packt Tech Reviewer Grunt Cookbookhttpswwwpacktpubcomwebdevelopmentgruntcookbook May 2014 Tech Reviewer Troubleshooting Docker May 2015 LeanPub Programming in Clojurehttpsleanpubcomprogrammingclojure Jul 2015 Python for Programmershttpsleanpubcompythonforprogrammers Jun 2016 NET Magazine 8 ways to improve your grunt setuphttpwwwcreativebloqcomtutorial8waysimproveyourgruntset111413407 Nov 2014 PDFhttpsdldropboxusercontentcomu3687270NetMag2020Gruntpdf DalekJS vs CasperJShttpsdldropboxusercontentcomu3687270NetMag2020Dalek20vs20Casperpdf Nov 2013 Smashing Magazine My author pagehttpcodingsmashingmagazinecomauthormarkmcdonnell Building Software with Makehttpwwwsmashingmagazinecom201510buildingwebapplicationswithmake How To Build A CLI Tool With Nodejs And PhantomJShttpcodingsmashingmagazinecom20140212buildclitoolnodejsphantomjs How To Build A Ruby Gem With Bundler TDD Travis CI Coveralls Oh Myhttpswwwsmashingmagazinecom201404howtobuildarubygemwithbundlertestdrivendevelopmenttravisciandcoverallsohmy NetTuts My author pagehttptutspluscomauthorsmarkmacdonnell Testing Your Ruby Code With Guard RSpec Pry Part 1 RubyGuardRSpechttpcodetutspluscomtutorialstestingyourrubycodewithguardrspecprycms19974 Testing Your Ruby Code With Guard RSpec Pry Part 2 RSpecPryTravisCIhttpcodetutspluscomtutorialstestingyourrubycodewithguardrspecprypart2cms20290 BuzzFeed Tech I wrote a three part series on BuzzFeeds core HTTP routing service built upon NGINX called Scalable Request Handling An Odyssey Part 1httpstechbuzzfeedcomscalablerequesthandlinganodysseypart1d91a295af4d8 Part 2httpstechbuzzfeedcomscalablerequesthandlinganodysseypart2ad2433b2f6ed Part 3httpstechbuzzfeedcomscalablerequesthandlinganodysseypart3c29aac9c39a InfoQ Interview How BuzzFeed Migrated from a Perl Monolith to Go and Python Microserviceshttpswwwinfoqcomarticlesbuzzfeedmicroservicesmigration Popular articles The following links are to some of my more popular articles My main focus when writing is to take a complicated or confusing topic and attempt to distil it in order for the subject to be more easily understood Algorithmic Complexity in Pythonpostsalgorithmiccomplexityinpython 2019 Data Types and Data Structurespostsdatatypesanddatastructures 2019 Engineer to Managerpostsengineertomanager 2018 Interview Techniquespostsarchitectureinterview 2018 Post Mortemspostspostmortemtemplate 2018 Thinking about Interfaces in Gopostsgointerfaces 2018 Multigrain Servicespostsmultigrainservices 2018 Authentication with AWS Cognitopostscognito 2018 A guide to effective 11 meetingsposts11 2018 Project Management in Five Minutespostsprojectmanagementinfiveminutes 2018 Interview Topicspostsquestionswheninterviewing 2018 Hashing Encryption and Encodingpostshashingandencryption 2018 Computers 101 terminals kernels and shellspoststerminalshell 2018 Statistics and Graphs The Basicspostsstatisticbasics 2017 Observability and Monitoring Best Practicespostsmonitoringbestpractices 2017 Logging 101postslogging101 2017 Fastly Varnishpostsfastlyvarnish 2017 Profiling Gopostsprofilinggo 2017 Profiling Pythonpostsprofilingpython 2017 Bits Explained inc base numbers ips cidrs and morepostsbitsandbytes 2016 Terminal Debugging Utilitiespoststerminaldebuggingutilities 2016 Big O for Beginnerspostsbigoforbeginners 2016 Git Merge Strategiespostsgitmergestrategies 2016 HTTP2postshttp2 2015 Client Cert Authenticationpostsclientcertauthentication 2015 DNS 101postsdns101 2015 Security basics with GPG OpenSSH OpenSSL and Keybasepostssecuritybasics 2015 Setting up nginx with Dockerpostssettingupnginxwithdocker 2015 Building Software with Makepostsbuildingsystemswithmake 2015 Thread Safe Concurrencypoststhreadsafeconcurrency 2014 GitHub Workflowpostsgithubworkflow 2014 Understanding recursion in functional JavaScript programmingpostsfunctionalrecursivejavascriptprogramming 2014 Refactoring Techniquespostsrefactoringtechniques 2013 MVCP Model View Controller Presenterpostsmvcp 2013 Basic Shell Scriptingpostsbasicshellscripting 2013 ObjectOriented Design OODpostsobjectorienteddesign 2013 Git Tipspostsgittips 2012 JavaScript 101postsjavascript101 2012 OpenSource Note listed alphabetically ie not prioritized in any way BBC AlephanthttpsgithubcomBBCNewsalephant The Alephant framework is a collection of isolated Ruby gems which interconnect to offer powerful message passing functionality built up around the Broker pattern BBC ImagerjshttpsgithubcomBBCNewsImagerjs Responsive images while we wait for srcset to finish cooking Bash HeadershttpsgithubcomIntegralistBashHeaders CLI tool written in Bash script for sorting and filtering HTTP Response Headers DOMReadyhttpsgithubcomIntegralistDOMready Cross browser DOM ready function Go ElastiCachehttpsgithubcomIntegralistgoelasticache Thin abstraction over the Memcache client package gomemcachehttpsgithubcombradfitzgomemcache allowing it to support AWS ElastiCache cluster nodes Go Fastly CLIhttpsgithubcomIntegralistgofastlycli CLI tool built in Go for interacting with the Fastly API Go Find RoothttpsgithubcomIntegralistgofindroot Locate the root directory of a project using Git via the command line Go RequesterhttpsgithubcomIntegralistGoRequester HTTP service that accepts a collection of components fansout requests and returns aggregated content Go Reverse ProxyhttpsgithubcomIntegralistgoreverseproxy A configurationdriven reverse proxy written in Go no dependencies outside of the standard library Grunt BoilerplatehttpsgithubcomIntegralistGruntBoilerplate Original Grunt Boilerplate Image SliderhttpsgithubcomIntegralistHTML5ImageSliderGame HTML5 Canvas Game MVCPhttpsgithubcomIntegralistMVCP MVC Presenter pattern in Ruby SinderellahttpsgithubcomIntegralistSinderella Ruby gem for transforming data object for specified time frame Spurious Clojure AWS SDK HelperhttpsgithubcomIntegralistspuriousclojureawssdkhelper Helper for configuring the AWS SDK to use Spurioushttpsgithubcomspuriousiospurious SquirrelhttpsgithubcomIntegralistSquirrel PhantomJS script to automate Application Cache manifest file generation StarkhttpsgithubcomIntegralistStark Node Build Script for serving HTML components Tools Languages and Tech I dont profess mastery but Im adept with most of the below and I have an aptitude towards learning what I need to get the job done right AWS CloudFormation CSS Clojure Design PatternshttpsgithubcomIntegralistRubyDesignPatterns Docker Functional Programmingpostsfunctionalrecursivejavascriptprogramming Git Go HTML JRubyMRI Ruby JavaScript clientside Jenkins Jira Make Meta ProgramminghttpsgistgithubcomIntegralista29212a8eb10bc8154b7 Node NGINX PHP Python Refactoringpostsrefactoringtechniques Regular Expressions Sass Shell Scriptingpostsbasicshellscripting Terraform Tmux Trello Vagrant Varnish VCL Vim Summary I ideally want to get across two fundamental things about me 1 Im very passionate about programming and the openness of the web 2 I love getting the chance to learn and experience new things "},{"title":"Search","href":"/search","content":" var lunrIndex results documents function initLunr retrieve the index file getJSONjslunrindexjson donefunctionindex documents index lunrIndex lunrfunction thisrefhref thisfieldcontent thisfieldtitle boost 10 thisfieldtags boost 5 documentsforEachfunctiondoc try consolelogdochref thisadddoc catch e this failfunctionjqxhr textStatus error var err textStatus error consoleerrorError getting Lunr index file err function searchquery Find the item in our index corresponding to the lunr one to have more info Lunr result ref sectionpage1 score 02725657778206127 Our result titlePage1 hrefsectionpage1 return lunrIndexsearchquerymapfunctionresult return documentsfilterfunctionpage try consolelogpage return pagehref resultref catch e consolelogwhoops 0 function renderResultsresults if resultslength return show first ten results resultsslice0 10forEachfunctionresult var result resultappend href resulthref text resulttitle resultsappendresult function initUI results results searchkeyupfunction empty previous results resultsempty trigger search when at least two chars provided var query thisval if querylength 2 return var results searchquery renderResultsresults initLunr documentreadyfunction initUI "},{"title":"A guide to effective 1:1 meetings","tags":["leads","management","one-to-one"],"href":"/posts/1-1","content":" What is a 11 whatisa11 What is a report whatisareport How important is a 11 howimportantisa11 How long should a 11 behowlongshoulda11be When should 11 meetings happenwhenshould11meetingshappen What do we talk about in a 11 meetingwhatdowetalkaboutina11meeting What about the silent typeswhataboutthesilenttypes Meeting structuremeetingstructure Making it personalmakingitpersonal Generality can be misleadinggeneralitycanbemisleading Understandingunderstanding Soliciting Ideassolicitingideas Evolve your processevolveyourprocess Track progresstrackprogress Dealing with issuesdealingwithissues Thought leadersthoughtleaders Conclusionconclusion What is a 11 A 11 one to one is a meeting between a manager and a report What is a report A report is a member of staff of which the manager is responsible for Specifically the manager is responsible for that persons happiness at the company and their career progression How important is a 11 A managers 11 meetings are their most important meeting as the primary goal of this meeting is for the manager and the report to build a strong and trusted relationship If youre a line manager then you should always attend these meetings Never skip or reschedule a 11 as that has the potential to give a bad impression upon your report eg your report feels like theyre unimportant and not appreciated by the organisation As a manager your most important priority is your staff How long should a 11 be At least thirty minutes no less as there just isnt enough time to really get into whats on someones mind If you schedule a fifteen minute 11 then this again suggests you dont have time or care enough to fit your direct report into your schedule When should 11 meetings happen 11s should happen regularly Typically they are scheduled once a week although fortnightly ie once every two weeks is more practical for some people Just remember that if you have them weekly then that helps to keep them shorter in length increase the length of the 11 depending on how far apart theyre scheduled Note unless youre an experienced engineer of many years dont fall into the trap of thinking I dont need regular 11s Youll be surprised to discover what tidbits of feedback and shared experience can help you progress with both your practical and soft skills What do we talk about in a 11 meeting If youre the report then quite literally anything that is on your mind Got an issue with someone at work Talk about it Got problems at home with a new born baby Talk about it Got ideas for ways to improve company communication Talk about it Remember what you talk about doesnt always have to be centered around work If youre having trouble sleeping whether thats because you have a new born baby or youve hurt your back from being sat in an office chair all day then talk about it The reason being is that your line manager can help expedite any tasks that help to resolve these problems Whether it be chatting with HR and Facilitaties to get you a standing desk or a new chair to help your back or whether they can talk to the tech lead on your team to allow you some leeway as far as the tasks youre expected to work on Maybe youre interested in taking on some extra responsibilities yourself and want to do some mentoring or looking after an intern Your line manager can help investigate all these possibilities on your behalf They are quite literally there to help you These meetings should be a safe space for you to share how youre feeling and thats important because if youre not feeling great for whatever reason whether it be physical or emotional issues then thats ultimately going to have an affect on your performance If youre a line manager then a 11 is your opportunity to perform weekly preventive maintenance and to understand the overall health of your team If youre effective in these meetings then you and the company will be rewarded with good work from your teams and you wont have any sudden builtup frustrations turning into drama or someone resigning Unfortunately most 11s end up being some form of status or project update which isnt good because the focus of a 11 is for a report to communicate with their line manager about how their doing and not how the project is going thats what projectproduct managers are for What about the silent types Now not everyone is going to be overly talkative and so its the line managers responsibility to tease out the conversation The most common question a line manager will or should ask is How Are You But if the response is a nonchalant Not bad then the line manager might have to resort to some other options for kick starting a meaningful conversation Some examples could be 1 Mini performance review 2 A current disaster 3 How to improve X Remember we dont want the 11 to be a statusproject update Suggestions If you are the report and you have nothing to say or have zero issues with any thing happening in the company which to me would be doubtful then just be aware that the meeting if not handled properly could turn into a status update and thats ultimately a pointless use of everyones time So try and make an effort to care about whats going on in the organisation as there is usually always something that can be discussed If you are the line manager then use the above three points to help guide the conversation to a meaningful topic that benefits both yourself and your direct report A mini performance review is great because we should always be looking at keeping staff on track with their goals In order to do this we need to understand their motivations and what they want from life so start the discussion there and that will help you to understand how you can guide their progress Discussing a current disaster is a good way to get some feedback on a topic thats important to the company and allows you to get the perspective of someone either not involved in the problem space or who doesnt even work in the same area of the business This equally allows the direct report to feel included and that their opinions matter Lastly choosing an area of the business where you feel could use some improvement is also a good option for opening up the discussion and having a meaningful 11 and it results in similar benefits as mentioned in the previous comment Meeting structure Heres the approach Im currently taking in my 11 meetings Hows things in general Any wins this past week positivity boost Any updates from last week Any team or project concerns can I unblock Any projects youre interested in helping with Any broader company wide updates I can share Any career progression updates either from me or the report Any feedback on my performance so far make me more effective Anything you looking forward to can be work related or personal Finish by celebrating their hard work another positivity boost This structure can change depending on what I know going into the meeting as well as what arises from discussions within the meeting Generally I want to be sure to take advantage of any opportunity where I can help a report to learn from mistakes and to share my own experiences as a way of expressing how I believe certain interactionsincidents could have been improved Especially when it comes to soft skills and communication I also like to understand what a persons strong points are because this person might be able to have some greater impact on a project or team that they otherwise might not realize was an opportunity for them to be a part of I can help them progress by getting them onto projects they wouldnt normally consider or even know about or start helping them to prepare and to learn new relevant skills Making it personal I like to find a common interest with my report This normally happens organically but at the very least I like to have a basic understanding of what my report enjoys doing ie I take an interest in them and once I do that I feel more of a connection and more invested in helping them succeed Do they like to read books in their spare time Do they like rock climbing etc This helps me to keep the conversation light and informal when the meeting starts and I ask Hows things with you because if I dont get much output eg Im dealing with a quiet one then I feel like I can tease out some light conversation by asking about their interests Generality can be misleading If you ask how are things youve asked a very general question and that can sometimes be ok but it can also be too vague a question for your report to answer in a meaningful way So to tackle this I like to ask specific questions such as ok what happened in the past week that was Exciting Frustrating Rewarding Challenging Engaging Boring Notice I flipflop between positive and negative toned queries This helps me to tease out characteristics of the report and this helps me to then align their interests with whats potentially happening within the organization Understanding The following questions are useful for getting a clearer understanding of what will and not work with your report Remember like how a good teacher will adapt their lesson plan for different types of students some students have different learning styles you should adapt your 11 to best fit your report yes this takes effort but this is your responsibility and an important one at that Most of these questions are taken verbatim from Camille Fournierhttpstwittercomskamilles awesome book The Managers Pathhttpshoporeillycomproduct0636920056843do and is strongly recommended reading How do you like to be praised in public or in private What is your preferred method of communication for serious feedback Do you prefer to get such feedback in writing so you have time to digest it or are you comfortable with less formal verbal feedback Why did you decide to work here What are you excited about working here How do I know when youre in a bad mood or annoyed Are there things that always put you in a bad mood that I should be aware of eg does your report fast for a religious holiday which can make them a bit grumpy Are there any manager behaviours you hate Do you have any clear career goals that I should know about so I can help you achieve them Any surprises since you joined good or bad that I should know about eg where are my stock options you promised me a relocation bonus and Ive not received it Would you like more or less direction from me on your work What could I do to make you enjoy your work more What part of the day do you have the most energy and focus What changes could we make to accommodate this Soliciting Ideas Some questions can help the report feel like their voice is heard and that their opinions matter How could we change our team meetings to be more effective Do you feel your ideas are heard by the team and I Evolve your process You try things They fail Tweak them as necessary If my earlier suggestions arent working for you then obviously make sure youre comfortable with them and that you feel like youve given them your best effort but if theyre not working dont keep forging ahead Try a different approach Remember these meetings arent about you and your success as a manager They are about your report and their success Drop the ego and start thinking about the best interests of your reports and how you can leverage their best self to make the company as a whole better If Im taking over reporting duties from another manager I like to talk to the previous manager to understand what worked or didnt work in their opinion when they had 11s with this report I might ignore their suggestions or I might not but its good to dig into the previous managers experience with the report Track progress Some managers like to jot down details of their reports alongside their normal work notes If that works for you then fine but I personally feel that you should work with a medium that allows your notes and thoughts to be easily shareable with your report You can continue to write private thoughts separately of course but having a document thats shared with the report so they can see how theyre progressing and have a place to review previous conversations is a good way for them to feel these regular conversations are providing value Note this will also be useful to you either as a line manager or as a report when it comes time to givereceive six monthly performance reviews as youll have a complete track record of all discussions Its important for reports to understand that they own their career progression and so once youve had an opportunity to understand your report the next step is for the report to come up with their goals and to proactively work through the relevant steps that lead to the realization of those goals As a manager its your responsibility to help and support the report with this process Dealing with issues If youre a line manager then there are two types of issues 1 Mild 2 Serious The first is someone just venting frustrations the second is where theyve reached their limits and have exploded their anger all over you In both cases you should Say nothing for now Listen Do not attempt to problem solve yet Do not comfort or try to take away their experience Once they have had an opportunity to say what they need to then you can start to identify the various moving pieces of the problem although if theyre that angry you may find them repeating themselves and so at some point you might have to interject This could include discussing with them topics such as What the next steps are eg intermediary and long term resolutions How we can reach each each step as quickly and efficiently as possible How we might prevent this issue from happening again If youre the direct report and youre struggling with an issue to the point where youre now very angry then this is a good example of why its important to have frequent qualitative and effective 11s They allow a lot of issues to be caught ahead of time Thought leaders For anyone interested in learning more I would highly recommend following these key playershttpstwittercomintegralistlistsleadsmembers Patty McCordhttpstwittercompattymccord1 April Wenselhttpstwittercomaprilwensel Fred Kofmanhttpstwittercomfredkofman Camille Fournierhttpstwittercomskamille Kim Scotthttpstwittercomkimballscott Lara Hoganhttpstwittercomlarahogan Conclusion Hopefully youve found this breakdown of 11 meetings interesting and are able to bring some new learnings into your own 11s to make them more effective and successful Good luck out there "},{"title":"DNS 101","tags":["bash","caching","dns","records","ttl"],"href":"/posts/dns-101","content":" DNS Domain Name System1 Lookup Process2 Resolver3 Root Name Servers4 TLD Name Servers5 SLD Name Servers6 Tree Hierarchy7 DNS Structure8 Authority Zones9 Name Servers10 Commands11 whois12 dig13 Flags14 option nostats15 option nocomments16 option trace17 Other options18 host19 nslookup20 Record Types21 NS22 A23 CNAME24 MX25 SOA26 SRV27 PTR28 TXT29 TTL Time to Live30 Local Caching31 References32 Conclusion33 DNS Domain Name System You likely have many applications running on your laptop right now and chances are some of them require an external resource or piece of data that will be retrieved from across a network In order for your applications to access these resources they need to know their location The location is typically identified by an ip address and are written in either decimal or hexadecimal When dealing with ips they can be very hard to memorise especially as there are two different forms of ip 1 IPv4httpsenwikipediaorgwikiIPv4 2 IPv6httpsenwikipediaorgwikiIPv6 An IPv4 identifier is 32 bits long written in decimal and separated by periods 7412522472 An IPv6 identifier is 128 bits long written in hexadecimal and separated by colons 3ffe190045453200f8fffe2167cf Its even more awkward if you have a program with a hard coded ip address for a specific resource If the resource has to change its location then the program now has to be updated as well to point to that new location Whats easier would be the use of a name that maps to an underlying value So for example the ip address 7412522472 points to Googles search page Rather than type that ip into your web browser or hard code it inside an application you would likely instead find it much easier to reference the resource by its name googlecom Your application eg web browser whatever when encountering this resource will lookup the name provided resolve the underlying ip address and ultimately retrieve the relevant resource This is the basis of what is known as DNS Domain Name System Lookup Process In order for you to access a resource using a name you need to now know the location of the system that contains the information also known as records for the resource youre interested in The idea being is that if you knew which DNS held a record containing information about the resource you wanted you could query the DNS and say hey whats the ip for resource X the DNS would then respond with the information you needed This is whats known as the lookup process The lookup process is where your machine communicates across different networks until it finds one that knows the ip address of the resource youre looking for Now there are a few different parts to this lookup process Resolver Root Name Servers TLD Name Servers SLD Name Servers Resolver The resolver knows the location of the root name servers When you make a request for a DNS resource then the resolver is what you communicate with first The resolver is typically either assigned via DHCP clientside or via etcresolvconf serverside The resolver will have a root hints file with a hardcoded list of root name servers The resolver takes the DNS resource request and queries all the root name servers at once Whichever root name server responds first the resolver will keep a note of it as being the quickest so for future requests itll favour that server over the other root name servers Note resolvers also include additional logic for invalidating the favoured root name server but thats outside the scope of this article Now depending on what your query is the resolvers job isnt necessarily finished For some queries such as wwwexamplecom the resolver doesnt just send a request to one level of name servers The resolve first makes a request to the root name servers waits for a response and then sends another query this time the query is sent to a sub level name server known as a TLD name server The TLD name server also returns a referral this time to a SLD name server which the resolver will query The SLD should now be able to identify the requested DNS resource Root Name Servers There are multiple root name servers and each one knows the location of a sub level name server capable of handling the requested DNS resource or at least knowing the next level of name servers to delegate the request onto Effectively the root name servers parse the requested resource and identify which TLD name server the resolver should query Once the root name server has identified the sub level name server it returns that information to the resolver TLD Name Servers There are multiple TLD name servers and each one knows the location of sub level name servers capable of handling the requested DNS resource and identifying its underlying ip address The TLD name servers parse the requested resource and identify which SLD name server the resolver should query Once the TLD name server has identified the SLD name server it returns that information to the resolver Note TLD name servers are split into two gTLD com org net etc and ccTLD us uk etc SLD Name Servers SLD name servers are known by a few different names User DNS name server Authoritative name server The latter authoritative being the most well known because these name servers are the final step to hopefully discovering the identity of the requested DNS resource and this DNS server will be responsible for the DNS settings applied to the resource Note you may find some people use the term authoritative quite liberally because in essence even a TLD name server can be considered authoritative in respect to the TLDs it manages depends on your perspective Tree Hierarchy It can be useful to try and visualise the hierarchy of this lookup process The following code snippet shows a potential lookup flowing from an initial request to a root name server into a TLD name server I show both sub categories gTLD and ccTLD just to be clearer another request is then made to a specific gTLD com in this case and from there the next step is for the resolver to query the SLD in this example were looking for the resource wwwexamplecom root gTLD ccTLD com examplecom The requests that would have been made to fulfil the above flow diagram would have looked something like Resolver makes multiple requests for wwwexamplecom to its list of root name servers One of the root name servers would have responded first to say go talk to the com gTLD The resolver then sends the same wwwexamplecom query to the com gTLD The gTLD responds to say go talk to the examplecom authoritative name server The resolver then sends the same wwwexamplecom query to the relevant authoritative name server The authoritative name server has the record the resolver is looking for and sends back its details The resolver can now provide those details back to the application who originated the request Note so far weve spoken about name servers that run DNS software There are many DNS software programs available some paid some free but the most popular is BINDhttpswwwiscorgdownloadsbind Well see references to it throughout this article DNS Structure Each period in a domain is actually a level Hence extensions such as com or us are considered a TLD toplevel domain This also explains why an SLD is named so because it sits at a secondary level it sits below the TLD The syntax structure of a domain resembles the following This would materialise into something like httpwwwexamplecom Where http is the protocol www is the hostname sld is example tld is com Note things get a little more complicated with ccTLDs As country codes can have a third level eg examplecouk uk TLD co SLD example 3LD Typically the combination of is referred to as the domain name Authority Zones Now that we understand that a domain is made up of levels the concept of zones should be easier to comprehend A level is equivalent to a zone but when using the term zone were being explicit in what context were talking about Effectively each zone is responsible ie an authority for its own resource records RR "},{"title":"GitHub Pull Request Formatting","tags":["pr","github","git"],"href":"/posts/github-pull-request-formatting","content":" Introduction1 Why2 Size3 Process4 Communication5 Merge Strategies6 Conclusion7 Introduction What makes a good Pull Request The answer will depend on your team and to a certain extent your organisation Currently I work at BuzzFeed and prior to that I worked at the BBC Both have a large number of engineers and teams but due to different organisational structures they have differing opinions on what constitutes a good pull request format These opinions arent happening at the organisational level either but are very much team specific Teams work differently and so have different needs Below I discuss some ideas around what Ive used in the past and what Im using today and Ill leave it as an exercise for the reader to determine what parts they decide to takeaway with them Why Probably the most important part of a pull request is understanding why the change is needed in the first place What exactly are you changing and is it even needed Are you solving a problem that promotes a real business requirement or are you just adding a nice feature that doesnt actually serve to improve the end users experience Maybe the feature youre adding is an internal improvement eg refactor dev tooling etc Thats fine and I guess it comes down to how your team prioritises its work But taking a moment to stop and think about a new code change before you start working on it is an important step to take Size Pull requests should be small A large pull request that touches many different files across a project and has many different side effects outcomes and responsibilities is extremely difficult to reason about from the perspective of the person reviewing the code Small pull requests allow for quicker reviews and merging It promotes an iterative approach to implementing new features It also helps to avoid conflicts when merging or rebasing Were also able to fail fast if our work priorities change and we need to switch gears or change direction altogether Were not trying to solve every problem all at once Process Pull requests should be opened almost immediately to allow for team feedback and help in direction depending on what you need For me this typically means making a single small change and opening a pull request around it Once the pull request is open I utilise labels to signify the state of the pull request wip work in progress rtr ready to review rtm ready to merge help I actively want engineers to chip in early and help me flesh out the design if they see something wrong I ensure I have a good pull request description that indicates what the problem is that the pull request solves and how it solves that problem In markdown that would look something like Problem Solution Notes Todo Screen Shots image alt texthttpsomeimage As you can see above this is a generic template I like to have the problem and solution lines to be very concise and not to take up multiple lines The notes section can be multiple lines so I keep it as an isolated section This section can also include things like automated code linting or test suite coverage results The todo section helps me to keep track of what tasks I have in order to complete this pull request but it also helps other engineers to understand my thought process and see where I might be going before I even get there allowing the team to ask questions or make suggestions as per the help label Finally the screen shots section is useful for those unfamiliar with the side effects of the change to be able to visually identify where the change appears or what it looks like This isnt always necessary depending on the code change being made but is useful for UI changes Communication Its important to notify fellow team members that your pull request exists in order for them to provide appropriate feedback You can manually people within a comment in the pull request or you could create a team in GitHub and then instead helps especially for teams who like to rotate members across other teams within an organisation Merge Strategies Ive written in the past about different git merge strategiespostsgitmergestrategieshtml My preferred way is to git merge squash and luckily GitHubs UI provides a one click way to squash merge a pull request Thats what I suggest using for merging pull requests into master but you may have different requirements in your teamorganisation so pick whatever works best Conclusion So this has been my preferred approach to creating a good pull request To recap here is a top level look at the structure and concepts I suggest Small pull request Utilise labels to indicate status Consistent formatting Gif optional ProblemSolution Notes optional Todos optional Screen shot images optional Agree a merge strategy for team consistency "},{"title":"GitHub Workflow","tags":["git","github","pr"],"href":"/posts/github-workflow","content":" Introduction1 Rebase before merge2 Example3 Bonus Modifying content within an interactive rebase4 Introduction This is a quick post to cover a GitHub workflow that is utilised by our specific team Frameworks here at BBC News The basis of our workflow is this Open a GitHub PR Pull Request by creating a new feature branch from master Make feature specific changes and request a code review If given a thumbs up this means the PR author is allowed to handle merging the PR The merge process requires a set of sub steps see below Rebase before merge At this point the PR author has been given a thumbs up and is preparing their PR to be merged back into the master branch The steps in summary are Interactively rebase master onto the feature branch Squash all feature commits into a single commit well see an example of this later as this isnt exactly a squash the first line of the commit message should be the same as the GitHub PR title As part of the rebase ensure the authors is mentioned in the commit message the PR is mentioned and any associated issues as well Move back to the master branch and cherrypick in the newly squashed feature commit Example Below is an example set of commits well be working from I create a master branch and then branch off from that with a new feature branch git init touch testtxt git commit am Initial file master rootcommit 85919e1 Initial file 1 file changed 0 insertions 0 deletions create mode 100644 testtxt git checkout b feature Switched to a new branch feature echo foo testtxt git commit am Foo echo bar testtxt git commit am Bar echo baz testtxt git commit am Baz Check commits we now have in this feature branch Note this is a custom shell alias git lg 62d4c80 Baz HEAD feature a5827db Bar ae1a4a5 Foo At this point lets imagine our feature PR has been approved to be merged Make sure master is up to date git checkout master git pull rebase origin master Carry out the interactive rebase git checkout feature git rebase i master Now at this point you should see something like the following in your terminal pick ae1a4a5 Foo pick a5827db Bar pick 62d4c80 Baz Rebase 85919e162d4c80 onto 85919e1 Commands p pick use commit r reword use commit but edit the commit message e edit use commit but stop for amending s squash use commit but meld into previous commit f fixup like squash but discard this commits log message x exec run command the rest of the line using shell These lines can be reordered they are executed from top to bottom If you remove a line here THAT COMMIT WILL BE LOST However if you remove everything the rebase will be aborted Note that empty commits are commented out Were now ready to modify our git history So lets squash all our commits down into a single commit reword ae1a4a5 Foo fixup a5827db Bar fixup 62d4c80 Baz Note we dont use squash as that automatically uses the existing commit message from the commit were squashing other commits into but our requirements mean we wish to modify that commit message so we use reword and fixup instead Lets apply these changes by executing wq Once you do this Git will carry out the rebase and then drop you back to the COMMITEDITMSG screen You can now modify the commit message so it is the same as the title of your GitHub PR and you can inform GitHub of what PR to automatically close when this commit arrives in master by using the keyword closeshttpshelpgithubcomarticlesclosingissuesviacommitmessages youll notice there is the keyword fixes which indicates a GitHub issue to close New Feature X Closes 1 and Fixes 11 Authors integralist stevenjack Please enter the commit message for your changes Lines starting with will be ignored and an empty message aborts the commit Date Sat Dec 27 161954 2014 0000 rebase in progress onto 85919e1 You are currently editing a commit while rebasing branch feature on 85919e1 Changes to be committed modified testtxt We can now see when executing git log that the three individual feature commits are now a single commit with a commit hash of 68f5bee We can move back to master ie git checkout master and then cherrypick the squashed commit into it git checkout master git cherrypick 68f5bee git branch D feature git push origin master Bonus Modifying content within an interactive rebase One aspect of carrying out an interactive rebase that seems to confuse a lot of users is the ability to edit the content of a specific commit I think the reason being is that when you select a commit to edit the interactive rebase drops you at that specific commit so youll find there arent any files in the staging area To make edits at that point you need to undo the commit so the files end up back in the staging area ready to be modified and a new commit made Lets take a look at an example using our earlier example git lg 76c99a2 New Feature X HEAD master 85919e1 Initial file Lets say we want to make an edit to the commit 76c99a2 To do that well need to start up an interactive rebase like so we specify the commit before the one we want to edit git rebase i 85919e1 Stopped at 76c99a27e64b5f749ac8e3d3c7032f53954c760a New Feature X You can amend the commit now with git commit amend Once you are satisfied with your changes run git rebase continue At this point we want to execute the following command which will undo the commit but keeps the changes from that commit and place the files back into the staging area git reset mixed 85919e1 Unstaged changes after reset M testtxt Now if you check the diff on the files youll see the changes from that commit have been made and are waiting to be committed again git diff diff git atesttxt btesttxt index e69de2986e041d 100644 "},{"title":"JavaScript 101","tags":["javascript"],"href":"/posts/javascript-101","content":" Introduction1 What is JavaScript2 Terminology3 Global Object4 Variables5 Types6 Objects7 Arrays8 Conditional Statements9 Coercion10 Functions11 Code Reuse inheritance12 Conclusion13 Introduction Ive written this very brief guide to JavaScript just as an aid for people new to the language and who need a basic starting point to see what the syntax looks like and to get a feeling for some of its features This articles main purpose is to give readers new to the language a glimpse of the JavaScript environment and to hopefully spur them onto further readinglearning What is JavaScript JavaScript is a scripting language this means it is not compiled like traditional software languages like C or C but is executed line by line via its host environment at run time the host environment can be Web Browser Server Command Line Desktop Terminology Term Example expression An expression is a command that the JavaScript engine can evaluate to produce a value statement A statement is a command that can be executed statements are terminated with a semicolon identifier A name eg variable name function name labels for loops function declaration function myFunction code function expression var myFunction function code primitive undefined null boolean string and number operator typeof Expressions An example of an expression would be 11 Looks straight forward enough and it should be As mentioned above an expression is simply something that can be interpreted as a value So 11 when evaluated by the JavaScript engine results in an number with a value of two Now the expression 11 is pretty useless in a JavaScript program because were not doing anything with it For us to more effectively use this expression we ideally want to store it somewhere so we can reference it later and thats where variables come in see later on this article Statements Examples of JavaScript statements if condition code while condition code return break throw are single command statements Global Object When the JavaScript interpreter starts up it creates a Global object and any propertiesmethods added to the Global object are available to the entire JavaScript program The global object is a regular object as per the Object section seen later in this article The global object is different depending on the context In a web browser environment the global object is said to be the window object But this isnt completely accurate The global object happens to have a window property which is set back to the global object itself A little confusing I know If you open a browser console eg Firefoxs Firebug JavaScript Console or SafariGoogle Chromes Console Tool and enter the command window youll see all the propertiesmethods available on that object things like windowlocation will be one option that is listed this references the location bar API of the web browser One quote youll hear a lot in JavaScript is Dont pollute the global environment and what this means is try to avoid creating global properties and methods The less globals you create then the less likely your code will conflict with another piece of code written by someone else that may be included in the same page To avoid declaring global propertiesmethods there are certain patterns that have been designed to work around this issue such as the IIFE pattern this pattern is referred to as an IIFE immediately invoked function expression the function is immediately executed and all code within it is scoped to the function when executed we pass through this which refers to the global object we accept this into the function as an argument called global var app function global private data private as in it cant be accessed from other code outside this function note dont EVER name your functions like this function willbemadepublicbutcantbeaccesseddirectly consolelogPrivate return public API private code weve chosen to make public dosomething willbemadepublicbutcantbeaccesseddirectly this appwillbemadepublicbutcantbeaccesseddirectly TypeError Object has no method willbemadepublicbutcantbeaccesseddirectly appdosomething Private Another pattern is to use AMD Asynchronous Module Definition which is a module based pattern see Beginners guide to AMD and RequireJShttpsgithubcomIntegralistBlogPostsblobmasterBeginnersguidetoAMDandRequireJSmd Variables Variables hold valuesdata You declare a variable like so var myvar 123 in this example weve declared a variable and assigned the value 123 to it You must always declare a variable An undeclared variable looks like this myvar 123 notice the lack of the var keyword Undeclared variables are a bad practice because they cause confusion as to whether the variable should be global or not Here is a break down of the different variable scenarios Variable is declared within a function variable becomes scoped to that function ie it isnt accessible outside of that function Variable is declared at the top level of the script file eg not inside of a function variable becomes a global variable ie is available any where within the JavaScript program Variable is undeclared eg doesnt matter where in the program the undeclared variable was created variable becomes a global variable ie is available any where within the JavaScript program As you can see missing a var declaration will mean the JavaScript engine will make that undeclared variable a global variable eg it is assigned to the Global object To see why this is a problem first imagine you have a web page youre working on and in which you have included a 3rd party JavaScript file ie a script written by another developer Now imagine the content of that 3rd party script is as follows 3rd party script written by another developer var myname Mark in this instance the 3rd party script hasnt wrapped the line var myname Mark inside of an IIFE see above pattern so the variable myname will be created as global property eg it is available any where within the JavaScript program as well as explicitly via windowmyname Now you dont want to overwrite the variable myname because the 3rd party script that defined it is relying on it being a String with a value of Mark if you change the value then it could cause the 3rd party code to break So now imagine youre going to add your own code to the page You write the following function dosomething myname Bob this may cause unintended side effects on the 3rd party code Why Because you coincidentally defined a variable by the same name of myname and although you likely intended it to be scoped to the function dosomething youve accidentally forgotten the var keyword and so your variable is undeclared so your variable has been assigned to the Global object thus overwriting the myname variable that is already available on the Global object set by the 3rd party code To fix this you would simply make sure you properly declared your variable like so function dosomething var myname Bob So now even though the variable name is the same the variable is scoped to the dosomething function In todays world youll be very hard pushed to find a library that sets anything more than one global variable but its an issue to be aware of in case you inherit a code base from a less educated developer Global properties arent completely avoidable A library will generally set one global which will act as a namespace for their entire app For example jQuery sets two globals jQuery and and all their methods are available through those two globals so unless youre trying to use jQuery or as your own namespace then you shouldnt notice any conflicts Most companies name their global namespace after their company var mycompanyapp code Types JavaScript types are split into two groups primitive types and reference types Numbers Booleans Strings Null and Undefined types are primitive Objects Arrays and Functions are reference types Its important to know the different types because at some point in your JavaScript code youll want to pass around values and objects and you might notice some issues with doing so For example youll notice that JavaScript passes ObjectsArraysFunctions by reference and passes primitives such as BooleansStringsNumbersNullUndefined by value What this means is if you have an Array and want to copy it so you can make changes to the copy eg you want to leave the original as it is then you might think to do something like this var myarr a b c var newarr myarr newarrpushd you might think that newarr will be a b c d and myarr would still be equal to a b c but youd be wrong because Arrays are passed by reference and so your change newarrpushd is effectively doing this myarrpushd hence myarr now has the value of a b c d and so does newarr because it really is just referencing ie pointing to myarr So be careful Objects To create a new Object use the syntax For example var obj name Mark location London England A variable added to an Object is known as a property A function added to an Object is known as a method this naming convention originates from traditional ObjectOriented languages So for example var obj Property name Mark Property location London England Method getdetails function return thisname is from thislocation Mark is from London England NearlyEverything in JavaScript is an Object One thing to be clear on is that nearly everything in JavaScript is an object eg Functions Arrays Numbers Strings etc They all inherit their propertiesattributes from the top level Object and this is made possible in JavaScript via whats called the prototype chain We wont go into the prototype too deeply now but the following section will touch on how it works briefly Prototype Every Object you create in JavaScript has a hidden object tied to it This hidden object is called the prototype for that object and what this means is that your object youve created will inherit properties from the prototype All objects that you create eg var obj have the same prototype object they point toreference which is the top level Object in JavaScript We call this Objectprototype and this top level Object itself has no prototype its the only object that has no prototype because nothing precedes it All builtin Constructors eg Arrayprototype Dateprototype inherit from the Objectprototype and this linked set of objects is known as the prototype chain and is how inheritance works see below for a short mention of inheritance and other forms of code reuse Accessing propertiesmethods To access propertiesmethods you can use either the dot notation or the bracket notation The difference is in whether the propertymethod name is known at the type of execution or not Using the previous example code if your program was written by you then to access the name property you would simply use objname but if your application accepted input from the user where by you asked them what propertymethod they wanted to access you obviously dont know before hand what the user is going to choose In this instance you would use the bracket notation var userinput documentgetElementByIdmyforminput consolelogobjuserinput The other time youll need to use bracket notation is if you want to access a property that has special characters var obj my property 123 YOU SHOULD NEVER NEED TO CREATE A PROPERTY NAME LIKE THIS consolelogobjmy property So the majority of the time youll just use the dot notation but depending on the dynamic aspect of your program you may need to use bracket notation every once in a while Class Attribute Every object has a class attribute which provides information about the objects type Sounds pretty useful but unfortunately neither of the current specifications ES3 or ES5 allow you to access this attribute But there is a trick to accessing an objects class attribute and that is to call the toString method on the top level Objectprototype but using it on the relevant object you want to get the class of The following example demonstrates the most common use case of accessing the class attribute trying to work out if an object is actually an Array var arr abc consolelogtypeof arr object well thats not right it should return the type as Array note this is a known JavaScript bug consolelogObjectprototypetoStringcallarr object Array thats more like it what weve done here is used a method available to all Functions called call What this method does is let you call another function ie borrow a function but use the calling context as the this value That previous sentence probably didnt make a lot of sense because weve not covered this or anything to do with contextsexecution environments etc but try and stick with it for a moment and understand that what weve done is called the Objectprototypes toString method but weve called it as if it was our Array that had executed toString The way this trick works actually requires a deep level understanding of how JavaScript handles its conversion of data types something I definitely wont go into here Arrays Arrays are like a simplified Object An Object is effectively a mapping of names identifiers to values var obj name value name value name value an Array is the same with the exception that the name identifiers are automatically incremented numerical values So an Array like this var arr a b c would effectively be similar to the following object var obj 0 a 1 b 2 c Note the above object with numerical keys requires you to use the bracket notation to access the identifiers eg obj0 as obj0 would result in a SyntaxError Unexpected number Methods Arrays come with many methods functions that let you manipulate and filter the data contained within the Array Some methods change the data in the Array mutators Some methods return a new Array with the data changed accessors not the bestmost descriptive name really as it suggests these methods just access data when thats not always the case Some methods loop through the data iterators Below are some examples of each Mutators Accessors Iterators pop concat filter push join forEach reverse slice every shift indexOf map sort lastIndexOf some splice reduce unshift reduceRight Conditional Statements Conditional statements are control logic This means that depending on the result of a specified condition the corresponding branch of logic will be executed There are a few different conditional statements such as if switch ternary The simplest way to understand them is to see the syntax If Statement if condition if condition evaluated to true then run this code else if othercondition if othercondition evaluated to true then run this code else other wise well run this code as a fallback So a basic example would be something like this var candrink false var age 18 if age 18 candrink true Switch Statement If you have lots of checks against the same variablecondition then youre better off using a switch statement switch condition case x Execute this code block break case y Execute this code block break case z Execute this code block break default Execute this code block break An example of using this statement would be var car Porsche switch car case Ford Execute this code block if car is Ford break case Porsche Execute this code block if car is Porsche break case BMW Execute this code block if car is BMW break default Execute this code block if car is none of the above break Ternary Statement For very short if statements you can also use a shortened syntax a conditional operator sometimes referred to as a ternary operator because of its three operands The syntax is like so condition true false and can be used like so var age 18 var candrink age 18 true false candrink will equal a Boolean value of true var x abc var y x abc def xyz y will equal a String value of def Coercion One of the areas of most confusion in JavaScript is its ability to coerce data types The best thing to do is to just try and take advantage of JavaScripts ability to coerce objects into different types and use it to your advantage to make your code more succinct For example with an if conditional statement JavaScript will try to coerce the expression into either true or false like so var element documentgetElementByIdjselement if element if the DOM element is available then this conditional will coerce element into a Boolean value of true and hence the condition will pass If the DOM element doesnt exist then the resulting value will be null and null is converted into a Boolean value of false and hence the condition will not pass In the above example JavaScript automatically converts the condition into a Boolean but you can manually coerce a value into a Boolean by using the double negation operator like so var obj age 0 year 1980 objage false because zero coerces to false objyear true any number greater than zero coerces to true You can also use a single negation operator to return the opposite Boolean value of an object this is useful for saying if NOT x var bool false if bool this statement is executed if bool is NOT true ie if its false Some quick notes All ObjectsArrays coerce to true even an empty Array An empty String wil coerce to false There is a lot to learn about JavaScripts coercion process so for full details please see the following article httpwebreflectionblogspotcouk201010javascriptcoerciondemystifiedhtmlhttpwebreflectionblogspotcouk201010javascriptcoerciondemystifiedhtml Functions Functions make it easier to create reuseable code They are simply blocks of JavaScript code that can be calledexecuted multiple times An example of the function syntax is as follows function identifier parameters statements which could be used like so function add a b return a b consolelogadd1 1 2 Parameters Functions accept any number of parameters also known as arguments Within a function you can access all arguments via a special arguments property function test a b c consolelogarguments test1 2 3 1 2 3 youll notice in the above example it looks like the arguments property is an Array but its not Its an arraylike object So you can access the keys values much like an Array but you dont have access to the Array methods Return values If a function doesnt explicitly return a value if you look at our add function example above youll see we explicitly returned a b then the function will have a return value of undefined Hence when you run certain code snippets in a browsers web console youll normally see undefined appear directly underneath the code youve just written Borrowing methods Objects have two core methods made available to them call and apply These two methods are the same ie they do the same thing which is to call a function indirectly while specifying a value for this but with one small difference call requires you pass through any arguments for the function as a comma separated list while apply requires you pass through any arguments for the function as an Array of values The purpose of these methods is to allow you to borrow a functionmethod from another object The following code demonstrates its usage and how powerful it can be var obj1 name Bob var obj2 name Mark speak function return My name is thisname obj2speak My name is Mark obj1 doesnt have a speak method and we dont want to add extra code to that object when obj2 already has the same method already defined We want to be able to just borrow obj2s speak method obj2speakcallobj1 My name is Bob you can see how by using the call method we can make better use of existing functions and so we become more productive as well as have smaller and more efficient code Code Reuse inheritance The final subject I want to briefly cover is code reuse In most objectoriented programming languages the main principle of code reuse is done via inheritance this is where you have a base object that all other objects inherit propertiesmethods from much like how JavaScript already works ie all objects inherit propertiesmethods from the top level Objectprototype The way you implement inheritance is by taking advantage of the prototype chain in JavaScript There are multiple ways to use the prototype chain one populate way is to try and emulate the Classical Inheritance style syntax ie most programming languages have a Class keyword that makes creating objects based off a blueprint Class very easy but JavaScript doesnt have the concept of Classes not yet any way An example of how to emulate Class style syntax in JavaScript currently is by using functions as Constructors like so var Person function settings Instance properties any new instances of the Person class will have these properties thisname settingsname no name given thisage settingsage no age given Instance method any new instances of the Person class will have this method thisgetName function return thisname Create a new instance of the Person Class var bob new Person nameBob age7 Add a method to this instance of the Person Class only no other instances created will have this method bobgetAge function return thisage Test the bob instance has access to both methods bobgetName bobgetAge Create another instance of the Person Class var user new Person nameMark Notice the user has access to a getName method but not a getAge method usergetName I know this will error so Im wrapping it in a try statement try usergetAge catcherr alerterr Uncaught TypeError Object object Object has no method getAge Add a method to the Person Class prototype chain all instances of the Person Class will now get this method even those already defined PersonprototypegetNameAndAge function return Hi my name is thisname and Im thisage years old Test this new method is accessible to all instances of the Person Class bobgetNameAndAge usergetNameAndAge Composition There is another code reuse pattern you can use instead of inheritance called composition The way it works is instead of inheriting methodsproperties from a blueprintbase object you instead borrow the methods using call or apply as weve seen previously An example of this is as follows var person names James Neil Russ Stuart var people names Ash Brad Mark Mike speak functionwhich return Hi my name is thisnameswhich Composition not Inheritance peoplespeakcallperson 1 Hi my name is Neil Mixins Another code reuse pattern is called a mixin which instead of using inheritance we simply copy over the functionsproperties we want to use The following example demonstrates how this works function extenddestination source overwrite var overwrite overwrite false for var i in source if sourcehasOwnPropertyi If were not allowed to overwrite an existing property if overwrite then we check to see if the property is undefined if destinationi undefined if it is then we know we can copy the property to the destination object destinationi sourcei else destinationi sourcei return destination var person names James Neil Russ Stuart var people names Ash Brad Mark Mike speak functionwhich return Hi my name is thisnameswhich extendperson people copy properties from people to person person now has a speak method it didnt have originally personspeak1 Hi my name is Neil Conclusion OK this has been a super briefquick run through of different JavaScript concepts and language features This isnt supposed to be even remotely an exhaustive discussion have you seen the size of JavaScript The Definitive Guide Hopefully this has at least been enough to get you started and interested in learning more If there are any issues or errors then please get in contact and let me know "},{"title":"RPC Variations in Go","tags":["go","grpc","rpc"],"href":"/posts/rpc-variations-in-go","content":" Introduction1 Outline2 Variations3 Requirements4 RPC over HTTP5 RPC over TCP6 JSON7 Calling from Ruby8 gRPC9 Introduction Lets begin by understanding what an RPC Remote Procedure Call actually is RPC is a way of connecting two separate services via a raw TCP socket Outline The fundamental principle behind RPC is to create a service that exposes a function behind that service The steps typically look something like Write a function Add some RPC configuration Register our function as part of our RPC service Start the service and have it listen for messages on a specific port From here we would need to have a client that calls the RPC service Write code which calls RPC function Call the function via a TCP socket with a specific ipport The resulting message can be passed back in different formats eg JSON Variations With this understanding we can now start to look at the Gohttpsgolangorg programming language and the different variations of its RPC packages that it offers Effectively they consist of behaviour such as RPC over HTTP RPC over TCP The latter variation allows the use of either the standard netrpc package or a JSON formatted version found under netrpcjsonrpc In this post well take a look at code examples for each of these packages When utilising RPC youll typically find there are three parts 1 Backend the RPC function 2 Service exposes the RPC 3 Client calls the RPC In most cases the backend will be unaffected By this I mean its just a package with a set of behavioursfunctionality which are being remotely exposed The actual use of the netrpc and netrpcjsonrpc packages are typically used within the Service and Client packages unless the client is implemented in another language then youll use whatever is best suited to that language Requirements Only methods that satisfy the following criteria will be made available for remote access all other methods will be ignored so if you hit a problem in the below code chances are youre not exporting the expected items the methods type is exported the method is exported the method has two arguments both exported the methods second argument is a pointer the method has return type error RPC over HTTP Ive yet to find a justification for using HTTP over TCP but you may have your reasons If thats the case then here is an example of how to achieve this in Go First heres the directory structure Im using remote rpchtmlbackendgo rpchtmlclientgo rpchtmlservicego rpchtmlbackendgo As mentioned earlier the backends responsibility is to define a specific function or behaviour see the code comments for additional information package remote import fmt Args is a data structure for the incoming arguments This needs to be exported for the RPC to be validwork type Args struct A B int Arith is our functions return type This also needs to be exported type Arith int Multiply does simply multiplication on provided arguments This also needs to be exported func t Arith Multiplyargs Args reply int error fmtPrintfArgs received vn args reply argsA argsB return nil rpchtmlservicego The services responsibility is to expose the specific function Below we do this using RPC over HTTP so youll notice the use of rpcHandleHTTP for setting up a HTTP based handler and httpServe for serving back a response to the client package main import log net nethttp netrpc githubcomintegralistrpcremote func main arith newremoteArith rpcRegisterarith rpcHandleHTTP l e netListentcp 1234 if e nil logFatallisten error e for httpServel nil Note I was a little confused originally about having to manually open a TCP socket I just assumed that in using HTTP that step wouldve been abstracted away for me But its not oh well rpchtmlclientgo The clients responsibility is to connect to the remote service and call its exposed function As our service is using RPC over HTTP youll notice our client uses rpcDialHTTP to create the TCP socket connection just before calling the remote function via the returned client instance package main import fmt log netrpc type args struct A B int func main client err rpcDialHTTPtcp localhost1234 if err nil logFataldialing err var reply int e clientCallArithMultiply args4 2 reply if e nil logFatalfSomething went wrong s errError fmtPrintfThe reply pointer value has been changed to d reply The output of the following program is as follows Args received A4 B2 The reply pointer value has been changed to 8 RPC over TCP Most of the time when youre exposing functionality and behaviour remotely youll want to have the least amount of overhead as possible and so youll resort to stripping out the HTTP application layer and moving down to using just the TCP layer First heres the directory structure Im using remote rpctcpbackendgo rpctcpclientgo rpctcpservicego rpctcpbackendgo As before the backends repsonsibility is to define a set of behaviours and functions as mentioned above in the HTTP example we need to export certain items in order for the RPC to be valid and work package remote import fmt TCPArgs is structured around the clients provided parameters The structs fields need to be exported too type TCPArgs struct Foo string Bar string Compose is our RPC functions return type type Compose string Details is our exposed RPC function func c Compose Detailsargs TCPArgs reply string error fmtPrintfArgs received vn args c some value reply Blah return nil rpctcpservicego Our service will now expose the above behaviour by using rpcRegister along with rpcAccept This is the simplest implementation possible The call to rpcAccept is just a helper for directly accepting and serving an incoming request package main import net netrpc githubcomintegralistrpcremote func main compose newremoteCompose rpcRegistercompose listener err netListentcp 8080 if err nil handle error rpcAcceptlistener If on the other hand you wish to interrogate the request or at the very least execute some other behaviour inbetween the request being accepted and it being served you can change the code as follows to swap out rpcAccept for a for loop which calls Accept on the listener instance instead and then manually execute rpcServeConn but remember to do this via a goroutine because its a blocking call package main import net netrpc githubcomintegralistrpcremote func main compose newremoteCompose rpcRegistercompose listener err netListentcp 8080 if err nil handle error for conn err listenerAccept if err nil handle error go rpcServeConnconn rpctcpclientgo Lastly as we already know the clients responsibility is to call the exposed function This time we use the rpcDial function instead of rpcDialHTTP package main import fmt log netrpc type args struct Foo Bar string func main client err rpcDialtcp localhost8080 if err nil logFataldialing err var reply string e clientCallComposeDetails argsFoo Bar reply if e nil logFatalfSomething went wrong v eError fmtPrintfThe reply pointer value has been changed to s reply But if you want to implement a timeout to prevent a call from taking too long then youll want to change rpcDial for netDialTimeout notice theyre separate packages rpc vs net Also be aware that the returned type isnt a client any more as it is in the previous example instead it is a connection Once you have the connection you can then pass that to rpcNewClient Once you have your client youll notice that the rest of the code is the same as before ie the calling of the exposed function via the client package main import fmt log net netrpc time type args struct Foo Bar string func main conn err netDialTimeouttcp localhost8080 timeMinute if err nil logFataldialing err client rpcNewClientconn var reply string e clientCallComposeDetails argsFoo Bar reply if e nil logFatalfSomething went wrong v eError fmtPrintfThe reply pointer value has been changed to s reply The output of the following program is as follows Args received FooFoo BarBar The reply pointer value has been changed to Blah JSON There is another option available when creating an RPC and that is to expose a JSON formatted variation which is required if youre planning on using a different programming language to communicate with your Go RPC service as well see below when we write a client using the Ruby programming language The standard netrpc package uses httpsgolangorgpkgencodinggob Which is a Go specific streaming binary format If your client isnt Go then itll have a hard time communicating If we look back at our TCP example from earlier the one which utilised rpcServeConn we can switch that over to being JSON formatted by just using the same code but making some minor changes In both the service and the client swap netrpc to netrpcjsonrpc In the service swap rpcServeConn to jsonrpcServeConn In the client swap rpcDial to jsonrpcDial Calling from Ruby If you want to utilise a client written in another programming language such as Ruby youll need to have the Go service setup to use netrpcjsonrpc Once thats done your client can connect via a raw TCP socket and pass over JSON data as shown in the below example require socket require json socket TCPSocketnew localhost 8080 Details of JSON structure can be found here httpsgolangorgsrcnetrpcjsonrpcclientgoL45 Thanks to Albert Hafvenstrm albhaf for his help b method ComposeDetails params Foo Foo Bar Bar id 0 id is just echoed back to the client socketwriteJSONdumpb p JSONloadsocketreadline The output from this program would be id0 resultBlah errornil gRPC Google has started work on a new package called gRPC which as per the site grpciohttpwwwgrpcio states is a high performance open source general RPC framework that puts mobile and HTTP2 first They currently support C Java ObjectiveC Python Ruby Go C Nodejs and PHP You can either go to the main GitHub repo githubcomgrpchttpsgithubcomgrpc or if youre only interested in the Go version then you can find it here githubcomgrpcgrpcgohttpsgithubcomgrpcgrpcgo Ive not tried it yet but it looks interesting Update Ive setup gRPC now You can find a beginners guide Ive written herehttpwwwintegralistcoukpostsgrpchtml "},{"title":"Algorithmic Complexity in Python","tags":["design","algorithms","data-structures","python"],"href":"/posts/algorithmic-complexity-in-python","content":" Introductionintroduction Asymptotic Analysisasymptoticanalysis Measuring Algorithmic Performancemeasuringalgorithmicperformance Orders of Complexityordersofcomplexity Growth Typesgrowthtypes Constant Timeconstanttime Logarithmic Timelogarithmictime Square Root Timesquareroottime Linear Timelineartime Linearithmic Timelinearithmictime Quadratic Timequadratictime Polynomial Timepolynomialtime Exponential Timeexponentialtime Factorial Timefactorialtime Introduction In this post were going to review some different algorithmic time complexities Let me begin by clarifying when I say algorithm I mean logic written in code and when I say operation I mean a unit of code was evaluated and that operation could be something as simple as x y Asymptotic Analysis Asymptotic analysis is the computing of an algorithms running time and there are actually a few different variations that allow us to measure different aspects of that running time Big Omega represents the lower bound Big Theta represents both the lower and upper bounds Big O represents the upper bound Were interested in the last notation in that list Big O notationhttpsenwikipediaorgwikiBigOnotation and what the various algorithmic complexity symbols mean when applied to simplified implementations of specific algorithms written in Python The reason for selecting Big O over other notations is that its the most relevant for performance analysis as it helps us to understand the worst case behaviour Note a fantastic quick reference for Big O notation is bigocheatsheetcomhttpbigocheatsheetcom but also Python documents the time complexityhttpswikipythonorgmoinTimeComplexity associated with its various builtin functions which is super useful Measuring Algorithmic Performance When measuring the performance of an algorithm were interested in how the increase in input size will affect the growth rate of operations required and this attribute is typically referred to as the algorithms time complexity There are two types of complexity we might be interested in both dependant upon the length of the input to be processed 1 time quantifies the amount of time taken by the algorithm 2 space quantifies the amount of memory used by the algorithm Note the Big O notation used to describe these complexities is telling us the growth rate of a function which is typically referred to as the order of the function hence the O in Big O Orders of Complexity Good O1 Olog n On OK On Bad On log n Awful On2 O2n On Note when writing a logarithm you are expected to specify the base log 2n but with Big O notation you typically omit the two resulting in Olog n Growth Types O1 constant timeconstanttime OLog n logarithmic timelogarithmictime On square root timesquareroottime On linear timelineartime On Log n linearithmic timelinearithmictime Onn quadratic timequadratictime squared On2 polynomial timepolynomialtime O2n exponential timeexponentialtime On factorial timefactorialtime Constant Time An algorithm has constant time when the number of operations doesnt change as the number of elements increase l lappend1 lenl 1 l listrange1000 lenl 1000 lappend1 lenl 1001 In the above example code it doesnt matter how many elements are contained with the list l Regardless of whether we append a new element to a list consisting of one element or a thousand elements the time complexity is O1 constant time Similarly for acquiring the length of a list lenl regardless of whether the list has one element or a thousand the time complexity stays constant time Logarithmic Time An algorithm is logarithmic when the number of operations decreases by a specific factor with each step Consider an algorithm for searching a specific element from a given input def binarysearchl item first 0 last lenl1 found False while firstlast and not found midpoint roundfirst last2 if lmidpoint item found True else if item Note I discuss binary searchpostsbigoforbeginners8 in more detail in an older post Square Root Time An algorithm has sqrt or time complexity when the number of operations increases dependant on the number of primes under the square root of the given number Consider an algorithm for checking if a number is a prime This would have square root time complexity def isprimenumberx if x 2 for y in range2x if x divides with zero remainder ie equal to bool False if not x y return False else return False return True Linear Time An algorithm is linear when the number of operations increases linearly with the number of elements Consider an algorithm for searching a specific element from a given input def searchx input for i in input printi if i x printfound element return search5 range10 This example search function will loop over every element until it finds the number 5 resulting in it having On linear time complexity Meaning if the input range changes from 10 to 1000 then the number of operations ie loop iterations increases linearly with it The worst case scenario is if x happens to be a number that doesnt exist in the given input We would have to iterate over the entire input before we realized the number didnt exist Note a much better algorithm to use if the input was guaranteed to be orderedsorted would be a binary searchpostsbigoforbeginners8 Linearithmic Time An algorithm is linearithmic when the number of operations increases by the number of elements ie linear time times the result of log n ie logarithmic time Consider the quick sort algorithm whose implementation below selects a random element as the pivot and then loops the entire input list minus the pivot in order to identify elements that are less than the pivot and elements that are greater than the pivot this is the reduce by half logarithmic principle The function recursively calls itself passing in smaller and smaller subsets of input which are iterated over from random import randrange input 10 5 2 3 7 0 9 12 def quicksortarr if lenarr 2 return arr else rand randrange0 lenarr grab a random index pivot arrpoprand less i for i in arr if i pivot return quicksortless pivot quicksortgreater printsorted quicksortinput Note I discuss quick sortpostsbigoforbeginners13 in more detail in an older post Quadratic Time An algorithm is quadratic when the number of operations become the square of the number of elements Consider an algorithm whose implementation is looping over the same input twice def searchinput for i in input for j in input printfi i j j searchrange10 This example search function will loop over every element in input and then itll loop through it again resulting in it having Onn quadratic time complexity Meaning if the input range changed from 10 to 1000 then the number of operations ie total loop iterations increases as a square of the number of elements Polynomial Time A polynomial time complexity is effectively a quadratic algorithm in the sense that with quadratic Onn where n is 10 we have a number of operations equal to 100 and if we compare that to polynomial On2 where n is 10 then again we have the number of operations equal to 100 Now the difference comes when the exponent 2 in the polynomial equation is increased where as with quadratic it will always be a squared number For example On3 is also referred to as being polynominal Exponential Time An algorithm is exponential when the number of operations grows exponentially with the number of elements ie growth whose rate becomes ever more rapid in proportion to the growing total number or size Note exponential time complexity O2n is worse than polynomial On2 because maths tells us that over time exponential will quickly overtake polynomial Consider an algorithm whose implementation is calculating fibonacci numbers which grows exponentially relative to the calculated output The fibonacci numbers are calculated as the addition of the last element with the current element The sequence itself starts at zero so the first seven numbers in the sequence would be 0 1 1 2 3 5 8 So we can see 0 1 1 then 1 1 2 then 1 2 3 and so on Below is a possible implementation def fibonaccinum if num Note I discuss factorialspostsbigoforbeginners5 in more detail in an older post "},{"title":"Algorithms in Python","tags":["algorithms","data-structures","design","python","searching","sorting"],"href":"/posts/algorithms","content":" In this post Ill be demonstrating a few common algorithms using the Python language Im only covering a very small subset of popular algorithms because otherwise this would become a long and diluted list Instead Im going to focus specifically on algorithms that I find useful and are important to know and understand Sorting Merge Sortmergesort Quick Sortquicksort Searching Binary Searchbinarysearch Breadth First Searchbreadthfirstsearch Depth First Searchdepthfirstsearch Dijkstras Algorithmdijkstrasalgorithm Three out of the four search algorithms listed above will be implemented around a graph data struture Graphs appear everywhere in life For example your Facebook list of friends mutual friends and extended friends ie friends of your friends who you dont know is a perfect example of a graph Graphs can also be weighted so they can indicate that a relationship between two nodes within the graph are possibly stronger than another connection and is typically used in road maps for determining the quickest path to a particular node well come back to this later when reviewing Dijkstras Algorithmdijkstrasalgorithm Merge Sort Given an unsorted list of integers the merge sort algorithm enables us to sort the list Best n logn Average On logn Worst On logn The algorithm can be broken down into the following pseudosteps Recursively split given list into two partitions Divide the list until reaching its smallest partition Recursively merge each pair of partitions repeat till reconstructed list That short list actually defines two distinct processes a break up into partitions step followed by a merge step The merge pseudosteps could be further broken down to look something like the following To merge two partitions we iterate over each of them Compare elements from both partitions left and right Append smaller element to new results ie sorted list After comparing the elements increment index for winning partition The above merge steps process is carried out against each recursively sorted set of partitions and is done until we reach a final sorted list Below is an implementation of this sorting algorithm def mergeleft right leftindex rightindex 0 0 result while leftindex lenleft and rightindex lenright if leftleftindex rightrightindex resultappendleftleftindex leftindex 1 else resultappendrightrightindex rightindex 1 result leftleftindex result rightrightindex return result def mergesortcollection if lencollection 1 printcollection is Note notice the less and greater lists are passed recursively back through the quick sort function Below is an implementation of this sorting algorithm from random import randrange collection 10 5 2 3 7 0 9 12 def quicksortcollection if lencollection 2 return collection else random randrange0 lencollection pivot collectionpoprandom less i for i in collection if i pivot return quicksortless pivot quicksortgreater clonedcollection collectioncopy avoid mutation result quicksortclonedcollection printfquick sort of collection result resultnn Difference between Merge and Quick sort Both merge sort and quick sort are divide and conquer algorithms eg they divide a problem up into two and then process the individual partitions and will keep dividing up the problem for as a long as it can So when considering which you should use ie which is better youll find merge sort is more performant because its particular implementation is more efficient with the operations it carries out Specifically the difference between merge sort and quick sort is that with quick sort youll loop over the collection twice in order to create two partitions but remember that doing so doesnt mean the partitions are sorted These quick sort partitions are not sorted until the recursive calls end up with a small enough partition length of 1 where the left and right partitions can be analyzed joined and therefore considered sorted Compare that to merge sort which recursively creates multiple partitions all the way down to the smallest possible partition before it then recursively rolls back up the execution stack and attempts to sort each of the partitions along the way Additionally with a merge sort its possible to parallelize the data over multiple processes while quick sort requires data to be processed within a single process This means that quick sort has potentially more operations to be carried out than merge sort and therefore has a greater time complexity Although quick sort can offer better space complexity worst case Ologn compared to merge sort worst case On All that said the implementation of the algorithms can be tweaked as needed to produce better or worst performance For example quick sort could be modified to use another algorithm called intro sorthttpsenwikipediaorgwikiIntrosort which is a mix of quick sort insertion sort and heapsort thats worstcase On logn but retains the speed of quick sort in most cases Radix Search Another searching algorithm that I see crop up in discussions every now and then is radix search The way it works is to loop over your data structure twice and if were dealing with integers for the first iteration youll bucket the elements by their 1s digit followed by another iteration to bucket the elements by their 10s digit So with a collection like 25 17 85 94 32 79 after the first iteration we would have created numbered buckets that looked something like the following 1 2 32 3 4 94 5 25 85 6 7 17 8 9 79 If we remove the empty buckets it means well end up with a partially sorted list of 32 94 25 85 17 79 Now for the second iteration we rebucket by the 10s so this means we end up with 1 17 2 25 3 32 4 5 6 7 79 8 85 9 94 Again if we remove the empty buckets well find we now have a fully sorted list 17 25 32 79 85 94 Its an interesting algorithm but ultimately is quite limited and so other sorting algorithms like mergequick sort are generally preferred Some constraints to be aware of are is that its generally less efficient than other comparison sorting algorithms Radix sort is also targeted at integers fixed size strings floating points and or lexicographic order comparison predicates Binary Search The most popular algorithm by far for searching a value in a sorted list is binary search The reason for its popularity is that it provides logarithmic performancepostsalgorithmiccomplexityinpythonlogarithmictime on average for access search insertion and deletion operations Average Ologn Worst On The algorithm can be broken down into the following pseudosteps define start and end positions usually length of list locate middle of list stop searching if correct value found if value is greater change end position to the middle index if value is smaller change start position to the middle index repeat above steps until value is found What this ultimately achieves is shortening the search window of items by half each time thats the logarithmic part So if you have a list of 1000 elements then we can say itll take a maximum of ten operations to find the number youre looking for thats log 210 210 1024 Thats outstanding Below is an implementation of this popular search algorithm def binarysearchcollection item start 0 stop lencollection 1 while start item stop middle 1 else start middle 1 return None collection 1 3 5 7 9 11 13 15 17 19 20 21 result binarysearchcollection 9 printfthe value was found at index resultnn index 4 Note you could swap roundstart stop 2 for start stop 2 which uses Pythons floor division operator but I typically opt for the clarity of using the explicit round function Breadth First Search A BFS breadth first search is an algorithm that searches a data structure from either a root node or some arbitrary starting point It does this by exploring all the neighbouring nodes before moving onto other connected nodes The following graph represents a group of people Some people know each other eg both Alice and Bob know each other where as other people dont eg Dave knows Ethan but Ethan knows no one else in this group Well use the BFS breadth first search algorithm to locate Ethan The time complexity of this algorithm will be at worst OVE Note in the case of dealing with a graph V vertex a node in the graph and E edge the line between nodes the worst case scenario will mean we have to explore every edge and node The algorithm can be broken down into the following pseudosteps Pick a starting point in the data structure Track nodes to process eg a queue While the queue has content Take a node from the queue If the node has been searched already then skip it If not searched check if its a match otherwise update queue Queue should be updated with that nodes adjacent nodes If match is found then exit the queue loop Below is our example implementation of the BFS algorithm import random from collections import deque graph alice bob charlie dave bob alice charlie charlie alice bob dave alice ethan def searchstartingpoint name printfstarting point startingpoint queue deque queue graphstartingpoint add startingpoints neighbours printfqueue queue searched while queue person queuepopleft printfperson person if person not in searched if person name printffound a match person return True else queue graphperson add this items neighbours printfqueue updated queue searchedappendperson else printfskipping person as they have already been searched return False startingpoint randomchoicelistgraphkeys searchstartingpoint ethan Note an alternative implementation might use Pythons set data structure to avoid having to filter already searched people Lets take a look at the first run of this program starting point dave queue dequealice ethan person alice queue updated dequeethan bob charlie dave person ethan found a match ethan From that output we can see that we started very conveniently at the Dave node Daves adjacent nodes are Alice and Ethan Due to the order of the nodes in the queue we attempt to process Alice next Then we check the next node Ethan and find what were looking for Now consider a second run which has a different starting point notice the difference in the number of operations starting point charlie queue dequealice bob person alice queue updated dequebob bob charlie dave person bob queue updated dequebob charlie dave alice charlie person bob person charlie queue updated dequedave alice charlie alice bob person dave queue updated dequealice charlie alice bob alice ethan person alice skipping alice as they have already been searched person charlie skipping charlie as they have already been searched person alice skipping alice as they have already been searched person bob skipping bob as they have already been searched person alice skipping alice as they have already been searched person ethan found a match ethan Note Ive used a dict to represent a graph which helped to make the code simpler to understand A different data structure eg a directed tree would require a different implementation of the algorithm Remember the basic premise is to search a graph nodes adjacent fields and then their adjacent nodes Depth First Search The following image shows a tree structure that represents various people A DFS depth first search is an algorithm that searches a data structure such as a tree or a graph from its root node It searches downwards through each child node until there are no more children Well use the DFS depth first search algorithm to locate the node Ethan Using our example tree structure above we would start with Alice then check the first child Bob Bob has no children so we would move onto Charlie Charlie has a single child Fred so we would check him next Fred has no children so we start back up at Dave Finally we check the child of Dave which is Ethan Notice how if dont find a match for what were looking for then we backtrack up to the top of the tree and start again at the root nodes next child node The time complexity of this algorithm will be at worst OVE as noted with Breath First Search this means we could end up hitting every single node and edge in the data struture Note this example uses a traditional tree data structure instead of a graph to represent the underlying data to be searched The algorithm can be broken down into the following pseudosteps Start at the root node Check first child to see if its a match If not a match check that childs children Keep checking the children until a match is found If no children are a match then start from next highest node Below is our example implementation of the DFS algorithm class Treeobject def initself nameroot childrenNone selfname name selfchildren if children is not None for child in children selfaddchildchild def reprself return selfname def addchildself node assert isinstancenode Tree selfchildrenappendnode tree TreeAlice TreeBob TreeCharlie TreeFred TreeDave TreeEthan def searchtreetree node printfcurrent tree tree if treename node printffound node node in tree return tree for child in treechildren printfcurrent child child if childname node printffound node node in child return child if childchildren printfattempt searching child for node match searchtreechild node if match printfreturning the match match return match result searchtreetree Ethan printfresult result Note in our example we use a simple tree data structure to represent the data to be searched Our implementation is designed to work with that structure So for example if we had multiple children per node left and right child nodes then we would need to account for the backtrack to the relevant child right node near the top of the tree Lets take a look at the output of this program current tree Alice current child Bob current child Charlie attempt searching Charlie for Ethan current tree Charlie current child Fred current child Dave attempt searching Dave for Ethan current tree Dave current child Ethan found node Ethan in Ethan returning the match Ethan result Ethan So from this output we can see we started at Alice and first checked Bob but because Bob has no children we moved back up to Charlie From Charlie we go down to Fred but as theres no more children we move back up to Dave Finally we check Daves children to find Ethan When to choose BFS vs DFS Generally BFS is better when dealing with relationships across fields where as DFS is better suited to tree hierarchies That said below is a short list of things to consider when opting for either a BFS or DFS Note the data strutures used including the implementation of your algorithm can also contribute to your decision If you know the result is not far from the root BFS If results are located deep in the structure then DFS If the depth of the structure is very deep then in some cases BFS If the width of the structure is very wide then memory consumption could mean DFS Ultimately it all depends Searching an unsorted collection If you have an unsorted collection then your only option for searching is a linear time complexity On To improve that performance we would need to first sort the collection so we could use a binary search on it Another potential option is to run a linear search across multiple CPU cores so youre effectively parallelizing the processing Its still On linear time complexity but the perceived time would be shorter depending on the restructuring of the collection split chunks Dijkstras Algorithm The Dijkstra algorithm tells you the quickest path from A to B within a weighted graph In the above graph we have a few options Start A End Cost 7 Start B End Cost 7 Start B A End Cost 6 Note as you can see the route that visually looks longer is actually quicker when considering the weighted nature of the graph The time complexity of this algorithm will be at worst OVE as noted with Breath First Search and Depth First Search this means we could end up hitting every single node and edge in the data struture The algorithm can be broken down into the following pseudosteps its important to note that in this implementation we calculate the route in reverse Identify the lowest cost node in our graph Acquire the adjacent nodes Update costs for each node while accounting for surrounding nodes Track the processed nodes Check for new lowest cost node These steps are very specific to our graph so if your data structure is different then the implementation of the algorithm will need to change to reflect those differences Regardless this should be a nice introduction to the fundamental properties of the algorithm Without further ado below is an example implementation of the Dijkstras algorithm graph start a 6 b 2 a end 1 b a 3 end 5 end costs a 6 b 2 end floatinf set to infinity until we know the cost to reach parents a start b start end None doesnt have one yet until we choose either a or b processed route def findlowestcostnodecosts lowestcost floatinf lowestcostnode None for node in costs cost costsnode if cost newcost costsn newcost parentsn node processedappendnode node findlowestcostnodecosts def displayroutenodeNone if not node routeappendend displayrouteparentsend elif node start routeappendnode reverseroute listreversedroute printFastest Route joinreverseroute else routeappendnode displayrouteparentsnode findfastestpath mutates global costs parents arrays displayroute Fastest Route start b a end The output of this program is as expected Fastest Route start b a end "},{"title":"Interview Techniques","tags":["architecture","engineering","goals","interviews","jobs","manager"],"href":"/posts/architecture-interview","content":" Introduction This post will explain a little bit about a particular type of interview an architecture interview Itll breakdown what it is why using whiteboards isnt necessarily a bad thing to use in an interview context as well as understanding a bit about what it is were looking for from a particular candidate in this post the information will be related to interviewing someone for an Engineering Manager opportunity What is an architecture interview An architecture review can be a few different things depending on the context of the interview and the type of role being applied for Typically for a software engineering role a candidate will be asked to design a system architecture based on the interviewers given requirements Note this is typically carried out on a whiteboard but pen and paper is sufficent too although harder for both candidate and interviewer to share and discuss over Whereas for a more senior role like an engineering manager or even a director of engineering the format will change to one where the candidate is asked to present a system architecture theyve been involved with The reason for the switch around of formats is that generally the more senior roles wont necesarily be controlling the system design theyll have input but generally speaking this would be the responsibility of individual teams and so what youre trying to gauge from that type of interview is an understanding of whether they have a solid technical grounding because their other skills are of a bigger priority Is one interview all it takes No To clarify when interviewing a candidate you wont give them a single architecture interview and call it a day ie make a decision There are many interviews that need to happen before you can make such a decision Architecture Organizational Team Values Executive CrossDiscipline Team Managment Note the above are based on a Director of Engineering day of interviews but youd have similar formats for varying engineering roles Arent whiteboard interviews bad In summary no at least theyre not inherently bad When you say whiteboard interview people generally tend to cringe and think of past experiences of being told to solve X problem using Y algorithm and writing out chunks of pseudocode with pen and paper But lets be honest that rarely correlates to the actual work you typically end up doing and so to describe that format as wrong would be correct although as with everything it can depend on the role youre hiring for of course For me that type of whiteboarding session is a problem Whiteboard interviews of that nature the ones that have you jumping through hoops to perform some kind of mathematical trickery are at best useless and at worst very stressful Theyre also likely to result in a candidate leaving unenthused and saying bad things about the company and its interview process on social media You should not be having whiteboard interviews consisting of questions of that kind Whereas architecture interviews are in my experience any way about more about discussing highlevel concepts and demonstrating practical problem solving skills which are applicable at many different levels of experience and useful in many different situations If structured and presented correctly by the interviewer a whiteboard interview such as an architecture interview can actually be fun for both the candidate and the interviewer and be an enlightening experience rather than a stressful one How should an architecture interview be handled The goal for any interview is to allow the candidate to shine and to present themselves in the best light that they can Even if you decide theyre not right for the role or the organisation you want them to walk away feeling good about themselves and what they did that day To hopefully achieve that goal we want the candidate to feel relaxed and that they have everything they need before the interview begins do they need a toilet break do they need a glass of water make sure you ask and that theyre comfortable Next tell them the agenda Just as an example it might go something like We have 90mins and then you move to your next interview So I reckon we should spend five minutes introducing ourselves and what we do here the teams we work in etc and then the next hour chatting about the problem to be solved or the architecture youre going to present and then after that its up to you we could either use up the rest of the time by having you ask us questions anything you like or we could do that for 15 minutes and the remaining time you can have back to relax and we can show you around the office a bit Doesnt have to be exactly like that but basically you want the candidate to know whats going on no surprises or ambiguity So you might also want to make sure the candidate is ok with you taking notes my memory is terrible and so much happens during an interview that you want to be able to go back over your notes to be sure youve taken everything into consideration As far as the actual test portion of the architecture interview remember that people solve problems in different ways to you so be openminded when critiquing their design Lastly and most importantly when dealing with a software engineer ie youre giving them requirements to fulfill as part of the design make sure youre working with that person Help guide them when it looks like they might be losing their way you dont have to outright tell them the answer but you can ask leading questions that should otherwise help kickstart their thinking down a better path Youd do this for any colleague you work with so afford this person the same respect and kindness What are you looking for from a candidate Well for me it depends on the role Ive carried out quite a few Engineering Manager interviews recently and so theyre the most prominent in my mind so Ill use that as my measuring stick but just remember none of this is set in stone and is open for interpretation and tweakingevolving So below are the highlevel perspectives Im looking for and taking notes on But realise you wont be able to incorporate all of these into your single architecture interview It wouldnt be practical nor would it make sense to do that especially if you have multiple interview formats for this candidate These topics are more generally applicable Communication are they remote aware Im generally interviewing remotely so when there are other interviewers in the room in real life its often the case Im not interacted with in the same way as others do they use gender appropriate language do they communicate clearly and circle back to questions they forgot to answer System are they able to describe the history of the project its reason for existing the value its supposed to offer the teams and other comms involved Architecture is it a good design fundamental but just be aware that in most cases this becomes a subjective opinion and so unless there are some horrific consequences to the design then its not actually something to worry too deeply abouthttpstwittercomaprilwenselstatus1030322280042180610 Risk Management Is the system fault tolerant What scenarios were considered Were risks identified if so what were they How were those risks managedmitigated Was the risk monitored Observability What telemetry system was used to gather data What types of instrumentation is in places logging metrics both or other What monitoring tools are in place See also this post about monitoring best practiceshttpswwwintegralistcoukpostsmonitoringbestpractices How is oncall handled is there a culture of responsibility or is there a centralized operations team looking after things Strategy What teams were consultedimpacted Who were the stake holders and what were their involvement How was the system proposed and communicated Distribution Is the candidate used to working within a large multiregion organisation where teams are distributed across many different time zones are they used to those types of challenges Organisation What is the structure and hierarchy of their current employer Is it fairly flat or lots of layers and red tape Maybe its goldilocks ie just right Has there been any restructuring of the teams why and how was it communicated how was the changes received by staff Awareness Was the candidate actively thinking about things outside of just the technical aspect Were they thinking about for example the costs associated with the architecture they were designing and how to reduce those costs whilst trying to still solve the problem at hand Team Management How does the candidate handle 11 meetings build relationships and give feedback How big is the teams they manage what conflicts have they needed to resolve and how Organisation FeedbackInclusion Does their current employer provide staff the means to give them feedback and to understand the health of their organisation If so what are those tools and what has their effectiveness and impact been Diversity Is the candidate thinking about diversity and how to improve it in the hiring process What tools do they use to help with that goal Are there any community programs they support or host Interest Is the candidate excited to work forwith us What about our organisation do they like the most this isnt a deal breaker this is more out of interest Anything else I dont know but maybe you could let me know on twitter if theres anything Ive missed or should be doing differently Feedback appreciated "},{"title":"Bash Watchtower","tags":["bash","shell","monitoring"],"href":"/posts/bash-watchtower","content":" Introduction1 How does it work2 Comparison3 Code4 Explanation5 Conclusion6 Introduction This is a quick post to demonstrate how I use a simple Bashhttpswwwgnuorgsoftwarebash shell script to report when web pages are failing eg returning a non200 HTTP status code It does this by sending notifications of the URL which returned a non200 status code into a remote application in my case Slackhttpsslackcom but you could modify the script to suit whatever service you happen to be using I run this script via Jenkins CIhttpsjenkinsciorg on a five minute cron The inspiration came from Charlie Revetthttpstwittercomcharlierevett who wrote a nodejshttpsnodejsorg package called Watchtowerhttpgithubcomrevettwatchtower I like shell scripts not so much Node and so I decided for no real good reason to replicate his package in Bash How does it work The script has the following steps 1 Cleanup remove any temporary files created during a previous run 2 Retrieve curl the remote endpoints in parallel 3 Notify parse the responses and send notification for any that fail Comparison Well the Node package has quite a few layers to it eg Dockerfile packagejson dependencies multiple nested files that take some time to navigate around whereas my Bash Watchtower is a single shell script So its actually a lot easier and quicker in my opinion at least to understand whats going on and how things work Note on the plus side hes got tests I couldnt be bothered with that for this quick hack My initial concern was going to be around the performance of requesting multiple endpoints as well as sending potentially multiple failure notifications to the remote service Slack I knew that Node is popular for its event driven concurrency and I was keen to ensure performance wasnt degraded in any way Id argue in theory I havent actually tested that performance would be equal or better because Im running the relevant sections of the code in parallel rather than concurrently using the shells operator to background each requestnotification into a separate subshell Im then utilising the wait command which as the name suggests waits for all currently active child processes to complete Note because of the background processes this script will not scale and be as performant once the number of URLs youre looking to check against becomes very large So if youre looking to validate 100s of URLs then youll likely hit performance issues Code So here is the code function cleanup rm resultstxt rm temptxt function pull local base1 local urls2 for resource in urls do curl baseresource head location silent output devnull connecttimeout 2 writeout urleffective httpcoden done wait function parse local results1 local remotehttpshooksslackcomservicesfoobarbaz cat results awk 200 print 2 1 temptxt while read line do curl header ContentType applicationjson silent output devnull request POST data text line remote done resultstxt display resultstxt parse resultstxt Note Ive multilined the curl request here for readability but I prefer one liners Explanation The script is broken out into functions cleanup removes specified files pull gets our endpoints only the HTTP headers parse looks for non200 status code and sends notification display prints specified file The cleanup and display functions arent of any special interest so well focus primarily on pull and parse The only thing I will say is that previously I was manually calling cleanup twice the function was originally written to take an argument a file path and remove the specified file if it indeed existed this has since changed to not take an argument but instead explictly remove the two files I know I create within this script I also now automatically run the cleanup function when the shell exits I do this using trap cleanup EXIT If youve not seen this before then please refer to help trap for more details Note most of the time the man will help you locate information But with builtin commands those that are part of the shell environment itself you need to use help eg help trap or help wait Failing that you could search inside man bash but thats lunacy Pull First we take in two arguments the first we store in a local variable called base while the other is stored in a variable called urls Youll notice weve had to convert the second argument into an Array by assigning something that resembles an Array eg the parentheses and then expand the incoming string of elements inside it 2 Note youll notice that when we call pull we have to pass endpoints and not endpoints this is to ensure we properly expand all elements within the Array Next we loop over the urls Array and for each item we send a curl request which in this case is a unique URL constructed from the base and resource variables but we specify that were only interested in getting back the HTTP headers for the request head We make sure that if the resource being requested actually 301 redirects to another endpoint then we should follow that redirect to the new location location Were also not interested in any progress bars or error output silent We direct any other output to devnull as we dont need it output devnull After this we specify a timeout for each request as we dont want a slow server to impact our scripts performance connecttimeout 2 Now we tell curl to make sure after a successful request it should dump out some additional information to stdout and that it should be formatted in a specific way writeout urleffective httpcode as this makes it easier for us to deal with as outside of this function we redirect this stdout information into a file called resulttxt Finally we call wait which as we now know see above will wait for each of the backgrounded child processes to complete before the function ends Parse In this function we take in a single argument the resultstxt file which would contain a set of results that could look something like httpwwwbbccouknewsbeattopicsentertainment 200 httpwwwbbccouknewsbeatpopular 200 httpwwwbbccouknewsbeattopics 200 httpwwwbbccouknewsbeat 200 httpwwwbbccouknewsbeattopicssurgery 200 httpwwwbbccouknewsbeatarticle32792353imengagedbutwillieverbeabletomarrymyboyfriend 500 Note here the results suggest only one URL has returned a 500 status code We also store off our remote endpoint in my case our Slack incoming webhook URL in a variable called remote This is where well be sending our JSON data of failed URLs to At this point we use AwkhttpsenwikipediaorgwikiAWK to check each line of the incoming resultstxt to see if it doesnt include 200 somewhere If it doesnt then we store that line into a temptxt file in the format of We then redirect the contents of temptxt into a while read loop and for each line we curl our remote endpoint in parallel using POSTing it a JSON object that details the URL that gave a non200 response Again like the pull function we utilise wait to ensure all the child subprocesses finish before doing some final displaying and cleanup of the temptxt file and then returning the function back to the caller Conclusion Thats it Fairly standard Bash scripting Im sure theyll be some unixlinux neckbeard wizards in the audience ready to shred me a new one because my chops arent as wizardy as theirs If thats the case feel free to get in contact as Id love to know how I could make this code simpler or easier to work with or just more idiomatic "},{"title":"Basic Shell Scripting","tags":["bash","shell"],"href":"/posts/basic-shell-scripting","content":" Introduction1 Basics2 What directory am I currently in How can I see whats in this directory Moving around Displaying content of a file Copy a file Move a file Create a file Rename a file Delete a file Delete a directory Create a directory Find searching for files3 Finding files over a certain size Grep Searching for patterns4 Sed Find and Replace5 Awk Looping Logic6 Piping IO7 Input and Output Redirection Piping Piping examples Sequences and Parallelism Processes8 Viewing processes Moving processes between the forebackground Miscellaneous Commands9 tee dig ps xargs cut tr du Ctrlr Conclusion10 Introduction The unix command line has a hundred or so commands and a small majority of those you can realistically find yourself using on a regular basis In this post I want to cover some common commands that can actually be quite useful to you Shell commands arent something you can cover in one post Entire books have been written on the subject So dont expect anything other than the bare bone basics here which should hopefully give you enough of an understanding to take your own learning forward So lets begin Basics OK so Ill assume you have absolutely no prior command line experience which means we need to start at the basics So first things first open up your shell if youre on a Mac then this will be your Terminalapp application What directory am I currently in pwd this stands for Print Working Directory How can I see whats in this directory ls this tells the shell to list out any files or folders in the current working directory You can also tell the command a directory path you want it to look inside of ls Desktop this will list all files and folders on your desktop ls l the l flag tells the command to stack the list when it prints its output to the shell ls la this is a combination of the previous flag and the a flag which means show all files by this I mean itll show you hidden files Moving around To move around your file system you use the change directory command cd So cd Desktop will put you in the Desktop You can also use relative paths such as cd which will take you up one level from wherever you happen to be Displaying content of a file The cat command is a concatenation command meaning that if you ran cat originaltxt newtxt it would display on your screen the combination of the content from both files specified So with that understanding we can use cat originaltxt ie specifying just a single file to show the contents of that file Copy a file To copy a file we need the cp command and we tell it what file to copy and where to copy it to So cp Downloadstesttxt Desktoptesttxt will copy the file testtxt which is inside our Downloads folder and put the copy on our Desktop Move a file To move a file you need the mv command and we tell it what file to move and where to move it to So mv Downloadstesttxt Desktoptesttxt will move the file testtxt from our Downloads folder onto our Desktop Create a file To create a file you need the touch command So touch sometestfiletxt will create an empty text file in your home directory Rename a file There is no rename command on Unix although there is in Linux and so we need to use a trick the trick being to use the mv command So mv Downloadstesttxt Downloadsnewtxt will actually rename the file testtxt to newtxt as weve moved the file into the same directory it was already in but with a different name effectively acting like we renamed it Delete a file To delete a file we need the remove command rm So rm Downloadstesttxt will delete our testtxt file Delete a directory To delete a folder we need the remove command rm again but this time we need to pass in a couple of flags to the command The first flag is f which means force the removal otherwise if you try to remove a folder then the shell will try and prevent this as itll assume youve made a mistake and deleting a whole folder could be a big mistake if youre not careful The second flag is r which means recursively So youll recursively delete files within the folder So rm rf Desktopsomefolder will delete our somefolder folder on the Desktop Create a directory To create a directory you can use the make directory command mkdir myfoldername To make creating lots of sub directories easier you can pass a p flag like so mkdir p AssetsScriptsStyles notice we use interpolation which lets use specify multiple folder names in the current directory level Find searching for files The find command is useful for walking a directory hierarchy and returning a list of files found based on a set of criteria you have determined The most common pattern is to search for a particular file or file type within a project folder Imagine you want to find a file called packagejson within your project directory which is inside your home directory Here is how wed use the find command to do that first well setup the project directory and some files cd move in to the users home directory mkdir projectfoobar make our project directory cd projectfoobar move inside the project directory touch testtxt packageblahjson this creates 3 files inside our project directory ls blahjson packagejson testtxt cd move back into the users home directory and well start searching here is the actual find command find projectfoobar name packagejson projectfoobarpackagejson find projectfoobar name json projectfoobarblahjson projectfoobarpackagejson As you can see from the above example the find command has three parameters the directory to search an optional flag to indicate what we want to search for the file were searching for Lets see this broken down projectfoobar the directory to search name the flagoption weve set is check the name of each item found json weve used a wildcard asterisk to indicate were interested in any json file Recursive searching To recursively search for content you need to quote the glob you use find name go Process results If you want to process the results you get without a pipe then use exec find name go exec grep mypatternimsearchingfor Note where is a placeholder for the file path Youll see the at the end indicates the end of the command You need to escape the otherwise it wont work Also you can replace it with which appends file paths The reason youd use this is if you want to execute your specified utility command once and have all matching file paths added as arguments Whereas means itll execute the command once for each file matched Finding files over a certain size all files greater than 1mb find HOME size 1024k all files inside current directory greater than 500k find name js size 500k find all files larger than zero but less than 500bytes find size 0 a size 500c a is AND c is bytes find all all files larger than zero OR o any that havent been accessed in over a year find size 0 o atime 365 Grep Searching for patterns Grep is a command that lets you find a pattern either a string or a regular expression inside of a file or list of files So grep something testtxt looks for the word something inside of the file testtxt To use grep on a directory of files then we need to use an additional flag r which means recursive similar to the rm command we saw previously So grep r something Desktop looks for the word something inside of any files on the Desktop Sed Find and Replace The sed command stands for Stream Editor and allows you to read in the contents of a file and then write the modified output to another file or pipe it through to another IO command well cover piping later A basic example of its use would be sed sdaynight noveltxt This replaces the first occurrence of day to night If we wanted to replace multiple occurrences then you would need to pass a g flag meaning global to the regular expression like so sed sdaynightg noveltxt Sed is very powerful and there are many features of the syntax that I dont use One thats interesting is that you can use regular expressions to match a pattern and then do stuff with the matches such as deleting the line or duplicating it Consider the following example echo hi therenmarknits nice tonmeet you testtxt If we now run the following sed command we should see the line that has the word mark is deleted cat testtxt sed markd If we run the following sed command we should see any lines that start with i are duplicated remove the carrot symbol to see any lines that have the character i duplicated regardless of whether theyre at the start of the line or not cat testtxt sed ip You can also edit in place if youre feeling brave find name go exec sed i s Awk Looping Logic The awk command reads in each line of a file and splits the line into fields using whitespace space tab as its default delimiter You can then execute commands for each line and reference each field A basic example of its use would be awk print 1 which means print the first field found in the current line So imagine you have the following testtxt file This is my first line This is my second line This is my third line you could print the line number followed by a specific word in this case the second from last word on each line using the following awk command awk print Line NR NF1 testtxt Which would display the following content on your screen Line 1 first Line 2 second Line 3 third Lets break this command down a little Awk commands are placed inside of single quotes awk commands go here Inside the single quotes we need a set of brackets to place our specific code we want to run awk code to run here We specifically tell awk to print something to stdout ie the terminal screen In this case we tell it to print the text Line followed by the current line number NR As part of the same print command we then tell it to print followed by the second from last number To do that we use two pieces of syntax and NF NF stands for Number of Fields The wrapping around NF is our process substitution This means were not just outputting some data but manipulating it by using logic to give us 1 field back from the last hence it needs to be wrapped in Piping IO The previous commands awk sed grep are all really useful but its when you can combine them that their true power shines Input and Output Unix is based on the principle of input and output known as IO In the Shell you have stdin standard input and stdout standard output By default stdin is your keyboard ie whatever you type into the terminal shell and stdout is the terminal ie your screen Redirection Once you understand stdin and stdout you can start to look at redirecting them For example when using the sed command you could use redirection to not overwrite your original file and instead direct the output stdout coming from the sed command to another file sed sdaynightg originaltxt newtxt Piping Another way to direct input and output is to use pipes a vertical bar A really simple example would be look at the sed command we used earlier sed sdaynight noveltxt Rather than actually execute it and have it make the specified change to our file noveltxt we could instead test the command to make sure it does what we expect it to To do that we would use the cat command which we looked at previously and pipe its output through to the sed command like so cat originaltxt sed sdaynightg So to clarify how this works were redirecting the cat commands stdout through to the sed commands stdin In our original sed example we directed the sed commands stdout to an actual file noveltxt but in this case it has no stdout specified so it falls back to the default stdout which in this case is the terminal shell itself Hence the results of the sed command the modified content are displayed on your screen Piping to Vim One thing I discovered recently via Crystal HirschornhttptwittercomPand0ra83 was that you cant just pipe stdout into Vim unless you add a hyphendash to the end of the command like so ls vim Otherwise Vim will complain that Input is not from a terminal Thats a good one to remember Also you can pipe the input into Vim in readonly mode using the R flag as well ls vim R Piping examples Here are three real world examples Ive used recently phantomjs 21 networktestjs tee logtxt In this example Im executing a PhantomJShttpphantomjsorg script networktestjs but I wanted to capture both the results of the script which just logs out DNS information into the terminal and any errors that may have occurred into a log text file The way it works might be a little confusing as it shows some things you might not have seen before 21 and tee Those two commands may look confusing but it just comes down to understanding the numbers that are associated with specific processes so 0 stdin 1 stdout 2 stderr this means 21 is saying direct 2 any errors through to 1 standard output We then pipe the stdout through to the tee command which copies it into a file called logtxt ls File sed sFilemv 12 sh In this example Im trying to remove a hyphen from some file names The files I have look like FileABgif and I want them to be renamed to FileABgif So first I list out any files in the current directory that begin File and then pipe those results through to sed Sed then uses Regular Expressions to store a reference to the opening part of the file name in this case FileA and then stores the end part of the file name in this case Bgif In the second part of the sed command instead of doing a replace of what weve found we actually pass in a mv command remember from before that we can rename a file by using mv originaltxt newtxt In this case the stored references to the beginning and ending parts of the files name can be referenced within the replacement section using 1 and 2 and the in regular expressions means the original string being inspected So when we use mv 12 were saying move the original file and move it to the same directory but using the new name of FileABgif remember 1 is FileA and 2 is Bgif Finally because the sed commands replacement is an actual command rather than just a string replacement we pipe that replacement content which is now seds stdout over to the sh bin command to execute and hence actually rename the files Note whenever you write a shell script you would store it for example inside a file with the extension of sh and then youd use the terminal command sh to execute that shell script tmux ls cut d f 1 xargs I tmux killsession t So in this example I wanted an easy way to destroy all my tmux sessions Typically I would run tmux ls to see what sessions I had it returns something like 0 1 windows created Fri Oct 4 182438 2013 129x33 where the opening 0 is the numbername of the session followed by details about the session in this case 1 window and when it was created and the size of that window Once I had my session number in this case 0 I could run the command tmux killsession t 0 but if I had loads of sessions open I would have to run the same command for all of them To fix this I tried using the commands Awk and Sed but discovered an issue with scope which Im still not 100 sure I understand but Ill explain what happened any way I was using tmux ls awk print 1 sed sg xargs I tmux killsession t This works but not when you stick it inside an alias for easy reuse The way it works is that it lists out all the tmux sessions and pipes it over to Awk Awk then grabs the first field 0 remember Awk splits the input line into fields using a space delimiter We then pipe that over to Sed Sed then uses a regular expression to remove the from the 0 leaving us with just 0 We then pipe that through to xargs xargs runs our killsession command and passes through the value of 0 into that command using the placeholder We define what the placeholder will be using I so we could of used I target instead if we wanted to like so tmux ls awk print 1 sed sg xargs I target tmux killsession t target and it would of achieved the same Like I say this works But I wanted it inside an alias so I could easily reuse it I mean just try and memorise that massive chunk of commands The moment it went into an alias the xargs failed to work because instead of getting 0 it got the entire line 0 1 windows created Fri Oct 4 182438 2013 129x33 The scope of the argument was being lost some how A bit annoying really My colleague at BBC News Simon Thulbournhttptwittercomsthulb all round command line wizard amongst many other technical talents helped me understand a more efficient and fully functioning version ie it can be safely aliased tmux ls cut d f 1 xargs I tmux killsession t So the only difference here is instead of using both Awk and Sed were just using Cut Ive not mentioned it before but cut works like this Cut splits the input into fields like Awk does We then tell it that we want the fields to be split by thats the d section Then finally we use f 1 to say we want the first field which we pipe over to xargs Otherwise the rest of the command is the same as before Nice huh Sequences and Parallelism The use of between commands means the commands are run in a sequence So for example if you run x y the y command will not be run until x has finished this is similar to using the semicolon to make commands run sequentially x y The use of a single between commands means the commands are run in parallel meaning they dont wait for each other So for example x y will mean x and y both run at the same time Processes Each command you execute is a process So when we execute the command vim which opens up the Vim text editor we have effectively started up a new process Viewing processes To view a list of all processes currently running across the system use the ps command you can also use ps aux If youre only interested in processes within your current terminal tab then use the jobs command Moving processes between the forebackground To background a process eg while we have Vim open if we wanted to move back to the terminal then we could execute the command which means pressing the and z keys at the same time To then bring the latest process ie the last process that was put into the background to the foreground again you would run the command fg If you have multiple processes in the background then you can look up the processes using job and then pick one and foreground it using the command fg n where n is the number of the job Miscellaneous Commands tee The tee command youve seen already now in our above example but just to reiterate its use here is how the manual describes it The tee utility copies standard input to standard output making a copy in zero or more files dig The dig command is used for carrying out DNS lookups dig integralistcouk returns the DNS records found for my domain name ps The ps command stands for process status It shows you all running processes on your computer You can use piping again to narrow down the results to something in particular you know is causing your computer to slow down and then execute another command to kill that process So ps aux grep ruby In the above example we also pass aux which basically specifies table of results that should be returned see httpenwikipediaorgwikiPsUnixhttpenwikipediaorgwikiPsUnix for more information We then pipe that through to grep and tell it were interested only in processes that have the text ruby somewhere that way we can narrow down the results printed to the screen Finally to kill a particular process youll need its PID number which ps aux would have displayed so locate that PID and then run kill 9 xxxx where xxxx is the PID number you want to stop xargs I know weve covered Xargs already in my previous examples but its worth mentioning that you can also use the 0 flag which helps with some commands that wont work when passed arguments that have spaces in them imagine a file name with spaces Using the 0 flag resolves that issue Also if the command you want to run only excepts a single argument for example echo 123 then you can omit the I placeholder definition cut Again weve covered Cut above already but just to note that you can change the field delimiter using d eg d would split the line on commas Also f allows a range not just a single field index So if you wanted fields 3 to 4 you could do f 34 Another feature of cut is the c flag which cuts based on character position rather than fields like f does One way you could use the c flag is to remove whitespace at the start and end of a line like so echo xyz cut c 2 rev cut c 2 rev notice our text xyz has one character of white space at the start and end So we specifically tell cut to start at character 2 x and cut until the end of the line 2 and then we use the rev command which reverses the content so it becomes zyx and then we again cut from the 2nd character this time z and cut until the end of the line and finally we reverse the line one more time so were back to where we were initially but with the white space removed There are probably more elegant ways to achieve this but it gives you an indication of how you might want to use a command in unique ways tr The tr command stands for translate characters and it allows you to change the characters in a string of text into a different set of characters For example echo foonbarnbaz tr bf here were saying any occurance of the letters b and f that are found in the string foonbarnbaz should be replaced with a Notice also the n new line character within our string which means that when tr executes and loops over the string itll see this single line as three individual lines foo on the first line bar on the second line and baz on the third line The result would be oo ar az We can also invert the translation so we can say any occurance of a letter that ISNT b or f then translate them into an For example echo foonbarnbaz tr c bf This is done using the c flagoption But now if we look at the result youll find it hasnt quite done what wed expect fbb As you can see weve not catered for our n new lines that were within our original string The tr command has seen the new lines and replaced them with an because as far as its concerned a new line isnt either a b or f character So well need to tweak our command slightly to accommodate our need to keep the new lines echo foonbarnbaz tr c bfn Which result in f b b The last thing I want to show you is the squeeze command which you use by adding the s flagoption What this does is any consecutive letters eg the o in fooobar is repeated consecutively will be reduced to a single replacement For example echo fooobar tr s o Results in fbar Whereas if we didnt use the squeeze command we wouldve seen fbar instead notice the o was translated multiple times du The du command stands for disk usage and it will display the amount of space a directory occupies du s k The s flag displays an entry for each specified file while the k flag displays block counts in 1024byte 1Kbyte blocks Meaning running the above command will result in the following output for me anyway 0UsersmarkmcdonnellApplications 1355232UsersmarkmcdonnellBox Documents backup 6987864UsersmarkmcdonnellBox Sync 1931180UsersmarkmcdonnellCode 1488196UsersmarkmcdonnellDesktop 13070884UsersmarkmcdonnellDocuments 964816UsersmarkmcdonnellDownloads 3578120UsersmarkmcdonnellDropbox 17251988UsersmarkmcdonnellLibrary 0UsersmarkmcdonnellMovies 232UsersmarkmcdonnellMusic 1992UsersmarkmcdonnellPictures 0UsersmarkmcdonnellPublic 38075864UsersmarkmcdonnellVirtualBox VMs 56Usersmarkmcdonnellbin 588988Usersmarkmcdonnelldb 528Usersmarkmcdonnelllib 2013Usersmarkmcdonnellman 818320Usersmarkmcdonnellsrc If we modify the command to pipe over to the sort command like so du s k sort k1nr less then we can make the feedback a little bit more useful ie the directories are sorted by overall size du 38075864 UsersmarkmcdonnellVirtualBox VMs 17253304 UsersmarkmcdonnellLibrary 13070884 UsersmarkmcdonnellDocuments 6987868 UsersmarkmcdonnellBox Sync 3578120 UsersmarkmcdonnellDropbox 1931180 UsersmarkmcdonnellCode 1488196 UsersmarkmcdonnellDesktop 1355232 UsersmarkmcdonnellBox Documents backup 964816 UsersmarkmcdonnellDownloads 818320 Usersmarkmcdonnellsrc 588988 Usersmarkmcdonnelldb 1992 UsersmarkmcdonnellPictures 528 Usersmarkmcdonnelllib 232 UsersmarkmcdonnellMusic 56 Usersmarkmcdonnellbin 20 Usersmarkmcdonnellman 0 UsersmarkmcdonnellApplications 0 UsersmarkmcdonnellMovies 0 UsersmarkmcdonnellPublic Double exclamation Sometimes you need to execute a command using sudo privileges If you forget to do this dont write out the command again but with sudo at the start Dont even use the up arrow key and then move the cursor to the start of the line to type sudo All you need to do is to run sudo The expands to the last command you executed Ctrlr Rather than try to remember an old command you typed a few hours ago let the terminal remember for you If you press which is and r keys at the same time then start typing what you think the command was the terminal will start to autocomplete using your command history You can even press multiple times to start cycling through your command history Conclusion This was a pretty fast paced run through of some different unix commands As time goes on Ill update this post to include other commands and real work use cases that I think would be interesting and useful to those readers new to the command line If there were any errors or any thing like that then just let me know by pinging me on twitterhttptwittercomintegralist "},{"title":"Big O for Beginners","tags":["bigo"],"href":"/posts/big-o-for-beginners","content":" Introduction1 Understanding Big O2 Logarithms3 Logarithm Example4 Factorials5 Back to Big O6 Simple Search7 Binary Search8 The Travelling Salesperson9 Calculating Operation Speed10 Arrays vs Linked Lists11 Selection Sort12 Quick Sort13 Conclusion14 References15 Introduction When you first start learning algorithms Binary Search Quick Sort Breadthfirst Search etc youll quickly realise that in order to take advantage of these algorithms you need to know how fast they are Otherwise when presented with a programming problem in which you want to select an algorithm to use to solve that problem how will you know which algorithm is more efficient One way to know how fast an algorithm is would be to use the Big OhttpsenwikipediaorgwikiTheBigO notation Understanding Big O Big O doesnt tell you how fast in time eg seconds an algorithm is Instead it informs you of the number of operations and how those operations will grow over time Although well see how to calculate the speed of operations later on in this post the primary benefit is to see at a glance the growth of operations as your data becomes larger So the O in Big O means Operation This means in order for you to really understand Big O youre going to need to know some maths Now this is OK Im genuinely terrible at maths but youll see as we go along that its not as complicated as you might think Effectively there are two math concepts we need to know 1 Logarithms 2 Factorials You dont even need to know that much about them Only the bare minimum is required So lets make a start with Logarithms and then move onto Factorials afterwards Once we understand those two concepts we can go back to Big O and start tying together some examples Logarithms As I said in order to understand Big O youll need to understand how LogarithmshttpsenwikipediaorgwikiLogarithm work Im terrible at Math but luckily the awesome book Grokking Algorithms An Illustrated Guidehttpswwwmanningcombooksgrokkingalgorithms which I highly recommend helped me at least understand the basics of Logarithms enough so that I could then go on to understand Big O In essence the Logarithm notation looks something like Log n Note I had used a nice icon instead of n but on my phone I noticed it wasnt showing so I had to change it to something that wasnt a symbol where the n is whats called the base and is the number youre aiming for But really what were interested in is the result of this calculation Logarithm Example Imagine we have the following Logarthim Log5100 What this is effectively asking is how many times do I need to multiple 5 by itself in order to reach the number 100 Lets find out 555 125 Looks like the result of our Logarithm wouldve been 3 because there were three 5s used in order to get to a number that was equal or greater than 100 In this case the calculation wasnt exactly equal By that I mean we went past 100 and ended up at 125 But thats the essence of how to understand what a Logarithm is asking and how to calculate the result So we can see the calculation looks like this Log5100 3 The 3 is effectively the worst case number of steps involved when calculating that particular item The number 100 in this case represents the number of items we have to execute our algorithm against So if you were using an algorithm such as a Binary Search demonstrated later on below and it was running over a collection of items then the length of the items in the above example Logarithm would be 100 Well see how this is useful in measuring an algorithms speed in a later section of this post But for now lets go and understand Factorials Factorials Factorials are one of those things that can come in handy for a number of reasons But generally theyre really useful for identifying variations Imagine you have three letters A B C How many variations of these letters can you produce So we have A B C as one variation A C B would be another variation and then maybe B A C would be another etc But how do you calculate how many variations there are The solution is the factorial n where n is the number of items you have So in our example we had three items so that would be written as 3 factorial Which really means 321 6 So thats six variations you have for three items But what if you have 10 items 10987654321 3628800 You can see that with a small number of items the number of operations is massive This is the total oposite of Logarithms which we looked at earlier remember its growth of operations stayed consistently good when the collection grew Although Factorials serve a useful purpose the example given in the book Grokking Algorithms is one called The Travelling Salesperson which is a problem that required calculating the quickest route the saleperson can take in order to visit n number of cities they are probably the worst performing algorithm So if you see n you should be wary Back to Big O So now we understand how Logarithms and Factorials work we can come back to the Big O notation and understand that its really a simple visual wrapper around these different mathematical calculations O In the above snippet is a calculation If we were considering Logarithms then it would look like the following OLog n If we were considering Factorials then it would look like the following On Its important noting that youll never see a specific calculation like OLog5100 or O3 in Big O Itll always be an abstract version like OLog n or On because Big O notation is a way of talking to other people and having a common language For example Hey Bob I dont think we should use the Simple Search algorithm because its On Wed be much better off using a Binary Search as thats OLog n See how this gives us a common language Its similar to Design Patterns Patterns can be implemented in a variety of different ways and yet we have common language for easily identifying code that follows a certain pattern or when we think a particular pattern might be a good solution to a design problem we have a common language for explaining the solution to someone else without needing to actually implement it first Now my mention of Simple Search and Binary Search might not mean much to you as you might not know how these algorithms work So lets look at these two algorithms next and then after that youll hopefully understand why Big O helps us understand the performance of these algorithms Simple Search Simple Search is probably the simplest algorithm youll ever learn Youll see why in just a moment Imagine you have a collection of twelve items and these items are numbers which are sortedin order You are required to locate a particular item within that collection Here is the collection 1 3 5 7 9 11 13 15 17 19 20 21 The Simple Search approach is to loop over the collection one item at a time and check whether the current item matches the item youre looking for So if the number you were looking for was 19 then that means youll first check 1 nope thats not it Youll then check 3 nope thats not it either and keep going until you reach the number youre looking for So what does this algorithm look like in Big O On What does Big O tell us Big O is telling us that this algorithm performs in linear time This means the number of operations increase linearly with the number of items in the collection So in the above example the worst case number of operations will be 12 But if the collection length was 100 items then the worst case number of operations would be 100 So in effect the bigger the collection the more expensive this algorithm becomes With a very small collection its fast because of its simplicity but beyond a small collection its a poor performing choice of algorithm This is what Big O is telling us here Binary Search Consider the same example as before a collection of 12 items The Binary Search algorithm is quite straight forward you set the start and end indexes usually zero for start and the length of the collection for the end Now you locate the middle of the collection and check if the value youre looking for either matches or is too lowhigh If it matches then hey great youve just reduced the number of operations by a large amount compared to the Simple Search If the middle item our guess is lower than the actual item were looking for then you reassign the value of start to be the middle index the end stays set to the length of the collection Youve now reduced the sliding window of items by half Alternatively if the middle item was larger than the item were looking for then you reassign the value end to be the middle index the start stays set to zero Youve again reduced the sliding window of items by half From here we rinserepeat until the next middle of the collection selection is the item were looking for This is a much more efficient search algorithm compared to Simple Search Lets see what this looks like in Big O notation OLog n What does Big O tell us Big O is telling us that this algorithm works in log time which means the performance improves as the size of the collection increases You can tell this at a glance with Big O syntax especially as you now know how Logarithms work This algorithm will perform well across a wide range of collection sizes Big O in effect tells us how the operations grow So for this particular algorithm example the worst case number of operations is 4 Take a look at our earlier explanation of Logarithms if youre unsure why that is but in a nonabstract sense this would look like OLog212 Were dividing our collection in two Log2 for each operation 12 This is probably one of the best Big Os youll come across as effectively the performance and by that I mean the growth of the operations stays consistently good as the size of the collection increases So if you have a collection of 1000 items then that would result in worst case 10 operations required to locate the item youre looking for 2222222222 1024 How about a collection of a million items That would be result in worst case 20 yes 20 operations to find the item you were looking for amongst one million items Thats incredible The Travelling Salesperson Not much to say here that we havent already mentioned earlier when talking about Factorials This problem is about calculating the number of different routes someone can take in order to ensure they reach all the specified number of cities and then working out which route was quickest In Big O notation this looks like On What does Big O tell us Big O is telling us this algorithm results in factorial time and this is one of the worst performing algorithms known to mankind Apparently it baffles the maths community in that there isnt actually a better algorithm to solve this problem So as we saw earlier if there were ten cities to visit and we need to identify the quickest route to visit all ten cities it would take us approximately 3628800 operations to just calculate all the variations before we could identify which one was quickest Calculating Operation Speed In order to calculate the speed of an algorithm we need to know the worst case number of operations This is what Big O in effect gives us whether it be linear time log time or factorial time So how do we calculate the speed based on the number of operations First we need to know how many operations can be executed within one second The answer to that question is ten operations 1 second 01 10 operations Note your computer can handle more than 10 operations a second but as weve said before Big O is about worst case scenarios and so although not realistic this number does give us a nice base line to work from We can then calculate the time associated with a number of operations Imagine we have the following Big O OLog n If this was a collection of 16 items then the nonabstract version would look something like OLog216 We know from our earlier discussions that this would result in a worst case result of 4 operations to find the item were searching for Remember 2x2x2x2 16 Thats 4 times we multipled 2 by itself to reach 16 We also now know that a single operation takes 01 of a second So with this in mind we can calculate the speed of OLog216 as being 04 seconds 01 4 operations 04 If we were using the Simple Search algorithm which is On on a collection of 16 items then we know that this would take 16 seconds to complete 01 16 operations 16 Arrays vs Linked Lists Lets consider what Big O looks like for the Array and Linked List data structures An Array supports index access meaning you can jump straight to an Array index Whereas with Linked Lists you have to traverse the entire list in order to locate a specific item Note another difference comes in memory management Arrays require n number of memory slots to be next to each other whereas Linked List memory can be sparse and spread out due to how it implements its internal chaining of nodes This is why Arrays are typically considered to be fixed size and not easily expanded because expansion of the Arrays size could potentially require an expensive movement of the Array to a new location in memory in order to faciliate a new index and yet still have memory slots sidebyside This suggests that Array lookups are O1 known as constant time because the lookup growth stays the same no matter the length of the collection ie its constant Linked List lookups on the other hand are On which we already know is linear time remember this is how the Simple Search algorithm performed But what about new data insertions Well with an Array you insert new items at the end of the Array and because of its index access it would indicate O1 But if youre inserting an item into the middle of the Array then this changes to On because of the internal implementation of Arrays in memory it means you need to reorder all the following items which is expensive hence the sideeffect of inserting into the middle is really more like linear time Linked List insertions are generally O1 if inserting at the beginning or end of the list but more like On O1 if inserting into the middle of the list because you have to traverse the list first and then insert your new item Selection Sort The selection sort algorithm sorts an unordered list by looping over the list n number of times and for each loop it identifies either the smallest or largest element which smallestlargest depends on how youre hoping to sort your list do you want ascending or descending order But ultimately youll end up constructing a new ordered list Specifically you loop a number of times to match the collection length Then you loop the collection looking for smallestlargest item Then you mutate the collection so its smaller by one This ends up being On n to the power of 2 or On n Note although youre looping over the collection multiple times you are in fact looping over a slightly smaller collection each time But its still considered On for each time you loop over the collection because regardless of how many items are in there you treat it as abstract So if your collection is 10 items long then it would calculate as follows 10 10 ie n meaning 10 to the power of 2 100 operations 01 100 10 seconds Quick Sort The quick sort algorithm achieves the same result as selection sort but is much faster This particular algorithm sorts an unordered list using recusion instead Specifically it uses the DC Divide and Conquer approach to problem solving The process is as follows You pick a pivot a random array index Loop the array storing items less than the pivot Loop the array storing items greater than the pivot Assuming the less and greater collections are already sorted You can now return less pivot greater In reality youll use recursion to then sort both the less and greater arrays using the same algorithm This ends up being On in the worst case but can be On Log n in the better case The explanation for this is that the quicksort function takes in a single collection and loops over it In the process it then splits the collection into three chunks less pivot and greater and it recursively calls itself ie quicksort calls quicksort on the less and greater chunks subsequently looping over those collections as well Big O notation helps us to understand the hidden complexity of the algorithm If you see something like On then you know that there are nested loops or some kind of recursion happening in order to cause that Now its worth being aware that Quick Sorts performance is dependent on the pivot you choose Take a look at some of the example implementations of the quick sort algorithm in the reference list below Gist Algorithms in Python15 There youll see we have three implementations one where we pick the first index every time as the pivot one where we pick the middle index every time and one where we pick an index at random Picking an index at random will give you the best chance of high performance If you for example always pick the first index then youll have a situation where youre potentially sorting items unnecessarily So in the best case scenario if your collection is 10 items long then it would calculate as follows 10 4 Log10 2222 40 operations 01 40 4 seconds But in the worst case scenario if your collection was 10 items long then it could calculate as follows 10 10 100 operations 01 100 10 seconds Conclusion This has been a very basic introduction to the concept of Big O Hopefully youve found it useful and have a greater appreciation for what Big O offers in the way of understanding the performance of particular algorithms although weve only really looked at a very small selection Weve seen the following Big O types OLog n x log time On factorial time On linear time O1 constant time On n also known as On selection sort example On Log n x quick sort example There are many more algorithms and calculations for Big O and as I learn them Ill be sure to update this blog post accordingly If in the mean time you notice any mistakes then please feel free to let me know References Gist Algorithms in PythonhttpsgistgithubcomIntegralist9763bded76e7d826535a3caeafc3bdff Beginners Guide to Big Ohttpsrobbellnet200906abeginnersguidetobigonotation "},{"title":"Bits and Bytes","tags":["bits","bytes","ram","cidr"],"href":"/posts/bits-and-bytes","content":" Introduction1 Bit2 Byte3 RAM4 Bit and RAM Visualisation5 Bits and ASCII51 Bits and Numbers6 IPs7 Base Numbers8 Convert Base10 into Base2881 Convert Base10 into Base1682 Convert Any Base to Base1083 CIDR9 Conclusion10 Introduction So this is a bit of a random journey through some different computer based subjects Things that I felt I should try to better understand Some of it will be very basic but hopefully itll be useful to those who are new to tech and are interested in learning these things or old dogs like me who should know better Bit The word bit is short for binary digit A bit is either a 1 true or a 0 false Computers only understand the binary format ie base2 We discuss base numbers below8 Byte A grouping of eight bits is called a byte Read the next section to realise why I mention this tidbit of information RAM The word ram is an acronym for random access memory Its nonpersistent memory Meaning it is lost when your machine is restarted and persists only for the lifetime of the program using it RAM consists of bits but each segment of memory is actually a grouping of eight bits which we already know is called a byte So in short we would say RAM is made up of bytes Bytes are uniquely numbered to allow easy lookup of their contents A bytes unique number is also referred to as its address Bit and RAM Visualisation We can see that each bit has an associated value which is calculated using the power of base2 well cover base numbers8 shortly So 20 1 21 2 22 4 ie 2 2 23 8 ie 2 2 2 24 16 ie 2 2 2 2 25 32 ie 2 2 2 2 2 26 64 ie 2 2 2 2 2 2 27 128 ie 2 2 2 2 2 2 2 With this information we can identify the value assigned by adding up the numbers associated with the ones and zeros So in the image we can see four of the bits are given 1 on and the rest are 0 off meaning if we add up all the associated values of the bits that are on we get 99 eg 64 32 2 1 Basic Bit Operators You may have seen bit operators like and wondered what they mean generally referred to as bitwise operators Essentially they manipulate bits Here are some examples 64 1 32 1 3 8 1 7 128 1 1 results in the value 32 Similarly if we start with the bit value 1 and move to the left by 15 bits well get back the result of 32768 because although weve only shown eight bit spaces above you can just keep moving to the left 64 2 128 128 2 256 now keep going seven more places until you reach the fifteenth bit 256 2 512 512 2 1024 1024 2 2048 2048 2 4096 4096 2 8192 8192 2 16384 16384 2 32768 For more bitwise operators refer to these posts httpswikipythonorgmoinBitwiseOperators and httpsmediumcomlearningthegoprogramminglanguagebithackingwithgoe0acee258827 Bits and ASCII ASCII is a set of codes where each code point represents a textual character such as a 1 a z etc Each code point has an associated binary number For example a has the binary number 0110 0001 which if we add up the values associated with those specific bits well find itll yield the code point number 97 In ASCII the character a is code point 97 Bits and Numbers 1 kilobyte or 1KB is 1024 bytes 1024 bytes is 8192 bits 81928 or 81024 whichever you prefer The following explanation is taken from Beginning C by Apress Publishing You might be wondering why we dont work with simpler more rounded numbers such as a thousand or a million or a billion The reason is this there are 1024 numbers from 0 to 1023 and 1023 happens to be 10 bits that are all 1 in binary 11 1111 1111 which is a very convenient binary value So while 1000 is a very convenient decimal value its actually rather inconvenient in a binary machineits 11 1110 1000 which is not exactly neat and tidy The kilobyte 1024 bytes is therefore defined in a manner thats convenient for your computer rather than for you So if we add up 5122561286432168421 notice this takes the existing 8 bit calculation from the above image and continues it for another two bits we get 1023 IPs Here is an example IPv4 IP 2162761137 IPv4 IPs are expressed in decimal format Note IPv6 IPs are eight 4character hexadecimal numbers which represent 16 bits each for a total of 128 bits eg 20010db80a0b12f00000000000000001 To translate the above IP into binary form for the sake of a computer to process it we could use the above visualisation image to help us The end result of which would look like this 11011000000110110011110110001001 Which breaks down into 11011000 128 64 16 8 216 00011011 16 8 2 1 27 00111101 32 16 8 4 1 61 10001001 128 8 1 137 This explains why each of the four numbers within the decimal formatted version ie 2162761137 are called octets as they represent eight bits or a byte as we learned earlier when viewed in binary form This also explains why IPv4 IPs are considered 32bit numbers because if you add each of the bits together ie the number of total bits not the value assigned to each bit youll find there are a total of 32 bits that make up the IP Each bit can have two different states 1 or zero meaning the total number of potential combinations per octet can be either 28 or 256 Meaning each octet can contain a potential value between zero and 255 Meaning if we were to combine the four octets we could potentially have 4294967296 variations We can see the decimal represenation of an IPv4 IP is made up of four base10 numbers 216 27 61 137 Where each of those four numbers represent the binary equivalent 21611011000 2700011011 6100111101 13710001001 which is a base2 representation of a byte or octet If youre unsure of what base numbers are and how they work then read on Base Numbers Its worth quickly covering what base numbers are as they help us understand the other different protocols we use on a regular basis such as binary and things like IPs Any number can be represented in multiple ways using a different base numbering system There are many numbering systems but the typical ones were used to are Base 10 Decimal Base 2 Binary Base 8 Octal Base 16 Hexadecimal The standard number system we as humans are most familiar with is called base10 and it consists of the following numbers 0123456789 Notice there are ten numbers hence it is called the base10 system If we were to look at a number like 66 then this would tell us the number is made up of 6 tens and six units These numbers 09 represent whole numbers while in the base10 system we can also use a decimal point to represent decimal fractions of a number eg 12 Below is an image credit to Jenny Eather that helps us visualise this model The base number is the number of numbers within the system So base10 has 10 numbers 0123456789 whereas binary is base2 because it uses two numbers only 0 1 If you want to know the unit each number in a system represents well use base10 as the example thanks to the following visualisation credited to Jenny Eather then you calculate this using the power of the base number So as per the above visualisation we can see 103 101010 1000 thousands 102 1010 100 hundreds 101 10 10 tens 100 1 1 unit So in practical terms if you have a number like 75 and want to represent it as base10 5 100 5 units 7 101 7 tens Similarly if we had a number like 675 and want to represent it as base10 5 100 5 units 7 101 7 tens 6 102 6 hundreds You can indicate what base you wish to represent a number like so nb Where n is the number and b is the base you wish to state it is in For example 7510 This is the number 75 and were stating the base it represents is 10 This is useful when youve converted a number like 75 into a different base lets say base8 which would be 113 and you want to give that number in the proper context to someone else You could write it as 1138 Convert Base10 into Base28 Note the steps are the same for converting to base2 and base8 Now lets consider how to convert the number 75 into another base like base8 To do so follow these steps 1 divide the number 75 by the desired base 8 take note of the remainder 3 2 take the result 9 and do the same ie divide by the base and take note of the remainder 3 keep doing this until the result of dividing the previous answer by the base is zero 4 now write out the remainders bottom to top and thats the number in base8 In long form this looks like this 75 number 8 base 9 rounded with a remainder of 3 9 previous answer 8 base 1 rounded with a remainder of 1 1 previous answer 8 base 0 rounded with a remainder of 1 Meaning 75 evaluated in base8 would be 113 all the remainders concatenated together bottom up Convert Base10 into Base16 The algorithm for converting from base10 into base2 and base8 works basically the same for converting into base16 But there is one caveat whereby a remainder can be in the double digits and apparently for reasons I dont completely understand we dont want that and so the number system was designed to replace the six instances where this can occur the remainders being 10 11 12 13 14 15 with a alphanumeric equivalent 10A 11B 12C 13D 14E 15F So if we want to convert 11010 into a hexadecimal the outcome of the algorithm would be 110 number 16 base 6 rounded with a remainder of 14 6 previous answer 16 base 0 rounded with a remainder of 6 We know that we need to replace 14 a double digit remainder with the letter E see above mapping Meaning 110 evaluated in base16 would be 6E Lets try it again but with the number 41110 411 number 16 base 25 rounded with a remainder of 11 25 previous answer 16 base 1 rounded with a remainder of 9 1 previous answer 16 base 0 rounded with a remainder of 1 We know that we need to replace 11 with the letter B Meaning 411 evaluated in base16 would be 19B Convert Any Base to Base10 What if you want to convert a base8 number lets say 113 why not into base10 The algorithm is to multiple the individual numbers by their associated power of the base and then add the numbers together So here are the base8 powers 3 80 1 81 1 82 And here is the algorithm 3 x 80 3 1 x 81 8 ie 18 1 x 82 64 ie 18 8 3 8 64 75 If youre dealing with base16 then again its the same but the difference is youre translating the letter back into the corresponding number Lets convert 19B from base16 back into base10 B x 160 11 x 160 11 9 x 161 144 1 x 162 256 11 144 256 411 Lets try one more conversion between base16 to base10 The number is 1A4 4 x 160 4 A x 161 10 x 161 160 1 x 162 256 4 160 256 420 CIDR A CIDR is a range of IP addresses We can use our understanding of bits bytes and octets to understand the format of a CIDR A CIDR typically resembles something like 10000n Where n is given the value 8 16 24 or 32 and these represent each of the 8bit blocks that make up the IP If we want an IP range between 10000 and 10255255255 wed specify the CIDR as 100008 What 8 states is that the last 8 bits of the 32bit number is accounted for this being the 10 weve specified in our example Meaning the rest of the 8bit segments can be added up to their max of 255 meaning the last IP in this CIDR range would be 10255255255 Similarly if we want an IP range between 10000 and 100255255 wed specify the CIDR as 1000016 Again 16 states that the next 8bits segment of the 32bit number is now accounted for this being the 0 weve specified in our example 100 Meaning the rest of the 8bit segments can be added up to their max of 255 meaning the last IP in this CIDR range would be 100255255 And so on So 1000024 gives us an ip range of 10000 to 1000255 256 IPs Whereas 1000032 gives us an ip range of 1 ip 10000 to 10000 Note you can use a tool such as httpwwwipaddressguidecomcidrhttpwwwipaddressguidecomcidr to help you generate CIDRs We can use the earlier byte visualisationvisualisation table matrix to help us manually calculate a CIDR range Ive reproduced it below with a HTML table Note youll likely need to scroll to the right to see the start of the 32bit IP 10 0 0 1 8 Bit Blocks 8 bits 2431 8 bits 1623 8 bits 0815 8 bits 0007 32 Bit 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00 Decimal 128 64 32 16 8 4 2 1 128 64 32 16 8 4 2 1 128 64 32 16 8 4 2 1 128 64 32 16 8 4 2 1 Binary 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 Conclusion There you go A whirlwind ride through different basic computer tech topics As always if Ive gotten anything wrong then just let me know on twitter "},{"title":"Building Systems With Make","tags":["bash","build","make","shell","utility"],"href":"/posts/building-systems-with-make","content":" Introduction1 Simple Example2 Terminology3 Prerequisites4 How Make Decides What To Do5 Automatic variables6 Commands7 Targets As Prerequisites8 Accessing Targets9 Parsing Targets And Prerequisites10 Dynamic Targets11 Dereferencing Variables and Macros12 Functions13 Filter14 Shell15 Eval16 Files17 UserDefined Functions18 Conventions19 Revisiting The For Loop Example20 Includes21 Conclusion22 Introduction Most web developers use a build tool of some sort nowadays Im not refering to continuous integration software like Jenkins CIhttpjenkinsciorg a very popular build system but the lowerlevel software it uses to actually acquire dependencies and construct your applications with There is a dizzying array of options to choose from Apache Anthttpantapacheorg XMLbased Rakehttpdocsseattlerborgrake Rubybased Grunthttpgruntjscom JSbased Gulphttpgulpjscom JSbased Broccolihttpsgithubcombroccolijsbroccoli JSbased NPMhttpswwwnpmjscom JSbased Good ol shell scripts although no real orchestration around it The build tool I want to look at in more detail here though is the granddaddy of them all Makehttpwwwgnuorgsoftwaremake Originally designed back in 1976 Make is the leading build utility for Unix Linux and Mac OS X Chances are most computers you log in to will already have it installed and available to use This really reduces the setup entry point which for other tools listed above can be tedious and error prone with the exception of shell scripts as the shell is something inherently available for all systems My hope is for you to see that Make is an automationorchestration tool that can be used in place of other modern build tools and will help to strengthen your understanding and ability to use the terminalshell environment which is a big plus in my opinion and helps open up many avenues of technical progression I couldnt hope to cover every aspect of what Make offers so please dont mistakenly consider this post as anything even remotely exhaustive Whole books have been written on the topic of Make and writing Makefiles so Ill leave it up to you to investigate further beyond this post if Ive managed to kindle your interest Let me start by referencing the GNU websitehttpwwwgnuorgsoftwaremake for its definition of what Make is and does GNU Make is a tool which controls the generation of executables and other nonsource files of a program from the programs source files Make relies on a Makefile being defined and which consists of a set of instructions for building your software If youve used another build system such as Grunthttpgruntjscom youll notice that most of them use a naming convention taken from Make eg Gruntfile The point of a Makefile in the traditional sense is to build a program although Make can be used to run any kind of task and so it isnt limited to compiling software Much like how other JavaScriptbased build tools arent limited to building JavaScript applications they can handle most tasks you wish to run maybe compiling CSS or optimizing images Youll find Make is widely distributed and is likely already on your computer For example Im using an Apple laptop with Mac OS X installed If I run the following command make version I get back the following response GNU Make 381 Copyright C 2006 Free Software Foundation Inc This is free software see the source for copying conditions There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE This program built for i386appledarwin1130 Which means I already have the make command available and I can start writing my Makefile right away Simple Example Lets consider a standard project requirement which is to run a linter such as JSHinthttpwwwjshintcom over a JavaScript file that is analyze the code for formatting issues and general errors and warnings Note as mentioned earlier traditionally Make is used to compile program files In this instance Ive opted for a simple example that doesnt require compilation but should instead demonstrate how Make is actually useful for many different types of task Imagine you have a testjs file and it contains the following content function foo bar baz If we were to execute the command jshint testjs shownonerrors assuming you have the CLI version of JSHint installed then we should see something like the following displayed testjs line 2 col 14 Missing semicolon 1 error testjs Implied globals bar 2 Unused Variables foo1 So we can see from this output that JSHint is warning us that we have a function foo thats not being used and a variable that appears to have been declared globally but it also indicates that we have an error in our program were missing a semicolon from line 2 in our JavaScript file OK great so how do we take this example further and automate the analysis process which will get more complicated as our application grows in size and features using the Make utility First we need to create a Makefile Below are the contents of the Makefile Im going to use to demonstrate how Make works Ill explain the structure of the file in the next section lint jshint js shownonerrors Note Makefiles use tabs instead of spaces so if your editor is set up to replace spaces with tabs then you could find things dont work as expected To run the Makefile above we would need to use the make shell command This by itself will run the first target it finds this is also referred to as the default target which in this case is lint You can also be more explicit and specify the exact target you want to execute by providing the name of the target to the make command like so make lint Executing the above command is the same as running jshint testjs shownonerrors Youll also have noticed we used a wildcard to indicate multiple JavaScript files at once In this instance using Make means its easier to remember specific commands for common tasks such as this Having to remember the format of the JSHint shell command is now not an issue especially considering that Im using the most bare bones example of running JSHint and the shell command itself can become much longer and unwieldy The Makefile also acts as a documented file that can now be committed into version control meaning we now have a record of the compilation step Both these points become even more important as the compilationbuild steps become more and more complicated which they will as your application or software system naturally grows and evolves Note if your Makefile is in a different directory you can pass its location to the make command using the f flag like so make f The convention for writing Makefiles is to have the default command your entry point at the top of the file and have Make process the commands from the top down You dont have to do this though as youll see Ive not really worried about it with the examples throughout this post and youre free to put your rules in whatever order makes sense to you But be aware that when you call the Make command youll want to specify the specific target if its not the default Terminology There are three key phrases you need to be aware of when talking about a Makefile Rules Targets Prerequisites The following snippet demonstrates the basic structure of a Makefile target prereq1 prereq2 commands You can see we have a single target this is what we reference when running the command make a set of dependencies ie prerequisites and a command to execute eg jshint testjs shownonerrors This entire structure is collectively referred to as a rule and a Makefile is typically made up of multiple rules Prerequisites Prerequisites are the dependencies for the target What this means is that the target cannot be built successfully without the dependencies first being resolved Imagine were compiling Sass into CSS An example Makefile which well look at in more detail shortly could look like compile fooscss sass fooscss foocss In the above example we specified the prerequisite as being fooscss meaning Make will either look for a target called fooscss or expect a file to exist in the current directory structure We dont have a target named fooscss and so if that file also didnt exist then we couldnt resolve the dependency and subsequently the rule would fail if it cant resolve the dependency then the command in the rule wont be executed How Make Decides What To Do How and why Make decides what to do when you run make is very important as itll help you understand the performance implications of certain tasks The rule of thumb for Make is pretty simple if the target or any of its prerequisite files are out of date or missing then the commands for that target will be executed Make uses the modification timestamp to avoid duplicate processing If the timestamp of the dependent files is older than the resulting output then running Make wont do anything Hence you can force Make to recompile a file by simply using the touch command on the relevant files Note if you want to see what Make will execute without it actually doing anything then run the make command as you normally would but ensure you include the n flag This will cause Make to print out all commands that would be executed including commands collated from any specified prerequisites Automatic variables Lets consider another example whereby we want to compile a Sasshttpsasslangcom style sheet into CSS compile fooscss sass fooscss foocss We have some slight duplication here the reference to fooscss We can clean this up a bit by using some special variables that Make provides also referred to as automatic variables Specifically for the problem we want to solve well be using the automatic variable When the compile target is run the variable will reference the first prerequisite in the list which will simplify the example and save you from having to repeat yourself The following example demonstrates what this looks like compile fooscss sass Note because Makefiles have their own special syntax the use of will conflict when writing our shell script which also has its own special syntax around This means if we want to use the dollar character and not have it be Makefile specific then we have to escape it using another dollar So rather than writing i which works fine within the context of a normal shell script weve had to write i instead Well see a few different automatic variables throughout this post but in the meantime check out the quick reference list below for some of the more useful ones first prerequisite list of prerequisites list of prerequisites that have changed target name the value of a target placeholder The full reference of automatic variableshttpwwwgnuorgsoftwaremakemanualhtmlnodeAutomaticVariableshtmlAutomaticVariables is available on the GNU Make website Later on in this post well revisit this for loop example and demonstrate a more idiomatic way to achieve the result we want Commands Its worth being aware that each command provided inside the overall rule is considered a separate shell context This means if you export a shell environment variable in one command it wont be available within the next command Once the first command has finished a fresh shell is spawned for the next command and so on Youll also notice that when running Make it will print out the command instructions before executing them This can be disabled in one of three ways You can either run Make with the s flag which will silence any output or you can use the syntax before the command itself like so list footxt bartxt baztxt for i in do echo Dependency i done The third way to silence output is to use the SILENCE flag The following snippet demonstrates how to silence three targets foo bar and baz SILENT foo bar baz Note silencing the output unfortunately also means silencing any errors Much like shell scripting if you have a command that is more complicated than what can feasibly fit on a single line then for the sake of readability if nothing else youll need to write it across multiple lines and escape the line breaks using the character as the following example demonstrates list footxt bartxt baztxt for i in do echo Dependency i done Targets As Prerequisites So far our prerequisites have been physical files that already existed But what if you need to dynamically create the files first via other targets Make allows you to specify targets as dependencies so thats not a problem Lets see how this works in the following example foo echo foo foofiletxt bar echo bar barfiletxt baz foo bar echo baz cat foofiletxt barfiletxt bazfiletxt Note Make typically uses the convention of naming targets after the files they create This isnt a necessity but its generally considered good practice What we have are three targets foo bar and baz The first two have no dependencies of their own and all they do is generate a new text file The last target baz specifies the other two targets as its dependencies So when we run make baz we should see no output as weve used the special syntax to silence any output but we should find we have the following files created foofiletxt barfiletxt bazfiletxt The last file in the list should contain not only a line that displays baz but also two other lines comprising the contents of the other files So running cat bazfiletxt should print baz foo bar Note if youve not seen it used before the in the cat command is telling it to expect input from stdin the echo command writes to stdout and that is piped over to the cat command as stdin Accessing Targets In the above example I was generating a file based on the contents of two other targets which themselves dynamically generated some files There was a slight bit of repetition that could have been cleaned up if we used another automatic variable provided by Make specifically The variable is a reference to the target name so lets see how we can use this with our previous example foo echo filetxt bar echo filetxt baz foo bar echo cat foofiletxt barfiletxt filetxt In the example above weve saved ourselves from typing foo bar and baz a few times but weve not eradicated them completely as we still have to reference foo and bar as prerequisites as well as referencing them from within the baz command itself With regards to the baz command we could use along with some shell scripting to clean that up so were again not relying on hardcoded values The following example shows how to achieve that foo echo filetxt bar echo filetxt baz foo bar filesecho sed E saz1filetxtg echo cat files filetxt Oh boy OK So yes weve removed some more hardcoded values but unless youre supremely confident with shell scripting then Im guessing the above refactor wont make much sense to you But lets break it down a bit so we can see what we have We use to get the list of dependencies in this case foo bar We pipe that over to the sed command We also use the extended regular expression engine E to make our regex pattern easier to understand The sed command replaces foo bar with foofiletxt barfiletxt We do that replacement within a subprocess which is a special shell syntax This means we have to escape the dollar sign within the Makefile The values returned from the subprocess foofiletxt barfiletxt are then stored in a variable called files and we reference that variable in place of the original hardcoded values On top of all that we still have duplication the foo and bar referenced within the prerequisites area That has to be hardcoded unless were going to use Make or some other form of shell scripting to dynamically generate the actual Makefile itself which even for me is a step too far in this case OK so what does this ultimately tell us That simplicity is the key The reason I went to all this trouble is it allowed me to demonstrate first how to really stretch what Make can do for you if you have enough shell scripting knowledge and second to allow me to now demonstrate how you can use more idiomatic Make to simplify the code and avoid overengineering like the previous example baz foofiletxt barfiletxt echo cat filetxt filetxt echo In this refactored version we define a target called baz and we set its dependencies to be two files that dont exist We also dont have any defined targets in our Makefile either To solve this problem we use a virtual rule one that uses Makes placeholder syntax to pattern match against Well see the syntax in more detail shortly but for now it will suffice to know that it acts like a wildcard When we run make baz Make will try to resolve the two dependencies The following rule filetxt will then match both foofiletxt and barfiletxt and so the command echo will be executed twice The command takes the dynamic part of the rule the foo and bar parts and makes them available via We write those two values into which is the target name in this case foofiletxt and barfiletxt and subsequently create those two files Weve now resolved the baz rules dependencies and we can move on to executing its command which completes the requirements as weve already seen Parsing Targets And Prerequisites There are many different automatic variables available for Make and well see a few more of them as we go along But as weve already discussed and its worth noting that you are also able to parse the specific directory and file name details for the first dependency and the target by using the syntax DF for the prerequisite and DF for the target Using the following snippet as an example you would run it with make foobarbaztxt bingboptxt do nothing foobarbaztxt bingboptxt echo D echo F echo "},{"title":"Calculating Big-O","tags":["design","algorithms","go","golang","big-o","performance","complexity","time"],"href":"/posts/calculating-bigo","content":" Introductionintroduction Algorithmalgorithm Analysis Stepsanalysissteps Explanationexplanation Significant or Insignificantsignificantorinsignificant Example Analysisexampleanalysis When is BigO not relevantwhenisbigonotrelevant Introduction This post includes a condensed version of the information gleened from the excellent interactivepythonorghttpinteractivepythonorgrunestonestaticpythondsAlgorithmAnalysisBigONotationhtml section on algorithm analysis I strongly recommend you read that if you require more detailed information The purpose of this post is simply to restructure and simplify the principles offered so I can use it as a quick reference for future practice Algorithm py def sumOfNn theSum 0 for i in range1n1 theSum theSum i return theSum printsumOfN10 55 Analysis Steps You want to quantify the number of operations or steps in the algorithm To do this Identify the basic unit of computation Track any operational constants eg theSum 0 Track repeatable operations eg theSum theSum i Identify the most dominant portion Explanation Think about our algorithm sumOfN we have n number of items and were concerned about the time complexity If we decide the basic unit of computation is variable assignment then the formula we would use to express this is T1n If T1n is the size of the problem then using sumOfN as our algorithm we can evaluate it to mean T1n 1n steps The 1 is a constant ie theSum 0 happens only once and n is the number of iterations we carry out where a single assignment is made ie theSum theSum i within the sumOfN function A critical part of understanding time complexity is that as the problem gets larger a portion of T1n is likely to overpower the rest and this is where we use the syntax f to represent that portion of the algorithm Instead of T1n we could say the dominant part is fn and is also referred to as being the order of magnitude which is what the O in BigO stands for Note order of magnitude describes the part of Tn that increases the fastest as n increases We can represent the order of magnitude in BigO syntax like so Ofn Where fn dominant part of Tn Typically well not include the f part of the syntax when using BigO though So instead of Ofn well just say On Significant or Insignificant As n gets larger continuing to use Tn 1n as our example the constant 1 ie the computation that happened once theSum 0 becomes less and less significant Meaning we can drop 1 from our syntax resulting in just On instead of O1n and our approximation is just as accurate without it To clarify this further Im going to paste verbatim the interactivepython description as I feel they explain this very well As another example suppose that for some algorithm the exact number of steps is Tn 5n2 27n 1005 When n is small say 1 or 2 the constant 1005 seems to be the dominant part of the function However as n gets larger the n2 term becomes the most important In fact when n is really large the other two terms become insignificant in the role that they play in determining the final result Again to approximate Tn as n gets large we can ignore the other terms and focus on 5n2 In addition the coefficient 5 becomes insignificant as n gets large We would say then that the function Tn has an order of magnitude fn n2 or simply that it is On2 Example Analysis py a 5 b 6 c 10 for i in rangen for j in rangen x i i y j j z i j for k in rangen w ak 45 v bb d 33 The above code can be calculated as Tn 3 3n2 2n 1 Which can be condensed slightly by combining the singular constants to Tn 3n2 2n 4 The constants 3 and 1 are the top level variable assignments a5 b6 c10 and d33 The 3n2 is because there are three constant variable assignments x y and z hence the 3 in 3n2 that are occurring within the first set of for statements The top level for statement iterates over n items and then does so again hence the n2 portion of 3n2 The 2n is because there are two constant assignments w and v and they happen n times due to the last for statement iterating over n items With this in mind we can say the code is On2 because when we look at the exponents of each segment of the time analysis ie the condensed version 3n2 2n 4 we can see that as n grows the n2 portion is the most significant Think about it looping over n items and making two assignments within each iteration which is the 2n is definitely less complexity than looping over n items twice and within each iteration making three assignments which is the 3n2 Remember although we write BigO as O the underlying principle is Of where f is the dominant part of T and when focusing in on the dominant part of the time complexity we drop the constants also known as the coefficient eg 3n2 thus becomes n2 This is because the constants become insignificant as n grows When is BigO not relevant I recently asked in a computer science forum for help in understanding what the BigO time complexity would be for a web crawler Specifically I was asking in relation to the following crawler implementation I had built gowebcrawlerhttpsgithubcomintegralistgowebcrawler The architecture of the program looks something like this I wasnt sure how to calculate the BigO for this program because there didnt seem to be any one unit of computation that made sense to use as the foundation of the algorithmic analysis In the earlier examples it was the variable assignment but in a web crawler there are so many different moving pieces that make up the whole program Also the implementation between web crawlers will determine different outcomes So based on my implementation the running time to handle a list of size n containing nested sublists of size x appears to be Onx I had not seen this type of BigO analysis before and is an indication of maybe Im analysing the wrong things Note see my previous post for common examples of BigOpostsalgorithmiccomplexityinpython So how did we come to Onx Heres the breakdown Im iterating over each list there are n of them and each contains x items so you iterate over nx items in all The amount of work per item appears to be constant ie O1 it doesnt appear to depend on n or x Multiplying those together we see that the total time to handle that list is Onx The feedback I received was that BigO might not be useful for analysing a system such as mine because BigO analysis ignores the constant factors where as for systems code we often care about the constant factors a lot In other words I was looking at the system from too high a view I was looking at the whole rather than picking a specific sub implementation of a particular algorithm Another issue is that not all operations are the same and yet BigO treats them as such For example a variable assignment is not as intensive computationally or time based as a network request that can suffer latency and require context switching etc So in that case BigO analysis isnt useful for understanding the performance of the system in practice So no it doesnt make sense to use BigO all the time Itll only make sense from a smaller algorithmic perspective These are all useful things to keep in mind when thinking about BigO time complexity analysis "},{"title":"Clean Coder","tags":["clean","responsibility"],"href":"/posts/clean-coder","content":" Who is Robert C Martin Whats it all about Take responsibility Work ethic Know your field Stay current Collaborate Mentoring Know your domain Identify with your employer Learn to say no Rules and principles for coding Acceptance Tests Managing time Meetings Scrum Sprint Planning Sprint Retro Discussions Code Estimations Pressure Conclusion Who is Robert C Martin Robert C Martin is a legend in the development industry Commonly referred to as simply Uncle Bob He is a software consultant and has been programming since the mid 60s Over the years he has written many books some listed below as well as shared his great knowledge of system design and coding best practices Designing ObjectOriented C Applications Patterns Languages of Program Design Extreme Programming in Practice Agile Software Development Principles Patterns and Practices UML for Java Programmers Clean Code A Handbook of Agile Software Craftsmanship The Clean Coder A Code of Conduct for Professional Programmers The last book in that list is the focus of this post Whats it all about Well Ive taken the liberty to reference the author directly here as I feel his words convey the books focus more than I ever could I presume you just picked up this book because you are a computer programmer and are intrigued by the notion of professionalism You should be Professionalism is something that our profession is in dire need of Im a programmer too Ive been a programmer for 42 years longer by the time you read this and in that time Ive seen it all Ive been fired Ive been lauded Ive been a team leader a manager a grunt and even a CEO Ive worked with brilliant programmers and Ive worked with slugs Ive worked on hightech cuttingedge embedded softwarehardware systems and Ive worked on corporate payroll systems Ive programmed in COLBOL FORTRAN BAL PDP8 PDP11 C C Java Ruby Smalltalk and a plethora of other languages and systems Ive worked with untrustworthy pay check thieves and Ive worked with consummate professionals It is that last classification that is the topic of this book In the coming years I would be fired from one job for carelessly missing critical dates and nearly fired from still another for inadvertently leaking confidential information to a customer I would take the lead on a doomed project and ride it into the ground without calling for the help I knew I needed I would aggressively defend my technical decisions even though they flew in the face of the customers needs I would hire one wholly unqualified person saddling my employer with a huge liability to deal with And worst of all I would get two other people fired because of my inability to lead So think of this book as a catalog of my own errors a blotter of my own crimes and a set of guidelines for you to avoid walking in my early shoes Take responsibility A professional takes responsibility for his code and his decisions His primary focus should be to do no harm Dont harm the code Here is a shortlist of things a professional will do Ensure that QA Quality Assurance find nothing wrong If there are bugs then a professional will take note of them how they occurred and aim to prevent it from happening again A professional doesnt rely on others to catch bugs for them Know how all the code works even the bits they didnt write Understand all of the codebase and how the different components interlock The only way a professional can apply an appropriate solution is to know the environment its limitations and constraints Write tests unit acceptance and will ensure they are automated A professional who writes tests first before code will be more confident in adding new features and refactoring existing code as they have the tests to back them up as they make changes especially if the tests are automated then the redgreenrefactor cycle can be tightly integrated into the professionals work flow 100 code coverage or at least in the 90 range TDD Test DrivenDevelopment means that the code design allows every line of code the professional writes to be easily testable Do not harm the codes architecturestructure A professional will keep in mind the following Software should be easy to change Making a change to the code base should be easy If it isnt then the professional will look to change the design of the code to accommodate this requirement so the code becomes more flexible and allows changes to be made more easily in future Any code the professional reads over or makes a change to will be analysed for any potential improvements to the codes structure this is known as merciless refactoring or the boy scout rule In other words always commit back a module in a better state than you originally checked it out Make continuous changes to the code base Constantly changing the code ie improving it should result in greater flexibility and modularity If you leave the code static eg youre afraid to change it in case it breaks something then when management requests a change or new feature you may likely find the code is too rigid to be adapted Dont be afraid to constantly change your code Your tests are there to back you up and should be quick to run Work ethic Your career is your responsibility do not leave it up to your employer to train you send you on courses or buy you books Take control of yourself The time you spend at work should be spent on your employers problems not yours A professional works hard for his employer and makes time for his career in his own time This also doesnt mean you should spend all your time on your career You have a familylife too Balance your work your career and your life in appropriate measures Know your field Professionals not only know their chosen favourite technologies but have an understanding and interest in other technologies outside of that circle Professionals will know past techniques and why they may no longer be relevant as well as the current best practices Professionals will also know the pros and cons to specific techniques and when to choose one over the other Uncle Bob makes a comment which references the Santayanas curse Those who cannot remember the past are condemned to repeat it Here is a short list of things you should know or at least be quite familiar with Design patterns GOF 24 patterns Design principles SOLID DRY SRP DIP Methods of working Agile Scrum Kanban Waterfall Disciplines TDD BDD OOP CI Pair Programming Artifacts UML sequence diagrams flow charts decision tables Stay current Practice Doing your job isnt practice Your job helps you improve your current skills practice is learning new related skills Professionals will participate in Code Katas These arent about solving problems because as a professional they would have already solved the problems presented in the kata The purpose of code katas for a professional is to help warm them up for the days work and as a warm down at the end of the day It allows them to try out different options for refactoring the solution and finding other shortcuts Collaborate A professional will plan design and write code together with their teamcoworkers This helps open their knowledge up to new ways of thinking and solutions they may not have considered otherwise This also facilitates faster learning and making fewer errors Mentoring Nothing demonstrates more how much you know about a subject than when you try to teach it to some one else Juniors will ask all sorts of questions you may have long forgotten about such as why you no longer build something in a certain way or using a certain technique Professionals look after juniors and dont let them struggle on without help and direction Know your domain A professional will research the business they are now building software for So if you work for a travel agent then you should know a bit about that industry You dont need to be an expert just know enough to recognise concerns with certain technical requests or specifications Identify with your employer Effectively your employers problems are your problems and so you need to take their perspective and understand the features they want implemented A professional will be in a better position to advise whether those new features are actually right for the business Learn to say no If your boss tells you to implement a feature by a certain date and you know that cant happen without compromising the quality and stability of the software then be vocal and say no thats not possible A professional wouldnt flat out say No and walk away they would suggest a date that the work could be completed by or possibly a compromise in the features that would be implemented by the deadline date A professional will work with management to agree on whats achievable but wont settle for intimidation because management has tried to enforce an unrealistic request As a professional you were hired to carry out a specific specialised job and its your responsibility to educate management on whats feasible or not Do not find yourself agreeing to impossible deadlines or saying Ill try Ill try is usually interpreted by management as yes Generally developers will agree to try because they want to avoid confrontations but it suggests that the work you were doing previously wasnt actually your all What does Ill try actually mean any way That youll work longer hours or work weekends No of course not So why tell management youll try when youre just going to go ahead and do what you was planning on doing before It will just let them down when you dont succeed to finish by the deadline because management will have interpreted your Ill try as I will Rules and principles for coding A professional has a specific set of principles when it comes to how they work Professionals are prepared always focused and understanding Professionals dont write code when tired or distracted as this just results in code that doesnt work or will just need to be rewritten later Professionals avoid the zone This sounds controversial as the zone is what most developers think gives them their edge That sweet moment where everything seems to be flowing just right But the zone just tricks you into thinking the speed youve gained processing problems means they are really efficient They arent You lose the bigger picture when you get into the flow So dont buy into it Be focused Professionals dont code while listening to music You arent as productive as you think you are This is something Ive personally experienced and agree with Working in silence may seem boring or too solitary but I do my best work when the background noise disappears Professionals handle interruptions politely and be willing to help regardless Have you ever given an agitated response to someone who has interrupted you while working Your snappy response could be because youre trying to figure out a complicated problem that requires total concentration maybe you were trying to get into the zone But a professional knows how to politely handle the situation and return to the problem at hand Professionals know that writers block can usually be solved or alleviated by pair programming Professionals know to produce creative output they need creative input Itll be different for each individual but find out what gets your creative juices flowing and make sure you indulge that as often as possible to allow your more creative output to flow Professionals avoid time fixing bugs No other profession would stand for bugs in this way too long could you imagine a doctor who made mistakes in diagnosis on every patient he wouldnt last in this profession very long so a software professional should aim to reduce debugging time to zero Professionals arent afraid to go home when there is a bug they cant solve They recognise their creative periods and so know when to take a break Youll find that your subconscious will figure out the solution while your brain takes a rest so remember that its ok to walk away and take a break Professionals know that being late for a deadline is a fact of life and unavoidable But a professional will constantly measure their progress and inform those who need to know as early as possible if theyre not going to make the deadline Dont let people down at the last minute Professionals dont give false hope that a deadline will be met if there genuinely is no chance Better to formulate a fallback plan or agree what can be achieved Stick to your estimates dont let a manager try to convince you to do what it takes to make the deadline Professionals dont build a wall around their code Any developer should be allowedable to make changes to other developers code Professionals understand that pair programming is a good thing in that it helps you learn the system find bugs faster and share knowledge more efficiently Acceptance Tests Acceptance tests help avoid ambiguity in feature requests and how the existing system functions They are different from unittests which test individual components of the code in that they ensure the finished application functions how the business expects it to without the business having to understand the technical aspects of the system These types of tests should be cowritten by a stakeholder a programmer to ensure clarity and accuracy and they should always be written so they can be executed automatically Acceptance tests provide the entire team with the definition of done eg code written tests passing code deployed QA satisfied Professionals should never be passiveaggressive when it comes to acceptance tests By this we mean if youve been asked to implement a test that you think is faulty then it is your responsibility as a professional to make the business understand why the test is faulty and to come to an agreement on what an acceptable test should be So dont act passiveaggressively in a well if this is what they wanted then this is what theyll get way by allowing a faulty test to pass Managing time A professional knows how to manage their time Meetings They dont go to every meeting they are invited to just because theyre invited They only go to those meetings that affect the immediate job theyre working on The people who invite you to meetings arent responsible for managing your time you are Be wise about the time you give up as it otherwise will mean your work wont get done A good manager will do what they should to keep you out of meetings If a meeting gets boring leave Its unprofessional to stay in a meeting where your input is not needed Be polite and excuse yourself Only agree to attend meetings where a clear agenda has been determined Scrum Participates in a Scrum standup meeting should spend no more than 20 seconds per question what did I do yesterday what am I doing today whats blocking me Sprint Planning Sprint Planning sessions are a notoriously difficult part of Agile to get right But effectively estimates for backlog items should already be done ready for selectionrejection Ideally acceptance tests will also have already been written No more than 10mins should be spent reviewing any item If more time is needed then another meeting should be scheduled with a subset of the team to discuss further Sprint Retro Sprint Retro should only take 20mins and your sprint demo should only take 25mins For most Agile teams it will only have been two weeks since your last retro so there shouldnt be that much to talk about Discussions Any argument that lasts longer than 5mins can not be settled by arguing Get data to back up your opinions and to solidify a decision Code Avoid blind alleys A blind alley is where you choose a technical solution and stick with it even though later on you realise its not the right choice Learn to stop digging that hole Back out and try something different Avoid messes A mess is like a time black hole that sucks you in Every step you think youre closer to the finish and it seems to be better than starting over Dont fall for it Blind alleys and messes have very subtle differences but basically both should be recognised as early as possible so you can escape them Estimations A professional will try not to estimate a job by themselves Any one who does this should expect to go over by at least 3 times as long as their original estimate Professionals will estimate with the help of their team as they know someone on the team may highlight an issue the others including yourself have missed or not considered If appropriate try breaking up a task into smaller tasks and estimating each smaller task In doing so youll likely find that the total sum of your estimate is larger than your original estimate would have been for the whole thing ie before you split the task into smaller chunks Pressure A professional knows to stay calm under pressure A clear head is essential No good comes from pent up frustration at management for tight deadlines or poor software when a build breaks just before going live A professional resorts to discipline and training to get through these situations Ultimately by handling your job and your responsibilities professionally youll be able to avoid pressure by avoiding situations that cause pressure Conclusion This has been a bit of a whirlwind tour of The Clean Coder A Code of Conduct for Professional Programmers but hopefully youve learnt a few things about how to handle yourself in a professional manner and will want to pick up the book to get the full benefit of Uncle Bobs experience and advice I cannot recommend this book enough I can see it being a useful tool for not only developers but for managers as well because although the book is primarily focused on people in the software engineering industry the information and tips are still relevant crossdiscipline "},{"title":"Client Cert Authentication","tags":["bash","certificates","curl","docker","https","keys","nginx","openssl","ssl"],"href":"/posts/client-cert-authentication","content":" Introduction1 Directory Structure2 Configuration3 Building4 Running5 Verifying6 Revocation7 References8 Conclusion9 Introduction The purpose of this post is to demonstrate how to configure nginx to use client certificates for authenticated access to your backend service in this example a RubySinatra application Note the focus of this post isnt about RubySinatra so dont worry if your backend service is built with another programming language I wont be going into the details of how most of it was setup as the majority of this was already documented in a previous blog post here Setting up nginx with Dockerhttpwwwintegralistcoukpostssettingupnginxwithdocker so I recommend reading through that first if youre new to Docker and nginx I will be showing most of the files but Ill assume youre familiar with nginx and Docker The changes involved for setting up client authentication is actually very minimal and in reality the majority of the work is in the creation of a CA CRL and signing certificates If you need a refresher on TLSSSL then please read Security basics with GPG OpenSSH OpenSSL and Keybasehttpwwwintegralistcoukpostssecuritybasics which covers the SSL handshake process and a lot more So lets get started Directory Structure First things first were going to need the following set of files and folders dockerapp Dockerfile Gemfile Gemfilelock apprb dockernginx Dockerfile certs cacrt cakey clientcrt clientcsr clientkey servercrt servercsr serverkey nginxconf html indexhtml testhtml You can get most of this structure from the following GitHub repository IntegralistDockerExamplesNginxClientCertAccesshttpsgithubcomIntegralistDockerExamplestreemasterNginxClientCertAccess I say most because the dockernginxcerts folder no longer exists in the repo This is OK because Ill demonstrate how to generate these files in the following sub sections of this article The reason the dockernginxcerts folder no longer exists is due to the last portion of this article where by we switch to another format for generating certificates and selfsigning specifically the RevocationCRL Management7 section Configuration As far as configuration is concerned the main part comes down to the nginxconf file user nobody nogroup workerprocesses auto events workerconnections 512 http upstream app server app4567 server listen 443 ssl on servername sslcertificate etcnginxcertsservercrt sslcertificatekey etcnginxcertsserverkey sslclientcertificate etcnginxcertscacrt sslverifyclient on root usrsharenginxhtml location app proxypass httpapp proxysetheader XClientCertDN sslclientsdn As we can see were telling nginx to listen to any interface on port 443 TLS connections only We then enable SSL and specify a few different ssl settings that direct nginx to certain locations where it can find the servers own certificate and private key as well as the CA certificate that was used to sign both the server certificate as well as the userclient certificate well be using shortly to connect to this service Youll probably also notice sslverifyclient has been turned on This could be made optional so that some connections are allowed to public endpoints But in my case I want everything to be protected by client certs Finally when we proxy traffic onto our backend service we also create a new custom HTTP header to be proxied on as well which Ive named XClientCertDN The value assigned to this custom header uses the nginx sslclientsdn variable which extracts the Common Name section of the clients certificate Interestingly the Ruby server receives the HTTP request with the custom header transformed into HTTPXCLIENTCERTDN Just something to be aware of if you decide to switch from Ruby to another programming language such as Gohttpsgolangorg as your mileage may vary Generating the certificates and keys So the first thing we want to do is to create the CA keycertificate which will be used for signing both the server and the client certificate requests openssl genrsa des3 out cakey 4096 openssl req new x509 days 365 key cakey out cacrt For the cacrt generation I pretty much entered which means no value for all details The only exception was the Common Name field which I entered TheCA so I could recognise it as the ca just in case I needed to inspect the certificate Note all of these commands I ran inside of the dockernginxcerts folder to make it easier later on to mount them as a volume into my Docker containers Next well create the server key along with a CSR Certificate Signing Request which the CA will use to generate the servers certificate openssl genrsa out serverkey 4096 openssl req new key serverkey out servercsr For the CSR I pretty much entered which means no value for all details The only exception was the Common Name field which I entered TheServer so I recognise it as the server just in case I needed to inspect the certificate Note I dont specify des3 in the command as I dont want to generate a passphrase for the private key If I have to restart my server I dont want the automation to be affected by requiring me to manually enter the passphrase Now well selfsign the servers CSR and generate its own certificate in case its not clear selfsigning isnt something you want to do unless you know you are going to ask your users to trust your certificate and ignore big warnings about an unknown CA signing the servers cert openssl x509 req days 365 in servercsr CA cacrt CAkey cakey setserial 01 out servercrt OK almost there We now need to create a private key and CSR for our client ie this will be the user trying to access the service openssl genrsa out clientkey 2048 openssl req new key clientkey out clientcsr Youll notice Ive made the encryption length 2048 instead of 4096 I did this as a speedperf compromise as higher length encryption keys can be slower to use with the TLS handshake process just one of the many security compromises that need to be considered For the CSR I pretty much entered which means no value for all details The only exceptions were the Common Name field which I entered Mark McDonnell so I recognise it as the client and the Email Address field which I entered something like markintegralistcom as I want to parse out that email in my Ruby application Note again I dont specify des3 in the command as I dont want to generate a passphrase for the private key Finally I sign the client CSR using the CA certificate openssl x509 req days 365 in clientcsr CA cacrt CAkey cakey setserial 01 out clientcrt Thats it thats all the different certificates setup and ready to be used Lets move onto building our Docker setup and running some containers Building This sections nice and short because I provide the Dockerfile for both the Ruby and nginx applications just make sure you cd back up into the projects root directory before executing the following commands docker build t myrubyapp dockerapp docker build t mynginx dockernginx Running This section is also nice and short First lets run the Ruby application docker run name rubyapp p 45674567 d myrubyapp Now lets run the nginx container docker run name nginxcontainer v pwdhtmlusrsharenginxhtmlro v pwddockernginxcertsservercrtetcnginxcertsservercrt v pwddockernginxcertsserverkeyetcnginxcertsserverkey v pwddockernginxcertscacrtetcnginxcertscacrt v pwddockernginxnginxconfetcnginxnginxconfro link rubyappapp P d mynginx OK so the nginx docker run command was a little bit more involved but really all its doing is mounting volumes from the host machine my Mac into the Docker container The most important items to be aware of are the certs were mounting into the container as well as the nginxconf file Youll also notice Im linking the running Ruby container to the nginx container This is important because it allows nginx to utilise the Ruby container as a backend service Verifying Now the containers are built and running we should verify that the services themselves are doing what they should be But before we do that its worth me mentioning now that when I reference and below youll need to swap these references for actual values To get the value for Im using the following dockermachine ip dev This gives me the ip address of my running docker VM If youre running on Linux then localhost 127001 would suffice But as you can see Im running things on a Mac and so Im using dockermachine with a VM named dev Your mileage will ultimately vary To get the value of Im using the following to access the dynamically allocated port number docker port nginxcontainer 443 awk F print 2 Where nginxcontainer is the name I gave to my container when executing the docker run command earlier So lets demonstrate some different application routes that should all FAIL for reasons Ill explain as we go along curl http So in the above example we should see the following error as the HTTP protocol was used instead of HTTPS remember that nginx was setup to only listen on 443 not 80 400 The plain HTTP request was sent to HTTPS port 400 Bad Request The plain HTTP request was sent to HTTPS port nginx146 Ubuntu Now lets try and switch to HTTPS curl https The above attempt should also error as the servers cert isnt trusted ie its selfsigned To fix this we can use the insecure flag curl insecure https The above attempt should now fail because no client certificate was provided for purpose of authentication with nginx Great So now were verified all the failing paths lets consider curl commands that should return us some actual working content But before we do that a slight intermission "},{"title":"Authentication with AWS Cognito","tags":["authentication","aws","cognito","jwt"],"href":"/posts/cognito","content":" Introductionintroduction What is Cognitowhatiscognito Authentication vs Authorizationauthenticationvsauthorization User Pools vs Identity Poolsuserpoolsvsidentitypools Implementation Optionsimplementationoptions Client SDKclientsdk Server SDKserversdk AWS Hosted UIawshostedui Stateless Authenticationstatelessauthentication Logic Processing with AWS Lambdalogicprocessingwithawslambda Beware the Lambdasbewarethelambdas Useful Lambdasusefullambdas Social Loginssociallogins Overloading the State Parameteroverloadingthestateparameter Scopescope JWTsjwts API Limitsapilimits Logout Issueslogoutissues Other Concernsotherconcerns Which is the right solutionwhichistherightsolution Updated Architectureupdatedarchitecture Native Mobile Social Signinsnativemobilesocialsignins User Pool Configurationuserpoolconfiguration IAM Useriamuser Lambda IAM Rolelambdaiamrole Example Cognito App Settingsexamplecognitoappsettings Example Cognito User Pool Federation Identity Providersexamplecognitouserpoolfederationidentityproviders Example Facebook App Configurationexamplefacebookappconfiguration Example Google App Configurationexamplegoogleappconfiguration Terraform Exampleterraformexample Conclusionconclusion Introduction In this post I would like to introduce you to the AWS Cognitohttpsawsamazoncomcognito service and to explain its various moving pieces and how they fit together If youre interested in a very highlevel view of what I was working on then this architecture diagramupdatedarchitecture should give you the basic idea Effectively I codesigned and implemented a new authentication system using AWS Cognito for BuzzFeeds existing community users to utilize and which opened the doors for new BuzzFeed services to also be able to offer additional features built upon authentication to their users Cognito is tricky to get up and running with for a variety of reasons which Ill explain as we go and to make things worse there arent many reference points outside of the official documentation to help you Hence this blog post now exists for those weary travellers looking for answers Note this post was written approximately five months into a year long project and so a lot has changed in the design of the system and the implementation But this post is still very relevant and useful for those looking to understand Cognito This post also was fed back to various internal AWS teams and has resulted in work being carried out to improve various aspects of their services mentioned here Lets start at the beginning What is Cognito According to the official blurbhttpsawsamazoncomcognito Amazon Cognito lets you add user signup signin and access control to your web and mobile apps quickly and easily In essence Cognito provides features that let you authenticate access to your services while also providing features to let you authorize access to your AWS resources Authentication vs Authorization Its important to clarify that in this blog post were only really discussing authentication and not authorization They are two different concepts Authentication is the process of verification that an individual entity or website is who it claims to be Authorization is the function of specifying access rights to resources which is different to and commonly confused with the process of authentication Note if youre new to these types of security concepts then take a look at this glossary documenthttpsdocsgooglecomdocumentd1qs3jEIQvocdVhSxCSPLF1BoLnp91aLnuUIasvlmaYoedit I put together which covers the various terminology User Pools vs Identity Pools In order for you to be able to authenticate and authorize access Cognito provides two separate services 1 User Pools 2 Identity Pools User Pools deal with authentication whereas Identity Pools deal with authorization and specifically that means AWS based resources only For the purposes of this post Ill only be focusing in on User Pools as our project requirements did not involve authorizing access for AWS resources to an authenticated user which is where Identity Pools would typically come into play Now with that said and what makes this oh the more confusing due to the design of the mobile SDKs mobile applications do utilize Identity Pools for authentication but the Identity Pool would be configured with a provider which happened to be our User Pool If youre interested in the various Identity Pool concepts then please refer to the official documentationhttpsdocsawsamazoncomcognitolatestdeveloperguideauthenticationflowhtml Implementation Options There are fundamentally three options available for implementing User Pools 1 Client SDK 2 Server SDK 3 AWS Hosted UI Client SDK The client SDK has a bit of a jagged history which makes reading the AWS docs a bit confusing at times or indeed when Googling for help as you may notice references to Amazon Cognito Identity SDK for JavaScripthttpsgithubcomamazonarchivesamazoncognitoidentityjs which is now a deprecated library What youll want to use instead is their new Amplifyhttpsawsgithubioawsamplify SDK which youll also find AWS has a strong bias towards or at least their solution architects push it really hard Note I get the feeling AWS put a lot more time into Amplify and having it be able to abstract away a lot of the Cognito complexity that theyre keen for consumers to utilise it Based on this I decided I would trust their opinion and just try and spin up something that works using Amplify which unfortunately took a long time and ultimately I ended up dropping the work in favour of a serverside solution I dont keep up with the constant changes to the JavaScript landscape and so Im not familiar with React or Angular which were the two examples the AWS docs and most example repos used the majority of the time So using Amplify required me to first do some reading up on React Babel WebPack and a whole host of other tools It was painful In the end we just had too much trouble trying to deal with Node and the various build systems that we decided to drop the work we had done and pivot to a new solution see next section Server SDK The serverside solution we chose was to use the Python SDKhttpsboto3readthedocsioenlatest This ended up being a bit of a double edged sword We were happier with the move to Python but we really struggled with both the AWS documentation and also the boto3 library documentation that the Python SDK is built upon In order to get up and running we initially opted to use a 3rd party abstraction library called Warranthttpsgithubcomcaplesswarrant which also incidentally helped us to understand the AWS documentation because we were able to reverseengineer the Warrant code to better understand the boto3 API calls that needed to be made Note I think that says a lot about AWS documentation If people need to read through how an abstraction library is using your API then your documentation must be pretty bad I would be the first to suggest maybe Im just too dumb to understand Cognito but a lot of people across the internet were having the same problems Ultimately Warrant didnt provide all the functionality we needed and so we eventually refactored out Warrant and were back to using the underlying boto3 Python SDK Its worth me taking a moment to also explain that some APIs require you to define a specific type of authentication flow which is a security feature and as far as I understand it is supposed to help you to more safely access data provided by these APIs What I didnt know originally and was one of the reasons we decided to use a library such as Warrant was that the code involved with some of these auth flows can be quite complex I still now struggle to follow exactly what the code does within Warrant when it uses one of these flows Just to give you an example of the type of code AWS Cognito would expect you to write take a look at the InitiateAuthhttpsdocsawsamazoncomcognitouseridentitypoolslatestAPIReferenceAPIInitiateAuthhtml API call with the USERSRPAUTH auth flow First of all I dont think its very clear what is expected to be provided in that documentation alone but also take a look at Warrants implementationhttpsgithubcomcaplesswarrantblobmasterwarrantawssrppy and specifically how to generate an SRPA which also doesnt appear to be explained anywhere no where obvious at least Note it wasnt until much later we discovered that we could in the case of InitiateAuth at least have avoided writing all the SRP generation code and instead used the admin version of that API called AdminInitiateAuthhttpsdocsawsamazoncomcognitouseridentitypoolslatestAPIReferenceAPIAdminInitiateAuthhtml which allows you to skip SRP in favour of implicitly trusting the caller which was fine for our use case as we were building a centralized authentication API service AWS Hosted UI AWS Cognito offers a hosted ui where by you redirect a user to an endpoint such as httpsauthuseast1amazoncognitocomloginresponsetypeclientidredirecturistate Note a custom domain can also be configured but it requires you use AWS Certificate Managerhttpsawsamazoncomcertificatemanager for the TLS cert The hosted ui option gives you all the interactions in a fully functioning interface which includes signin signup forgotten username forgotten password social logins But there are some caveats Very limited controls over the ui very basic font colors and css Custom domains only work with TLS certificates via ACMhttpsawsamazoncomcertificatemanager State parameter overloading Cant access new signup passwords this was necessary for my use case as I needed to cosupport a legacy system that wasnt ready to migrate over to Cognito There are other issues still that I have with the hosted ui but in a lot of cases it does the job well enough to put up with them The state parameter overloading is an interesting issue and Ill come back to that later on when I discuss a little bit about signins with social providers Stateless Authentication The thing we liked about Cognito was that it would allow us to build a stateless authentication system Due to the use of JWThttpsjwtios we could pass these tokens around and know that if the user had these tokens that they would be valid and untampered with because when decoding the tokens we could verifiy this using the public signing key AWS uses to sign the tokens at point of generation we only ever pass tokens around serverside using secure cookies with HttpOnly and Secure attributes set to avoid replay attacks that might occur if we exposed the tokens to the client The problem we then stumbled across was what happens if a user authenticates on a public computer but doesnt log out or they authenticate on their laptop but have no password to prevent someone from stealing it and thus their existing authenticated session tokens Well we would still decode the ID JWT we got back from Cognito to ensure there was no tampering of the token but we would then make a simple API call to AWS specifically the GetUserhttpsdocsawsamazoncomcognitouseridentitypoolslatestAPIReferenceAPIGetUserhtml as it required us to provide the ACCESS JWT we get back from Cognito The reason I mention this is because we needed a way to invalidate a session and the only way to do that was to call the GlobalSignOuthttpsdocsawsamazoncomcognitouseridentitypoolslatestAPIReferenceAPIGlobalSignOuthtml API We originally though that would invalidate all tokens IDAccessRefresh but we were wrong Only the Access and Refresh tokens are invalidated But that was fine as our system was already being passed the Access token and so once we invalidated the session if that user tried to reuse the tokens at one of our protected endpoints we could be sure that it would now fail to give them access as we not only verified the ID token but attempted to use the Access token to call an AWS API to see if it suceeded or not Our use case is probably quite unconventional but otherwise the whole point of having a stateless system was in danger of being made redundant by the fact that if for whatever reason we had a set of compromised users wed otherwise have no way to invalidate their sessions and we didnt want to have to build our own session state datastore to track all this Logic Processing with AWS Lambda With the hosted ui option youll likely also need to utilise AWS Lambdahttpsawsamazoncomlambda in order to do some logic processing The following diagram demonstrates how we were initially using the hosted ui 1 We redirect an unauthenticated user to Cognito 2 Once the user attempts to signin we trigger some additional hookshttpsdocsawsamazoncomcognitolatestdeveloperguidecognitouseridentitypoolsworkingwithawslambdatriggershtml 3 Cognito redirects the authenticated user to our API service 4 Our API service redirects the user back to the CMS with user tokens 5 The CMS asks the API service to validate the tokens this service exchanges the given Cognito auth code for the users Cognito User Pool tokens Once the tokens are validated the CMS will allow the user to view the relevant page Beware the Lambdas Its worth noting the second half of the above diagram the section after the lambda is triggered What we have there are two separate lambdas and which one is triggered depends on the scenario If the user has tried to authenticate using a usernamepassword set of credentials and those dont match an existing user within the Cognito User Pool then the User Migration lambda is triggered In that lambda we attempt to authenticate the user within our legacy system that is the call over to the WebApp in the diagram If the authentication with the legacy system is successful then well modify the users User Pool record which hasnt actually been created yet to include auth related details weve pulled from their legacy account We then return the event object provided to Lambda which lets Cognito know it can now create the user within its User Pool not returning the lambda event object indicates an error occurred and the whole request flow fails Note with the user migration for users from our legacy system over to Cognito before we return the event in the lambda we make sure to mark the new Cognito user as verifiedconfirmed that way they dont need to enter a verification code that gets emailed or sent via SMS thats because the user wouldve already verified themselves originally in our legacy system The reason I say beware the lambdas is because yes code errors can cause it to bomb out but more importantly they dont always fire when you think they will this is a user error thing not an AWS bug To clarify let me explain what we saw when testing the migration path of a legacy user account to Cognito when the user was signing into Cognito using their social provider details We had hoped the User Migration lambda hook would have been triggered by both a Cognito User Pool account login and also a Social Provider account login but it doesnt Note when a user signsin with a social account they have an account created within the Cognito User Pool but they are also added to a specific group such as a Facebook group or a Google group We eventually discovered that the Post Confirmation hook would fire at the right interval for us to do the processing we needed for users signing in with a Social Account But that wasnt immediately obvious Before settling on the Post Confirmation hook we originally started using Post Authentication for handling first time social logins the hook sounded reasonable enough but when we were testing this hook we already had the social account stored in our User Pool this was from earlier testing before we decided to do some postlogin processing The reason I mention this is because a week later we decided to clear out our User Pool and start testing our various scenarios again from scratch and we noticed the post authentication hook was no longer firing Turns out social accounts only trigger post migration hooks when they already exist in the User Pool In order to do the first time login modification we were looking for we needed the post confirmation hook Using this hook wasnt obvious to us because post confirmation makes it sounds like an event that happens once a usernamepassword user has entered their verification code for the first time and thus become marked as confirmed within the User Pool Well turns out social provider logins are automatically considered confirmed once they authenticate for the first time hence why that event would trigger when we needed it to Useful Lambdas There are some useful lambdas though for example the Custom Message Lambda Triggerhttpsdocsawsamazoncomcognitolatestdeveloperguideuserpoollambdacustommessagehtml is great for intercepting the emails or SMS messages that are sent to your users and allowing you to configure them however you like Take a look at the following code for an example def lambdahandlerevent context domain httpsyourdomaincom username eventgetuserName code eventrequestgetcodeParameter printevent if eventtriggerSource CustomMessageSignUp eventresponseemailSubject Validate your account eventresponseemailMessage Hi username Thank you for signing up Click here to validate your account elif eventtriggerSource CustomMessageForgotPassword eventresponseemailSubject Reset your password eventresponseemailMessage Hi username Click here to reset your password elif eventtriggerSource CustomMessageUpdateUserAttribute eventresponseemailSubject Validate your new email eventresponseemailMessage Hi username Click here to validate your new email if eventtriggerSource CustomMessageAdminCreateUser userattr eventrequestgetuserAttributes userstatus userattrgetcognitouserstatus if userstatus FORCECHANGEPASSWORD eventresponseemailSubject Validate your account eventresponseemailMessage Hi username You recently attempted to signin but your account is still unverified Your temporary password is code Click here to complete account validation return event Whats good about this lambda is that were able to improve the users flow a little bit Otherwise if we relied on AWS to generate the emailSMS wed have to create a separate UI that allowed for example when verifying an account using a code the user to copy paste their code into the UI and then submit that code to our server to process By controlling the email content ourselves we can construct an endpoint that has the verification code as a query param and make a GET request to an endpoint that will process that code for the user saving them from having to manually enter anything Just something to consider when using Cognito can I use lambda triggers to improve the user flow Social Logins One thing that might not be clear when opting for a serverside solution is how to handle social logins eg users signing inup using facebook or google It might sound a bit strange but in order to implement social logins youll need to make a call to the hosted ui endpoint mentioned earlier httpsauthuseast1amazoncognitocomloginresponsetypeclientidredirecturistate The specific endpoint you call will be based upon those supported in Cognitos User Pools Auth APIhttpsdocsawsamazoncomcognitolatestdeveloperguidecognitouserpoolsservercontractreferencehtml For example to attempt to signin a user with facebook you would provide a button that links to httpsauthuseast1amazoncognitocomoauth2authorizeresponsetypeclientidredirecturistateidentityprovider The value we use for the responsetype parameter is code What this does once the user has authenticated with their social provider defined by the identityprovider param is redirect the user back to your service specified via the redirecturi param and then your service is responsible for exchanging the code for the users User Pool Tokens see the following section on JWTsjwts The values you can assign to identityprovider are Facebook Google LoginWithAmazon Note if you were planning on handling authentication at a very low level instead of an SDK then for a User Pool login you would provide the value COGNITO Overloading the State Parameter The state param is used for CSRFhttpswwwowasporgindexphpCrossSiteRequestForgeryCSRF protection and is the only parameter that is persisted when the user is redirected to redirecturi A common problem for people using Cognito is that they need more than one redirect In my case see the earlier hosted ui architecture diagram I need to redirect the signedin user to an API service so we can handle the exchanging of the AWS code for the Cognito User Pool tokens before needing to then redirect the user back to our actual origin service The only way we can do this is to overload the state param so it has a value like state123redirecthttpswwwexamplecom The value 123 is the nonce for CSRF and the gives us a way to split the query param serverside to extract the secondary redirect endpoint Note its recommended you do validation on that input eg a whitelist of accepted URIs so hackers cant manipulate the endpoint a user is sent to once theyve authenticated Scope One thing I stumbled across and which took a while to figure out was when I tried to call the GlobalSignOuthttpsdocsawsamazoncomcognitouseridentitypoolslatestAPIReferenceAPIGlobalSignOuthtml API operation It worked fine for users authenticated against the Cognito User Pool but not for users authenticated via their social provider Turns out I needed to enable the right scope within the Cognito User Pool UI console within App Integration App Client Settings and under Allowed OAuth Scopes awscognitosigninuseradmin needed to be ticked But also when making the request to the Auth API endpoint eg oauth2authorize I needed to append a scope query parameter scopescopeopenidawscognitosigninuseradmin See the API docshttpsdocsawsamazoncomcognitolatestdeveloperguideauthorizationendpointhtml and the UI docshttpsdocsawsamazoncomcognitolatestdeveloperguidecognitouserpoolsappidpsettingshtml for more information on the reasoning JWTs When you exchange the cognito code for a user pool token youll actually be returned three tokens 1 ID token 2 Access token 3 Refresh token Note see documentation for more details on these three tokenshttpsdocsawsamazoncomcognitolatestdeveloperguideamazoncognitouserpoolsusingtokenswithidentityprovidershtml The ID token provides details about the user and the access token indicates the access allowed to that users attributes stored within the Cognito User Pool Both the ID token and access token will expire after one hour To use them after that youll need the refresh token to refresh the accessid tokens for another hour The refresh token expires after 30 days We use the ID token for verifying the user is authenticated and we do this by passing the token to an internal service that verifies the token hasnt been manipulated by checking it against the AWS JWKhttpsdocsawsamazoncomcognitolatestdeveloperguideamazoncognitouserpoolsusingtokenswithidentityprovidershtmlamazoncognitoidentityuserpoolsusingidandaccesstokensinwebapi that cryptographically signed the token The JWK is a set of keys that are the public equivalent of the private keys used by AWS to digitally sign the tokens We acquire these via a standard format endpoint httpscognitoidpregionamazonawscomuserPoolIdwellknownjwksjson Note the JWKs are rotated every 24hrs approx and so you need to ensure if youre caching the response your code gets a fresh copy of the JWK You can check this by inspecting the CacheControl header set on the JWK response API Limits One issue we stumbled across recently was the API limits which meant we couldnt make any further API requests and for an indeterminate amount of time Seems there is a Cognito API limitshttpsdocsawsamazoncomcognitolatestdeveloperguidelimitshtml reference page but its still unclear how long you have to wait before you can start making requests again Logout Issues AWS provides a logouthttpsdocsawsamazoncomcognitolatestdeveloperguidelogoutendpointhtml endpoint that when visited allows a user to clear any session tracking AWS might have on their browser This is different to the signout API functionality in that the user can call the logout endpoint without any special tokens Note you have to provide quite specific query params eg clientid and logouturi so AWS can redirect back to a preconfigured logout page that you host and so its likely youll want to wrap that long and ugly URL within an click to logout link This endpoint is useful because if you have a social user in your user pool and you delete that user something we would do regularly while we were testing in our stage environment you would find that AWS is tracking your browser via session and so if you tried to signup or signin again using that same social user you would get an invalid grant exception back from AWS this was super confusing behaviour and took a long time to figure out So for our use case we would have a social user eg facebook or google signin for the first time Cognito would once the user had authenticated with their social provider automatically create a social user account in our user pool BUT we have other postprocessing steps and if any of those fail we want to delete the social user and tell the user what went wrong But again we need to now call the logout endpoint so if that user decides they want to try and signup again they dont get a more confusing invalidgrant error message thrown at them and we dont have to translate that message into something that shouldnt even be a concern for them in the first place This brings us to the problem we have with the logout endpoint Our intention for when we were getting an error from an AWS API operation would be to catch the error then make a request to logout and have it redirect to a failure page we host the redirection is a builtin feature that AWS provides via a logouturi query param Our logouturi value a URL would itself have a query param specified of errmsg so that our failure page could use that param to indicate the original error to the user Problem was in Cognito you can only specify a logout url that matches whats predefined in your user pool Meaning if we wanted to redirect to httpsexamplecomfailureerrmsgfoo then thats exactly what needs to be defined in Cognito even down to the query param We didnt realise this and we only had httpsexamplecomfailure defined as a valid logouturi To solve this issue we could have explicitly specified the errmsg param but we have lots of error types to handle and so it wasnt practical to list each and every variant URL So to work around the fact that we cant specify a query param because there are too many value variants to be practical for us to explicitly list all of them in the UI we now catch the original API error and redirect the user to our failure page with the errmsg param passed so we can indicate the error back to the user At this point we still need to call the AWS logout endpoint so we can clear any session tracking So along with the error message we display to the user we also show a message to say we will redirect them automatically via JavaScript back to another part of our site in N seconds time enough time for the user to read the error were displaying Before we redirect the user to that page we first redirect them to the AWS logout endpoint this time specifying our failure page as the value for the logouturi query param but just without an errmsg query param provided Our failure page is configured with a conditional check that says if an errmsg param is passed then show error page with the JS redirect to logout otherwise if no errmsg provided just immediately redirect to our signin page So thats how were resolving this issue currently Its not elegant for sure but it works Other Concerns Its worth me mentioning a few other concerns we had the first of which wasnt directly related to Cognito but was specific to the system we were designing and that was atomic operations We had to make changes to Cognito and then sync some behaviours back to our legacy system If there was a network or other fault then we needed both systems to be tolerant and to undo any data modifcations in case of failure Other than that we would be storing off the JWT tokens we received from AWS into secure cookies Secure HttpOnly attributes and this caused us issues because our cookies needed to be scoped to the right domains to prevent overloading the HTTP request header limit Due to the JWTs being large in size and the fact that BuzzFeed has many servicesproperties for users to interact with we noticed that some users would end up seeing a 400 Bad Request caused by the large cookies The routing services in front of these upstreams are generally nginx instances and so we would use largeclientheaderbuffers to allow an increased size until such a time we could figure out an appropriate solution Which is the right solution The answer it depends For me the serverside solution made the most sense and although difficult in the beginning primarily due to documentation and general misunderstandings about the difference between User Pools and Identity Pools we found it worked the best for our requirements and gave us the most flexibility Updated Architecture If youre interested the updated architecture looked something like this None of the listed services are public theyre all internal The API Gateway is an internal tool that allows upstreams such as the buzzfeedauthapi to control concurrency and rate limiting of downstream consumers such as buzzfeedauthui and usersettings The reason we migrated certain user settings functionality out of our monolithic webapp and not other user features is because we only wanted to move behaviours that interacted with fields that needed syncing between Cognito and our legacy datastore As times goes on well start to migrate more and more functionality out into separate services Native Mobile Social Signins We discovered that the social signin for native mobile apps doesnt work as well as the web SDKs Mobile apps need to instead do things differently as their SDKs arent as integrated like web User Pool Configuration As far as the User Pool is concerned youll need a few things Note this is based on a serverside solution Application Client this will generate a client id and secret which your applications will need to use when making certain API calls Federated Identity Providers this is where you tell Cognito about your social providers facebook google etc IAM User some API calls require AWS credentials accesssecret key so youll need to create an IAM user and define the various Cognito APIs you want it to have access to even if you opt for the hosted ui solution youll still need an application client for two reasons Firstly youll configure which providers you want your client app to support and this will affect what the hosted ui will display to your users Secondly the client app id is used as part of the hosted ui uri meaning you can have different hosted uis all configured slightly differently IAM User The IAM user is necessary as we have to provide some credentials to the boto clienthttpsboto3readthedocsioenlatestreferenceservicescognitoidphtml botohttpsboto3readthedocsioenlatestreferencecoreboto3html is the Python SDK in order for it to make certain API calls Below is an example of the code to instantiate a client client boto3clientcognitoidp awsaccesskeyid accesskey awssecretaccesskey secretkey regionname region Notice the service name is cognitoidp and not cognitoidentity I mention this as the docs specify two different services CognitoIdentity and CognitoIdentityProvider which when we were first learning about Cognito we presumed the latter CognitoIdentityProvider was something associated with Cognito Identity Pools As we were only interested in the User Pool functionality we found it strange that the small number of examples we found online all referenced cognitoidp So we struggled for a bit to understand the difference and although we used the CognitoIdentityProvider service ie cognitoidp we were confused for a long time as to why that was the case Turns out that Cognitos User Pool is itself fundamentally a identity provider idp and because of that you can configure a Identity Pool to have a User Pool associated within it along with more common external identity providers such as Facebook and Google So with that understanding firmly in place the fact the SDK uses cognitoidp for interacting with a User Pool makes total sense because the User Pool is an idp and the Identity Pool is just a tool for handling identities via many different providers whether that be a User Pool or a social provider such as Facebook or Google and so the SDK using cognitoidentity for interating with AWS Identity Pools also makes perfect sense Its the little details that can really make a difference to even the simplest aspects of using an SDKAPI and why Amazons atrocious documentation is a real detriment to its users Lambda IAM Role Below is an example IAM role policy you can use for AWS Lambda if youre using the hosted ui option and need lambda for logic processing Version 20121017 Statement Effect Allow Action logsCreateLogGroup logsCreateLogStream logsPutLogEvents Resource arnawslogs Effect Allow Action cognitoidpAdminUpdateUserAttributes Resource arnawscognitoidpuseast1awsaccountiduserpooluserpoolid It simply sets up CloudWatch logs access and allows us as an admin to update user attributes within our User Pool Note if youre copying and pasting dont forget to update awsaccountid and userpoolid in the code snippet Example Cognito App Settings This isnt meant to be an exhaustive example but it gives you an idea of some of the configuration youll need Callback URLs httpsauthapiexamplecomauthsignincallback Sign out URLs httpsauthapiexamplecomauthsignout Allowed OAuth Flows Authorization code grant Implicit grant Allowed OAuth Scopes email openid profile awscognitosigninuseradmin Example Cognito User Pool Federation Identity Providers For each provider there is a Authorize Scope section Facebook publicprofileemail Google profile email openid Facebook Attribute Mappings fb id userpool Username fb email userpool Email fb name userpool Name Google Attribute Mappings google email userpool Email google name userpool Name google sub userpool Username Example Facebook App Configuration httpsdevelopersfacebookcomapps App Domains httpsyourorganisationauthuseast1amazoncognitocom Privacy Policy URL httpswwwexamplecomaboutprivacy Site URL httpsyourorganisationauthuseast1amazoncognitocomoauth2idpresponse Product Added Facebook Login Client OAuth Login Yes Web OAuth Login Yes Enforce HTTPS Yes Valid OAuth Redirect URIs httpsyourorganisationauthuseast1amazoncognitocomoauth2idpresponse httpsauthapiexamplecomauthsignincallback Example Google App Configuration httpsconsoledevelopersgooglecom Enabled APIs Google API Credentials Type OAuth client ID Application Type Web application Authorized JavaScript origins httpsyourorganisationauthuseast1amazoncognitocom Authorized redirect URIs httpsyourorganisationauthuseast1amazoncognitocomoauth2idpresponse httpsauthapiexamplecomauthsignincallback Terraform Example Examples for Cognito User Pools can be found here httpsgithubcomterraformprovidersterraformproviderawsblobmasterexamplescognitouserpoolmaintf maintf TODO split this file up into separate modules eg userpool identitypool provider aws region varawsregion assumerole rolearn varawsrolearn resource awscognitouserpool pool name varenvironmentvarnameuserpool aliasattributes email preferredusername phonenumber autoverifiedattributes email phonenumber admincreateuserconfig allowadmincreateuseronly false container for the AWS Lambda triggers associated with the user pool httpswwwterraformiodocsprovidersawsrcognitouserpoolhtmllambdaconfiguration lambdaconfig custommessage awslambdafunctioncustommessagelambdaarn mfaconfiguration OPTIONAL smsconfiguration externalid varenvironmentvarnamesnsexternalid snscallerarn awsiamrolecognitosnsrolearn passwordpolicy minimumlength 6 requirelowercase false requirenumbers false requiresymbols false requireuppercase false email was a required field but it ended up causing issues for any social users whose identity is actually their mobile number So to avoid problems authenticating those users we no longer require an email to be provided schema name email attributedatatype String developeronlyattribute false mutable true required true stringattributeconstraints minlength 1 maxlength 2048 schema name somecustomattribute attributedatatype Number developeronlyattribute false mutable true required false numberattributeconstraints minvalue 1 maxvalue 50000000 tags environment varenvironment service varname dependson awsiamrolecognitosnsrole resource awscognitouserpoolclient poolclient Federation Identity providers dependson awscognitoidentityproviderfacebookprovider awscognitoidentityprovidergoogleprovider General settings App clients userpoolid awscognitouserpoolpoolid name varenvironmentvarnameuserpoolclient generatesecret true refreshtokenvalidity 30 explicitauthflows ADMINNOSRPAUTH USERPASSWORDAUTH this flag is automatically set to true when creating the user pool using the AWS console however when creating the user pool using Terraform this flag needs to be set explicitly allowedoauthflowsuserpoolclient true issue httpsgithubcomterraformprovidersterraformproviderawsissues4476 readattributes email preferredusername profile customsomecustomattribute writeattributes email preferredusername profile customsomecustomattribute App integration App client settings supportedidentityproviders COGNITO Facebook Google callbackurls varcallbackurls logouturls varlogouturls allowedoauthflows code allowedoauthscopes awscognitosigninuseradmin email openid profile aws cert configured in certstf resource awscognitouserpooldomain pooldomain domain vardomainvarrootdomain certificatearn awsacmcertificatecertificatearn userpoolid awscognitouserpoolpoolid bug in httpsgithubcomterraformprovidersterraformproviderawsissues4807 that keep showing changes in plan resource awscognitoidentityprovider googleprovider userpoolid awscognitouserpoolpoolid providername Google providertype Google providerdetails authorizescopes profile email openid clientid vargoogleproviderclientid clientsecret vargoogleproviderclientsecret attributemapping username sub email email bug in httpsgithubcomterraformprovidersterraformproviderawsissues4807 that keep showing changes in plan resource awscognitoidentityprovider facebookprovider userpoolid awscognitouserpoolpoolid providername Facebook providertype Facebook providerdetails authorizescopes publicprofileemail clientid varfacebookproviderclientid clientsecret varfacebookproviderclientsecret attributemapping username id email email The identity pools are used by our mobile apps and allows them to authenticate their users via our Cognito user pool Note were not sure if we need to configure anything else in facebookgoogle uis were also not sure what serversidetokencheck set below really means resource awscognitoidentitypool appsidentitypool identitypoolname varenvironmentvarnameidentitypool allowunauthenticatedidentities false cognitoidentityproviders clientid awscognitouserpoolclientpoolclientid providername cognitoidpuseast1amazonawscomawscognitouserpoolpoolid serversidetokencheck false supportedloginproviders graphfacebookcom varfacebookproviderclientid accountsgooglecom vargoogleproviderclientid dependson awscognitouserpoolpool an identity pool used by mobile apps requires a role to be assigned to both authenticated and unauthenticated access even if the identity pool is configured to not allow unauthenticated access it still requires a role to be assigned httpswwwterraformiodocsprovidersawsrcognitoidentitypoolrolesattachmenthtml resource awsiamrole appsidentitypoolauthenticated name varenvironmentvarnameidentitypoolauthenticated assumerolepolicy EOF Version 20121017 Statement Effect Allow Principal Federated cognitoidentityamazonawscom Action stsAssumeRoleWithWebIdentity Condition StringEquals cognitoidentityamazonawscomaud awscognitoidentitypoolappsidentitypoolid ForAnyValueStringLike cognitoidentityamazonawscomamr authenticated EOF resource awsiamrole appsidentitypoolunauthenticated name varenvironmentvarnameidentitypoolunauthenticated assumerolepolicy EOF Version 20121017 Statement Effect Allow Principal AWS arnawsiam000000000000root Action stsAssumeRole Condition Bool awsMultiFactorAuthPresent true EOF we can then attach additional policies to each identity pool role resource awsiamrolepolicy appsidentitypoolauthenticated name varenvironmentvarnameidentitypoolauthenticatedpolicy role awsiamroleappsidentitypoolauthenticatedid policy EOF Version 20121017 Statement Effect Allow Action mobileanalyticsPutEvents cognitosync cognitoidentity Resource EOF we dont allow unauthenticated access so just set all actions to be denied resource awsiamrolepolicy appsidentitypoolunauthenticated name varenvironmentvarnameidentitypoolunauthenticatedpolicy role awsiamroleappsidentitypoolunauthenticatedid policy awslambdafunction awsiamrole awscognitouserpool So to avoid that we could have made the policy not depend on that specific user pool resource using arnawscognitoidp but we opted to create a separate policy which we then attach to the existing role and tell the policy it cant be attached until the user pool has been created resource awsiamrolepolicy cognitolambdapolicy dependson awscognitouserpoolpool name senduseremailpolicy role awsiamroleiamforlambdaid policy EOF Version 20121017 Statement Action logsCreateLogGroup logsCreateLogStream logsPutLogEvents Effect Allow Resource arnawslogs Action cognitoidpAdminUpdateUserAttributes Effect Allow Resource awscognitouserpoolpoolarn EOF resource awsiamrole iamforlambda name varenvironmentvarnamesendUserEmailLambdaRole assumerolepolicy EOF Version 20121017 Statement Action stsAssumeRole Principal Service lambdaamazonawscom Effect Allow EOF data archivefile generatecustommessagelambda type zip sourcedir pathmodulesource outputpath lambdazip resource awslambdafunction custommessagelambda filename lambdazip functionname varenvironmentvarnamecustomMessages role awsiamroleiamforlambdaarn handler custommessagelambdahandler sourcecodehash dataarchivefilegeneratecustommessagelambdaoutputbase64sha256 runtime python36 this resource allows lambda to be invoked by our user pool and tripped us up initially because it is automatically applied when setting up the lambda trigger in the AWS console however when creating the lambda trigger via Terraform this needs to be set explicitly resource awslambdapermission allowcognito statementid AllowExecutionFromCognito action lambdaInvokeFunction functionname awslambdafunctioncustommessagelambdafunctionname principal cognitoidpamazonawscom sourcearn awscognitouserpoolpoolarn certstf resource awsacmcertificate certificate domainname vardomainvarrootdomain validationmethod DNS tags environment varenvironment service varname outputstf output userpoolid value awscognitouserpoolpoolid output userpoolarn value awscognitouserpoolpoolarn output userpoolclientid value awscognitouserpoolclientpoolclientid output userpoolclientsecret this is only shown at creation value awscognitouserpoolclientpoolclientclientsecret output appusername value awsiamusercognitoappusername output appuserarn value awsiamusercognitoappuserarn output acmcertificatearn value awsacmcertificatecertificatearn output acmcertificatedomainname value awsacmcertificatecertificatedomainname output acmcertificatedomainvalidationoptions value awsacmcertificatecertificatedomainvalidationoptions requiredtf terraform No value within the terraform block can use interpolations The terraform block is loaded very early in the execution of Terraform and interpolations are not yet available requiredversion 0107 serviceiamtf resource awsiamgroup cognitoappgroup name varenvironmentvarnamegroup resource awsiamuser cognitoappuser name varenvironmentvarnameuser note we dont also create an awsiamaccesskey resource because we dont want the access key to be committed so we manually create accesssecret keys via the console resource awsiamusergroupmembership cognitoappusergroups user awsiamusercognitoappusername groups awsiamgroupcognitoappgroupname data awsiampolicydocument cognitoappgrouppolicy statement actions cognitoidpListUserPools cognitoidpListUsers resources statement actions cognitoidpAdminAddUserToGroup cognitoidpAdminConfirmSignUp cognitoidpAdminCreateUser cognitoidpAdminDeleteUser cognitoidpAdminDeleteUserAttributes cognitoidpAdminDisableProviderForUser cognitoidpAdminDisableUser cognitoidpAdminEnableUser cognitoidpAdminForgetDevice cognitoidpAdminGetDevice cognitoidpAdminGetUser cognitoidpAdminInitiateAuth cognitoidpAdminLinkProviderForUser cognitoidpAdminListDevices cognitoidpAdminListGroupsForUser cognitoidpAdminListUserAuthEvents cognitoidpAdminRemoveUserFromGroup cognitoidpAdminResetUserPassword cognitoidpAdminRespondToAuthChallenge cognitoidpAdminSetUserMFAPreference cognitoidpAdminSetUserSettings cognitoidpAdminUpdateAuthEventFeedback cognitoidpAdminUpdateDeviceStatus cognitoidpAdminUpdateUserAttributes cognitoidpAdminUserGlobalSignOut resources awscognitouserpoolpoolarn resource awsiampolicy cognitoappgrouppolicy name varenvironmentvarnamegrouppolicy policy dataawsiampolicydocumentcognitoappgrouppolicyjson resource awsiamgrouppolicyattachment cognitoappgroupattachment group awsiamgroupcognitoappgroupname policyarn awsiampolicycognitoappgrouppolicyarn snsiamtf data awsiampolicydocument cognitosnsassumerolepolicy statement actions stsAssumeRole principals type Service identifiers cognitoidpamazonawscom resource awsiamrole cognitosnsrole name varenvironmentvarnamecognitosnsrole assumerolepolicy dataawsiampolicydocumentcognitosnsassumerolepolicyjson data awsiampolicydocument cognitosnspublishpolicy statement actions snsPublish resources resource awsiampolicy cognitosnsrolepolicy name varenvironmentvarnamecognitosnsrolepolicy policy dataawsiampolicydocumentcognitosnspublishpolicyjson resource awsiamrolepolicyattachment cognitosnsrolepolicyattachment role awsiamrolecognitosnsrolename policyarn awsiampolicycognitosnsrolepolicyarn varstf variable awsrolearn variable awsregion default useast1 variable environment variable name default yourservicename variable callbackurls type list variable logouturls type list variable domain variable googleproviderclientid variable googleproviderclientsecret variable facebookproviderclientid variable facebookproviderclientsecret variable rootdomain description certificate root domain default yourexampledomaincom Conclusion Theres so much more to the story but I think this post is long enough as it is and I dont want to keep you any longer If you have any questions then please reach out to me on twitter I personally found the documentation around Cognito and the various tools to be both overwhelming and underwhelming Not to mention confusing in places as well as just downright frustrating at times Hopefully you found this short break down of AWS Cognito useful Theres so much more still to dive into but this should give you at least a decent starting point "},{"title":"Concepts From the C Programming Language","tags":["C"],"href":"/posts/concepts-from-the-c-programming-language","content":" Introduction1 Compilation2 Compilers21 C11 safe functions22 Hello World3 Constants vs Directives4 Quotations5 Char Type6 Null Terminator7 Pointers8 Arrays9 Enumerators10 Memory Allocation with different Types11 Reallocating Memory12 Function Prototypes13 Conclusion14 Introduction I decided recently to read a book on the C programming language The idea was to learn some more lowlevel concepts that other higherlevel languages were abstracting away from me This write up is the result of some of those learnings This is not a how do you write C code This is a grouping of topics and concepts that I found interesting and thought might be useful to other developers with similar interests Some of the stuff covered will be obvious but I appreciate some concepts wont be obvious for all readers so this write up will end up crossing back and forth between different levels of understanding and experience In other words your mileage may vary Compilation When writing a program in a language like ChttpsenwikipediaorgwikiCprogramminglanguage youll find that by itself it is not executable ie you cant run a C file directly You need to convert the C source code into machine codehttpsenwikipediaorgwikiMachinecode ie something the computers CPU can understand Machine code is as lowlevel as you can get when interacting with a computer So the C language is considered a higherlevel abstraction to save us from having to write machine code ourselves A language like Pythonhttpswwwpythonorg is an even higherlevel abstraction to save us from having to write C Note the Python language is actually written in C Although there are other Python interpreters implemented in different languages In order to convert C code into machine code we need a compiler Strictly speaking you also need a linkerhttpsenwikipediaorgwikiLinkercomputing which takes multiple compiled objects and places them into a single executable file Generally speaking when we say compile a C file were really combining two separate steps compiling and linking into the single generic term compile Compilers To compile C source code into an executable you need a compiler of which there are many options The two most popular being LLVMs clang and GNUs gcc You might also find on your computer a cc command but typically this is aliased to an existing compiler The Mac OS doesnt provide a compiler by default But if you install XCode youll get the LLVMs suite of compilers Below we see that we get quite a few alias and all of them point to the same embeded LLVM compiler gcc version Configured with prefixApplicationsXcodeappContentsDeveloperusr withgxxincludedirusrincludec421 Apple LLVM version 800 clang800038 Target x8664appledarwin1560 Thread model posix InstalledDir ApplicationsXcodeappContentsDeveloperToolchainsXcodeDefaultxctoolchainusrbin llvmgcc version Apple LLVM version 800 clang800038 Target x8664appledarwin1560 Thread model posix InstalledDir ApplicationsXcodeappContentsDeveloperToolchainsXcodeDefaultxctoolchainusrbin clang version Apple LLVM version 800 clang800038 Target x8664appledarwin1560 Thread model posix InstalledDir ApplicationsXcodeappContentsDeveloperToolchainsXcodeDefaultxctoolchainusrbin cc version Apple LLVM version 800 clang800038 Target x8664appledarwin1560 Thread model posix InstalledDir ApplicationsXcodeappContentsDeveloperToolchainsXcodeDefaultxctoolchainusrbin The first two alias gcc and llvmgcc are a little bit confusing and also a bit misleading as theyre not GNUs version Theyre still the LLVMs compiler but with some modifications In the first instance gcc the compiler is configured to use some additional libraries that are provided by c You can tell this by the flag withgxxincludedir Its worth noting that with a standardsimple C source code file all these alias work to compile the source code into an executable But some compilers allow you to utilise additional extensions not provided by the standard C language so you need to be careful your code doesnt try to utilise something thats not available at compilation time LLVMs licensing is BSD meaning Apple can embed it within their own software that is not GPLlicensed Typically LLVMs compiler is faster than GNUs but in some cases it might not support all the same targets as GNUs For more comparison details see httpclangllvmorgcomparisonhtmlhttpclangllvmorgcomparisonhtml C11 safe functions Youll likely be told that some functions provided within C arent safe usually around string manipulation For example some string functions allow for overflow of data because they dont check that the underlying array data structure is able to contain the strings being manipulated C11 compatible compilers will provide an additional set of string functions that are considered safe although this is a contentious area of discussion as some C programmers believe that these functions are just as unsafe and if anything the claims are misleading See this comment on Stack Overflow which goes into more detailhttpstackoverflowcomquestions40829032howtoinstallc11compileronmacoswithoptionalstringfunctionsincluded4083970240839702 Note Ive also discovered that none of the compilers I have on my OS support these functions and Ive since discovered one of the few environments to support these functions is the Windows platform Below is an example C file that demonstrates how to check if your compiler supports these additional safe functions include int mainvoid if defined STDCLIBEXT1 printfOptional functions are definedn else printfOptional functions are not definedn endif return 0 If your compiler supports these optional safe string functions then to enable them youll need to add a define directive that modifies the subsequent header file But you also need to add this directive before you include the preprocessor directive that imports the stringh header define STDCWANTLIBEXT1 1 include If you dont set STDCWANTLIBEXT1 to 1 then the header stringh will utilise the old unsafe string functions Hello World Below is a simple hello world C example include preprocessor directive to include code file at compile time define NAME World preprocessor directive to substitute any reference to NAME before compilation returns an int type and takes in no arguments void int mainvoid printfhello s NAME return 0 zero indicates no problems ie EXIT CODE Its important to note that the directives include and define are processed at the start of the compilation process This is at the request of the compiler So itll be one of the compilers first steps to pull in the preprocessor and have it ensure the file is setup ready for the rest of the compilation You would compile this file like so cc helloworldc o hw Now you have a macOS compatible executable hw prints the message Hello World To crosscompile for another OS eg Linux then use Docker or a VM Other modern languages like Gohttpsgolangorg or Rusthttpswwwrustlangorg allow you to crosscompiler without a VM Constants vs Directives We saw in the above Hello World example the use of the directive define which allowed us to use a single identifier NAME in this case throughout our program The benefit is that we can change the value once and have it updated everywhere But do not get this confused with a variable It is not This is just a sequence of characters that are blindly replaced at the preprocessing stage The value assigned to NAME will be replaced inside your program regardless of whether its valid code or not Meaning it could cause an ambiguous compiler error On the other hand you can define a proper constant like so include int mainvoid const char NAME World printfHello s NAME return 0 What this gives you is a variable that has an actual type assigned to it char Meaning the compiler will help you identify an incorrect value if necessary much more easily than using the define directive Quotations In C single quotes denote a char type and double quotes denote a string So if you had the following code char foo a printffoo sn foo It would error with format specifies type char but the argument has type char To get it to work you need to provide the memory address location of foo using the addressof operator char foo a printffoo sn foo Well come back to the operator and understand what means later when we discuss pointers8 Char Type When creating a variable and assigning a string to it the value assigned is really a pointer to a location in memory The char type is used when storing characters such as a but it also allows storing of strings such as abc When assigning a string the pointer is to an array where each element of the array is a character of the provided string For example the string abc would be stored in an array that looked something like a b c Note see section Null Terminator7 to clarify above code This happens even if the string you provide is just one character Although depending on your programs design it could be argued that you should not have assigned a single character string but instead used single quotes to represent a single char When assigning a character eg a to a variable of type char it takes on dual duty Meaning the char type variable can represent the specific character a but really it stores the ASCII integer code that defines that character Take a look at an ASCII tablehttpwwwasciitablecom to identify the code associated with a particular character This means we could also directly assign the integer 97 instead of the character a to the char type variable But also and more interestingly because of these characteristics we can perform arithmetic on the variable include int mainvoid print character and its associated ascii code integer char foo a printffoo c dn foo foo a 97 now modify the variable by adding to it foo foo 2 we can see the characterinteger reflects the change printffoo c dn foo foo c 99 return 0 Null Terminator Consider the following code char mystring4 abc The reason we specify a length of 4 and not 3 as you would expect with a string that is three characters in length is because the underlying array that mystring is being pointed towards looks like this a b c 0 yes it does actually have four elements The last element is known as the null terminatorhttpsenwikipediaorgwikiNullterminatedstring When this data is stored in memory we can start at the location in memory the address where the first element is stored and then step through memory until we reach the null terminator where well then find the end of the string Note you can set your variable to be the actual length of the content eg char mystring1 a but in some instances this can cause strange overlaps of data and strictly speaking isnt valid code either Pointers When declaring a variable the computer sets aside some memory for the variable Next the variable name is linked to the location in memory that was set aside for it Lastly the value you want to assign to the variable is placed into the relevant location of memory Lets consider the following code include int mainvoid int foo 1 printffoo dn foo int bar int barval 1 printfbarval dn barval bar barval printfbar pn bar int bargetval bar printfbargetval dn bargetval return 0 So we see that we create a foo variable and assign 1 to it We then print that integer in the typical way Next we make a slightly more convoluted version but this time were utilising a pointer in order to help us understand what they are and how to use them Here are each of the steps broken down int bar we declare a pointer variable called bar of type int but we dont initialize it with a value int barval 1 we both declare and initialize the variable barval as type int bar barval we initialize the pointer variable bar with the memory address of barval int bargetval bar we dereference the address ie follow the pointer assigned to bar which leads us to the value stored in that memory slot meaning we will be assigning an address to this pointer and the content at that memory address location will also be of type int The output of this program is foo 1 barval 1 bar 0x7fff59a1769c bargetval 1 OK so there are some things that we need to clarify and thats the and operators valueataddress operator used when declaring a pointer and when dereferencing a pointer addressof operator used to reference the memory address of a variable The first thing we should be aware of is that were not able to print a declared variable that has no value initialized for it So imagine the following code int bar printfbar dn bar This would cause the following compiler error format specifies type int but the argument has type int Which makes sense as weve declared the variable as the type bar So we can start by fixing that issue and correctly specifying the second argument to printf as bar int bar printfbar dn bar Note printf will now try to use to dereference the value from the variable bar Unfortunately this code still causes an error This time variable bar is uninitialized when used here Which again makes sense Nothing more to say about that portion of the code I just wanted to make it clear what happens when you try to print an uninitialized variable and also what happens when that variable is a pointer type So continuing through the program the next line of interest is bar barval This gives us the actual location in memory for the variable barval ie 0x7fff59a1769c So the value assigned to bar isnt 1 but the address of 1 in memory Finally we declare and initialize the variable bargetval with the actual value of 1 and we do that by using to deference the variable bar which contains a memory address int bargetval bar What that means is bar holds a memory address which isnt a concrete value its an indirection to somewhere else Hence we would say bar points to the actual values location and explains why we use the valueataddress operator to deference the value The following code shows how to print the location in memory of a variable even if it wasnt declared as a pointer simply by using the addressat operator which itself indicates a pointer to another location char foo a printfaddress of foo pn foo Note the isnt tied to in any way Its purpose is just to return the memory address for a given variable Remember a memory address isnt the value itself but a reference to where the value can be found One analogy Ive seen is of your home address on an envelope the envelope isnt your home nor is the address written on the envelope The envelope just indicates where your home can be found One last thing to considerremember is that C doesnt have a String type It stores strings in an array data structure An array will automatically return the address location of its first element to the variable it is assigned to This is why you may have seen a char pointer being assigned a variable without the need to use the operator to get the memory address of that variable because the variable in this case an array already provides a memory address The following example shows this char message6 hello array data structure used and memory location for message0 returned char messagePtr message no need to use message now printfmy pointer pn messagePtr printfmy message sn message Note compare C pointers and Go pointers here httpsdavecheneynet20140317pointersingohttpsdavecheneynet20140317pointersingo In C there are two ways to define a pointer 1 char foo 2 char foo Both of which are equivalent Although the first seems like the clearer option as someone new to C would read it define a variable called foo of type character pointer compared to the second option which could lead them to think the variable name was foo not foo For me the second option is preferred because otherwise the following code becomes a bit ambiguious char foo bar You might incorrectly think this would create two variables both of type character pointer but really only foo is the pointer and bar is a normal char type Whereas using the second format char foo this code becomes much clearer char foo bar Lastly if you want to create a const that happens to be a pointer then the syntax is as follows int count 43 int const pcount count We prefix the const keyword with the valueat operator and not the variable name Arrays Consider the following code which is broken by the way include int mainvoid char mystring abc printfmystring s mystring return 0 This code has the following compiler error incompatible pointer to integer conversion initializing char with an expression of type char 4 What this error tells us is that the variable mystring has a type of char 4 meaning it is actually an array hence the 4 syntax and so we should have declared the variable like so char mystring4 abc We saw earlier why this is required when talking about null terminators7 But just to recap its because a string should be stored within an array data structure So we need to declare it as such We also learned earlier using the above example why the length of the array is 4 and not 3 which you may initially have expected which a string of three characters again to recap this is because of the extra element added to the array for you 0 the null terminator So the reason Im talking about arrays is because in the original code above the one before declaring the variable correctly there were actually two errors linked together The second part of the error was format specifies type char but the argument has type char What this error tells us is that printf was expecting a pointer but all it got was something of a char type When declaring the variable as an array we fix both errors But both these errors has led some people to incorrectly assume that an array is a pointer when it is not Lets recap why this works When assigning a string the compiler expects the contents to be stored within an array Each element within the array is an address to the value given to it in memory So in our example above ie the string abc a is stored in memory and the address of that memory is placed in mystring0 Next b is stored in memory and the address of that memory is placed in mystring1 and so on A pointer in contrast is a single location in memory whereas an array hold lots of memory addresses Because of this an array variable automatically points to the first element within the array This is why if you try to printf a string the compiler will complain if you dont provide a pointer Because it expects a string to have been stored within an array which our earlier example didnt But when storing a string inside an array the variable that is passed to printf would already be a pointer due to it automatically referencing the first array element as its value Interestingly an arrays type is made up of the element type the overall array dimension So int foo3 is a different type to int bar4 Even though the value type int is the same the array dimension sizelength is different If you want to know how many bytes an array will occupy then you calculate it based upon the number of elements multiplied by the size of each element Lastly you can define and initialize a string without specifying an array dimension ie no size char foobar No dimension provided What this does is leave the decision of how much memory to allocate to the compiler But you can only do this when you initialize the variable with a value Although you couldnt do char foobar as there is no value for the compiler to utilise to know how much memory to allocate Enumerators Enumerators allow you to define new variable types They automatically assign numerical values to each of the identifiers within the enumerator although you do also have control over the specific values as well This concept is best explained by way of example include int mainvoid enum weekend Saturday Sunday 0 1 enum weekend today Sunday 1 enum weekend saturday Saturday 0 enum weekend yesterday today 1 0 now yesterday is Saturday printftoday dn today printfsaturday dn saturday printfyesterday dn yesterday return 0 You can see that we define a weekend type and each element in that set is assigned an automatic numerical value 0 and 1 Next we create a new variable today of type weekend and assign it the value Sunday which means it is effectively assigned the value 1 Youll also notice this is why were able to do simple arithmetic with the today variable eg today 1 to get 0 If you wish to provide your own values you can enum bool true 1 false 2 enum bool on true enum bool off false printfon dn on 1 printfoff dn off 2 Memory Allocation with different Types Read this articlehttpwwwintegralistcoukpostsbitsandbytes if you need a refresher on understanding RAM bits binary and stuff like that Array Consider the following code int foo3 123 printffoo variable points to pn foo int i 0 do printffoou pn i void fooi i whilei Note z is for sizet and the u prevents an invalid conversion specifier error Signed vs Unsigned In C you can define an integer to be either signed or unsigned The former means the number can be both negative and positive as well as zero Whereas the latter is always positive Note typically if a number is negative youll prefix it with If the number is positive then it is just the number For example 1 and 1 This is a little more convoluted in binary though resulting in concepts such as ones complement and twos complement Google it if you want to know more though You dont need to explicitly provide the signed keyword eg signed int its just implied The latter unsigned is an integer that can only be positive So if you need to store an integer and you know the value will always be zero or positive then you can define it as being unsigned and the compiler can make appropriate optimisations based on that understanding So although the underlying memory allocation is the same for signed or unsigned the actual values represented are slightly different in that unsigned allows for storing values that are twice the size of signed because half of signeds values have to account for negatives Reallocating Memory With strings you typically define them as follows ie the underlying data structure is an array char names20 hello But this can result in wasted reserved memory Also when reading input from stdin eg user input the amount of characters entered could exceed the specified reserved memory allocation Below is an example given as a Stack Overflowhttpstackoverflowcomquestions8164000howtodynamicallyallocatememoryspaceforastringandgetthatstringfromu response that reads each character from stdin and reallocates the memory space if required its advised that you ensure reallocation of memory is done as a multiple such as double the size char getln char line NULL tmp NULL sizet size 0 index 0 int ch EOF while ch ch getcstdin Check if we need to stop if ch EOF ch n ch 0 Check if we need to expand if size bufsize bufsize LSHRLBUFSIZE buffer reallocbuffer bufsize if buffer fprintfstderr lsh allocation errorn exitEXITFAILURE Notice c variable is declared as an int and not a char the author of the blog post makes mention of this as being because EOF is an int type The author then goes on to explain that in more recent releases there is a much shorter version that can be implemented thanks to the getline function char lshreadlinevoid char line NULL ssizet bufsize 0 have getline allocate a buffer for us getlineline bufsize stdin return line Function Prototypes A compiler will error if you try to call a function before it has been defined This can be mitigated by utilising function prototypes that let you define the signature of the function up front and defer the definition until a later point in time sort of like defining an interface type Function prototypes int Foodouble datavalues sizet count int Bardouble x sizet n int mainvoid int f Foosignature int b Barsignature Definitions for Foo and Bar Conclusion So that covers most of what I found interesting about C over the last few days Im not planning on writing any C code in the future why bother when I can get pretty compariable performance great tooling etc etc with a language like Gohttpsgolangorg But regardless this was a fun little exercise in learning something new Hope you learnt something too As always feel free to ping me on twitterhttpstwittercomintegralist if you feel Ive missed something "},{"title":"Data Types and Data Structures","tags":["design","algorithms","data-structures","python"],"href":"/posts/data-types-and-data-structures","content":" Data Typesdatatypes Data Structuresdatastructures Arrayarray Linked Listlinkedlist Treetree Binary Treebinarytree Binary Search Treebinarysearchtree RedBlack Treeredblacktree Btreebtree Weightbalanced Treeweightbalancedtree Binary Heapbinaryheap Hash Tablehashtable Graphgraph Conclusionconclusion In this post we will be looking briefly at and at a highlevel the various data types and data structures used in designing software systems and from which specific types of algorithms can subsequently be built upon and optimized for There are many data structures and even the ones that are covered here have many nuances that make it impossible to cover every possible detail But my hope is that this will give you an interest to research them further Data Types A data type is an attributehttpsenglishstackexchangecoma28098334144 of data which tells the compiler or interpreter how the programmer intends to use the data Primitive basic building block boolean integer float char etc Composite any data type struct array string etc composed of primitives or composite types Abstract data type that is defined by its behaviour tuple set stack queue graph etc If we consider a composite type such as a string it describes a data structure which contains a sequence of char primitives characters and as such is referred to as being a composite type Whereas the underlying implementation of the string composite type is typically implemented using an array data structure well cover data structuresdatastructures shortly Note in a language like C the length of the strings underlying array will be the number of characters in the string followed by a null terminatorpostsconceptsfromthecprogramminglanguage7 An abstract data type ADT describes the expected behaviour associated with a concrete data structure For example a list is an abstract data type which represents a countable number of ordered values but again the implementation of such a data type could be implemented using a variety of different data structures one being a linked listhttpsenwikipediaorgwikiLinkedlist Note an ADT describes behaviour from the perspective of a consumer of that type eg it describes certain operations that can be performed on the data itself For example a list data type can be considered a sequence of values and so one available operationbehaviour would be that it must be iterable Data Structures A data structure is a collection of data type values which are stored and organized in such a way that it allows for efficient access and modification In some cases a data structure can become the underlying implementation for a particular data type For example composite data types are data structures that are composed of primitive data types andor other composite types whereas an abstract data type will define a set of behaviours almost like an interface in a sense for which a particular data structure can be used as the concrete implementation for that data type When we think of data structures there are generally four forms 1 Linear arrays lists 2 Tree binary heaps space partitioning etc 3 Hash distributed hash table hash tree etc 4 Graphs decision directed acyclic etc Note for a more complete reference please see this Wikipedia articlehttpsenwikipediaorgwikiListofdatastructures Lets now take a look at the properties that make up a few of the more well known data structures Array An array is a finite group of data which is allocated contiguous ie sharing a common border memory locations and each element within the array is accessed via an index key typically numerical and zero based The name assigned to an array is typically a pointer to the first item in the array Meaning that given an array identifier of arr which was assigned the value a b c in order to access the b element you would use the index 1 to lookup the value arr1 Arrays are traditionally finite in size meaning you define their lengthsize ie memory capacity up front but there is a concept known as dynamic arrays and of which youre likely more familiar with when dealing with certain highlevel programmings languages which supports the growing or resizing of an array to allow for more elements to be added to it In order to resize an array you first need to allocate a new slot of memory in order to copy the original array element values over to and because this type of operation is quite expensive in terms of computation and performance you need to be sure you increase the memory capacity just the right amount typically double the original size to allow for more elements to be added at a later time without causing the CPU to have to resize the array over and over again unnecessarily One consideration that needs to be given is that you dont want the resized memory space to be too large otherwise finding an appropriate slot of memory becomes more tricky When dealing with modifying arrays you also need to be careful because this requires significant overhead due to the way arrays are allocated memory slots So if you imagine you have an array and you want to remove an element from the middle of the array try to think about that in terms of memory allocation an array needs its indexes to be contiguous and so we have to reallocate a new chunk of memory and copy over the elements that were placed around the deleted element These types of operations when done at scale are the foundation behind why its important to have an understanding of how data structures are implemented The reason being when youre writing an algorithm you will hopefully be able to recognize when youre about to do something lets say modify an array many times within a loop construct that could ultimately end up being quite a memory intensive set of operations Note interestingly Ive discovered that in some languages an array as in the composite data type has been implemented using a variety of different data structures such as hash table linked list and even a search tree Linked List A linked list is different to an array in that the order of the elements within the list are not determined by a contiguous memory allocation Instead the elements of the linked list can be sporadically placed in memory due to its design which is that each element of the list also referred to as a node consists of two parts 1 the data 2 a pointer The data is what youve assigned to that elementnode whereas the pointer is a memory address reference to the next node in the list Also unlike an array there is no index access So in order to locate a specific piece of data youll need to traverse the entire list until you find the data youre looking for This is one of the key performance characteristics of a linked list and is why for most implementations of this data structure youre not able to append data to the list because if you think about the performance of such an operation it would require you to traverse the entire list to find the endlast node Instead linked lists generally will only allow prepending to a list as its much quicker The newly added node will then have its pointer set to the original head of the list There is also a modified version of this data structure referred to as a doubly linked list which is essentially the same concept but with the exception of a third attribute for each node a pointer to the previous node whereas a normal linked list would only have a pointer to the following node Note again performance considerations need to be given for the types of operations being made with a doubly linked list such as the addition or removal of nodes in the list because you now have not only the pointers to the following node that need to be updated but also the pointers back to a previous node that now also need to be updated Tree The concept of a tree in its simplest terms is to represent a hierarchical tree structure with a root value and subtrees of children with a parent node represented as a set of linked nodes A tree contains nodes a node has a value associated with it and each node is connected by a line called an edge These lines represent the relationship between the nodes The top level node is known as the root and a node with no children is a leaf If a node is connected to other nodes then the preceeding node is referred to as the parent and nodes following it are child nodes There are various incarnations of the basic tree structure each with their own unique characteristics and performance considerations Binary Tree Binary Search Tree RedBlack Tree Btree Weightbalanced Tree Heap Abstract Syntax Tree Binary Tree A binary tree is a rooted tree and consists of nodes which have at most two children This is as the name suggests ie binary 0 or 1 so two potential valuesdirections Rooted trees suggest a notion of distance ie distance from the root node Note in some cases you might refer to a binary tree as an undirected graph well look at graphsgraph shortly if talking in the context of graph theory or mathematics Binary trees are the building blocks of other tree data structures see also this referencehttpsstackoverflowcoma22005884288305 for more details and so when it comes to the performance of certain operations insertion deletion etc consideration needs to be given to the number of hops that need to be made as well as the rebalancing of the tree much the same way as the pointers for a linked list need to be updated Binary Search Tree A binary search tree is a sorted tree and is named as such because it helps to support the use of a binary search algorithm for searching more efficiently for a particular node more on that later To understand the idea of the nodes being sorted or ordered we need to compare the left node with the right node The left node should always be a lesser number than the right node and the parent node should be the decider as to whether a child node is placed to the left or the right Consider the example image above where we can see the root node is 8 Lets imagine were going to construct this tree We start with 8 as the root node and then were given the number 3 to insert into the tree At this point the underlying logic for constructing the tree will know that the number 3 is less than 8 and so itll first check to see if there is already a left node there isnt so in this scenario the logic will determine that the tree should have a new left node under 8 and assign it the value of 3 Now if we give the number 6 to be inserted the logic will find that again it is less than 8 and so itll check for a left node There is a left node it has a value of 3 and so the value 6 is greater than 3 This means the logic will now check to see if there is a right node there isnt and subsequently creates a new right node and assigns it the value 6 This process continues on and on until the tree has been provided all of the relevant numbers to be sorted In essence what this sorted tree design facilitates is the means for an operation such as lookup insertion deletion to only take on average time proportional to the logarithmhttpsenwikipediaorgwikiLogarithm of the number of items stored in the tree So if there were 1000 nodes in the tree and we wanted to find a specific node then the average case number of comparisons ie comparing leftright nodes would be 10 By using the logarithm to calculate this we get log 210 1024 which is the inverse of the exponentiation 210 2 raised to the power of 10 so this says well execute 10 comparisons before finding the node we were after To break that down a bit further the exponentiation calculation is 1024 2 2 2 x 2 x 2 x 2 2 2 x 2 x 2 210 so the logarithm to base 2 of 10 is 1024 The logarithm ie the inverse function of exponentiation of 1000 to base 2 in this case abstracted to n is denoted as log 2 n but typically the base 2 is omitted to just logn When determining the time complexity for operations on this type of data structure we typically use Big O notation and thus the Big O complexity would be defined as Olog n for the average search case which is good but the worst case for searching would still be On linear time which is bad and Ill explain why in the next section on redblack treesredblacktree Note Ive covered the basics of logarithm and binary search in a much older postpostsbigoforbeginners8 about Big O notation and so Ill refer you to that for more details Similarly when considering complexity for a particular algorithm we should take into account both time and space complexity The latter is the amount of memory necessary for the algorithm to execute and is similar to time complexity in that were interested in how that resource time vs space will change and affect the performance depending on the size of the input RedBlack Tree The performance of a binary search tree is dependant on the height of the tree Meaning we should aim to keep the tree as balanced as possible otherwise the logarithm performance is lost in favor of linear time To understand why that is consider the following data stored in an array 1 2 3 4 If we construct a binary search tree from this data what we would ultimately end up with is a very unbalanced tree in the sense that all the nodes would be to the right and none to the left When we search this type of tree which for all purposes is effectively a linked list we would worst case end up with linear time complexity On To resolve that problem we need a way to balance the nodes in the tree This is where the concept of a redblack tree comes in to help us With a redblack tree due to it being consistently balanced we get Olog n for searchinsertdelete operations which is great Lets consider the properties of a redblack tree Each node is either red or black The root node is always black All leaves are NIL and should also be black All red nodes should have two black child nodes All paths from given node to NIL must have same num of black nodes New nodes should be red by default well clarify below Note when counting nodes we dont include the root node and we count each black node up to and including the NIL node The height of the tree is referred to as its blackheight which is the number of black nodes not including the root to the furthest leaf and should be no longer than twice as long as the length of the shortest path the nearest NIL These properties are what enable the redblack tree to provide the performance characteristics it has ie Olog n and so whenever changes are made to the tree we want to aim to keep the tree height as short as possible On every node insertion or deletion we need to ensure we have not violated the redblack properties If we do then there are two possible steps that we have to consider in order to keep the tree appropriately balanced which well check in this order 1 Recolour the node in the case of a red node no longer having two black child nodes 2 Make a rotationhttpsenwikipediaorgwikiTreerotation leftright in the case where recolouring then requires a structural change The goal of a rotation is to decrease the height of the tree The way we do this is by moving larger subtrees up the tree and smaller subtrees down the tree We rotate in the direction of the smaller subtree so if the smaller side is the right side well do a right rotation Note there is an inconsistency between what nodesubtree is affected by a rotation Does the subtree being moved into the parent position indicate the direction or does the target node affected by the newly moved subtree indicate the direction Ive opted for the latter as well see below but be aware of this when reading research material In essence there are three steps that need to be applied to the target node T being rotated and this is the same for either a left rotation or a right rotation Lets quickly look at both of these rotation movements Left Rotation 1 Ts right node R is unset becomes Ts parent 2 Rs original left node L is now orphaned 3 Ts right node is now set to L we now find Rs left pointer has to be set to T in order for it to become the parent node meaning Rs original left pointer is orphaned Right Rotation 1 Ts left node L is unset becomes Ts parent 2 Ls original right node R is now orphaned 3 Ts left node is now set to R we now find Ls right pointer has to be set to T in order for it to become the parent node meaning Ls original right pointer is orphaned Lets now visualize the movements for both rotations Left Rotation Right Rotation Note rotations are confusing so I recommend watching this short videohttpswwwyoutubecomwatchv95s3ndZRGbk for some examples and pseudocode Btree A Btree is a sorted tree that is very similar in essence to a redblack tree in that it is selfbalancing and as such can guarantee logarithmic time for searchinsertdelete operations A Btree is useful for large readwrites of data and is commonly used in the design of databases and file systems but its important to note that a Btree is not a binary search tree because it allows more than two child nodes The reasoning for allowing multiple children for a node is to ensure the height of the tree is kept as small as possible The rationale is that Btrees are designed for handling huge amounts of data which itself cannot exist inmemory and so that data is pulled in chunks from external sources This type of IO is expensive and so keeping the tree fat ie to have a very short height instead of lots of node subtrees creating extra length helps to reduce the amount of disk access The design of a Btree means that all nodes allow a set range for its children but not all nodes will need the full range meaning that there is a potential for wasted space Note there are also variants of the Btree such as B trees and B trees which well leave as a research exercise for the reader Weightbalanced Tree A weightbalanced tree is a form of binary search tree and is similar in spirit to a weighted graph in that individual nodes are weighted to indicate the more likely successful route with regards to searching for a particular value The search performance is the driving motivation for using this data structure and typically used for implementing sets and dynamic dictionaries Binary Heap A binary heap tree is a binary tree not a binary search tree and so its not a sorted tree It has some additional properties that well look at in a moment but in essence the purpose of this data structure is primarily to be used as the underlying implementation for a priority queuehttpsenwikipediaorgwikiPriorityqueue The additional properties associated with a binary heap are heap property the node value is either greater or lesser depending on the direction of the heap or equal to the value of its parent shape property if the last level of the tree is incomplete the missing nodes are filled The insertion and deletion operations yield a time complexity of Olog n Below are some examples of a max and min binary heap tree structure Max Heap Min Heap Hash Table A hash table is a data structure which is capable of maping keys to values and youll typically find this is abstracted and enhanced with additional behaviours by many highlevel programming languages such that they behave like an associative arrayhttpsenwikipediaorgwikiAssociativearray abstract data type In Python its called a dictionary and has the following structure on top of which are functions such as del get and pop etc that can manipulate the underlying data table name foobar number 123 The keys for the hash table are determined by way of a hash functionhttpsenwikipediaorgwikiHashfunction but implementors need to be mindful of hash collisions which can occur if the hash function isnt able to create a distinct or unique key for the table The better the hash generation the more distributed the keys will be and thus less likely to collide Also the size of the underlying array data structure needs to accommodate the type of hash function used for the key generation For example if using modular arithmetic you might find the array needs to be sized to a prime number There are many techniques for resolving hashing collisions but here are two that Ive encountered 1 Separate Chaining 2 Linear Probing Separate Chaining With this option our keys will contain a nested data structure and well use a technique for storing our conflicting values into this nested structure allowing us to store the same hashed value key in the top level of the array Linear Probing With this option when a collision is found the hash table will check to see if the next available index is empty and if so itll place the data into that next index The rationale behind this technique is that because the hash table keys are typically quite distributed eg theyre rarely sequential 0 1 2 3 4 then its likely that youll have many empty empty elements and you can use that empty space to store your colliding data Note Linear Probing is suggested over Separate Chaining if your data structure is expected to be quite large Personally I dont like the idea of the Linear Probing technique as it feels like itll introduce more complexity and bugs Also there is a problem with this technique which is that it relies on the top level data structure being an array Which is fine if the key were constructing is numerical but if you want to have strings for your keys then that wont work very well and so youll need to be clever with how you implement this Graph A graph is an abstract data type intended to guide the implementation of a data structure following the principles of graph theoryhttpsenwikipediaorgwikiGraphtheory The data struture itself is nonlinear and it consists of nodes points on the graph also known as vertices edges lines connecting each node The following image demonstrates a directed graph notice the edges have arrows indicating the direction and flow Note an undirected graph simply has no arrow heads so the flow between nodes can go in either direction Some graphs are weighted which means each edge has a numerical attribute assigned to them These weights can indicate a stronger preference for a particular flow of direction Graphs are used for representing networks both real and electronic such as streets on a map or friends on Facebook When it comes to searching a graph there are two methods 1 Breadth First Search look at siblings 2 Depth First Search look at children Which approach you choose depends on the type of values youre searching for For example relationship across fields would lend itself to BFS whereas hierarchical tree searches would be better suited to DFS Conclusion The discussion of data structures and the various highlevel data types in computing is a massive topic and so we cannot hope to try and cover every aspect and detail due to the sheer broad scope That said I hope you found some useful insights and are now able to look at your programming language of choice with a slightly clearer understanding of the possible structures that sit beneath the abstraction surface "},{"title":"Designing for Simplicity","tags":["aws","bash","ci","docker","jruby","patterns","rpm"],"href":"/posts/designing-for-simplicity","content":" This post was originally written for David Walsh back in 2015httpsdavidwalshnamedesigningsimplicity Ive since decided to reproduce it here for my own records Integralist August 2017 Before we get started its worth me spending a brief moment introducing myself to you My name is Mark or integralisthttpstwittercomintegralist if Twitter happens to be your communication tool of choice and I currently work for BBC News in London England as a principal engineertech lead and Im also the author of Pro Vimhttpwwwapresscom9781484202517 Quick shout out to Steven Jackhttpstwittercomstevenjack85 who took the time to review this post A lot of what weve done right he either helped instigate or was a fundamental part of its success The responsive BBC News website receives approximately 8 million visits per day thats on average for a quiet news day Those numbers will go much higher once the responsive site replaces the current static desktop offering and starts to incur many more users But for the moment that gives a rough idea of the sort of traffic we get on a daily basis This post aims to take a whirlwind tour of different code design and architectural discussion points that have cropped up at one point or another while Ive been working at the BBC We will be peeking at some toplevel system infrastructure in a bid to provide you with some food for thought on these topics Ill talk about some techniques and tools that work for us and well also see some that didnt work quite so well Ive always been intrigued by how other developers work and think about different types of problems so lets consider this a knowledge sharing experience from me to you Note the thoughts and comments here are my own and do not necessarily represent those of my employer Yup I had to go there just in case Now some readers will probably not have to worry about the same sort of scalability problems the BBC has to deal with when designingbuilding systems and applications But this isnt to say the information and thoughts Im going to share with you here in this post arent transferable In fact much of what Ill be discussing are concepts that can be utilized in applications of any size because good design is effective at any scale Note this post covers very little frontend technologies and techniques That topic of discussion is much more vast and has been covered substantially over the past few years especially the topic of performance which has since 2007 been brought into the mainstream mindset of frontend engineers by Steve Soudershttpstevesouderscom So without further ado lets begin Complexity vs Complicated When describing a piece of software youll often here the words complex and complicated used We need to be careful when using these phrases that we select the one that accurately expresses our opinion For example some one might say a piece of software is complicated when really they mean its complex and vice versa Unfortunately the definition of both these words doesnt help to clarify which we should use and in which context This can make it harder to express what we really mean to say For me describing something as being complicated infers a negative connotation it has used either bad logic or implemented a bad design Usually the phrase this feels overengineered will follow The word complexity on the other hand represents to me a varying value in that its dependent on the system being reviewed and so it feels like a more appropriate term to use when recognizing the softwares design has evolved and become much larger over time You can also still apply the term complex to individual units of code If a class or function is taking on too much responsibility then it can feel like its becoming too complex and subsequently some of its logic will need to be extracted into another class or function This is apposed to lets say a function using a bad algorithm in which you may identify that code as being complicated Now if a piece of software has a lot of moving parts most applications do and you have a difficult time building a mental model of it all and subsequently are not able to follow its thread very easily doesnt necessarily mean the application is complicated Lots of small simple and noncomplicated functional components can be composed together to create a much larger system and subsequently can make the overall picture become a little harder to decipher This is what makes software complex Good code design doesnt always help you to see the bigger picture What good code design does help with is making smaller functional units much more easily understood and composable eg SRP Single Responsibility Principle one part of SOLIDhttpenwikipediaorgwikiSOLID28objectorienteddesign29 The reason Im mentioning this upfront is because I do not want people to walk away mistakenly thinking their existing software or its architecture is fundamentally flawed or broken You must always take a critical eye to your systemsapplications but be aware that although they may be complex that might be the natural order of things A complete rewrite is not necessarily required Promoting simplicity With the discussion of complexity behind us lets move onto what it means for software to be simple Why is simplicity a good thing Simplicity itself is defined as the quality or condition of being easy to understand or do If a piece of software is considered simple then chances are it has been found to be easy to understand and easy to reason about Simple software is also easy to manipulate and apply changes to Kent BeckhttpenwikipediaorgwikiKentBeck renowned author of many top quality software engineering books and the cocreator of Extreme Programming which then evolved into agile practices made the following statement back in 2012 make the change easy then make the easy change What Kent was referring to was that for a piece of software to be easily changed you needed to simplify its design in such a way for it to be able to facilitate a future requirement Simplicity will also tend to result in less bugs because there are less complicated moving parts The irony of all this is that writing and designing code to be simple is usually quite a complex task Its important to realize that simplicity easy Lots of people fail to make the distinction between the two Simplicity can also not always mind you help towards other goals such as reusability and portability of your software Naming things Phil Karlton Netscape engineer sadly killed in 1997 once said There are only two hard things in Computer Science cache invalidation and naming things Youve likely heard this quote said many times already throughout your career Theres a reason for that because its a painfully universal truth Nothing causes our team to sit pondering in deadlock or maybe livelock would be more accurate together than when were trying to figure out what to call our new library People underestimate how important it is to properly name things Whether it be a new opensource library a class a function a variable it doesnt matter what it is there is a potential to cause confusion and in some cases real problems if named badly Theres the classic frontend dilemma where a developer creates a class called redbanner because it is applied to a component that is well red and then a month later the designers step in and change the component to have a blue background Great now this doesnt seem like much of an issue but scale up the size of the site and the potential places where that class gets used now add on top of that a little more ambiguity and youve suddenly incurred tech debt Now that was just a super basic example In practice youll find naming issues everywhere Does the name of your class truly represent its intent Is the name youve used too explicit by that I mean does it reference a particular object type or design aspect that restricts the class from being truly generic For example if you have a class that acts as a mediator between two objects have you named it MessageBus because youre currently using the ObserverPubSub pattern What if the class changes functionality to some other pattern or software Surely naming it to be ComponentMediator would be better as that is clear enough to express the intent of the class while open enough for the implementation details to change in future sort of like a less concrete OpenClosed principlehttpenwikipediaorgwikiOpenclosedprinciple Either way you need to not underestimate the problems and confusion that can be caused by a badly named objectclassthing Be vigilant and if youre unsure then discuss it with your team But be warned this isnt always a quick or obvious process Growth Applications are organic they grow and evolve over time which can increase their complexity But software also needs to contend with growing popularity which means increased traffic and the need to always be available ie up and running All software should have a modicum of scalability in mind when within the design stages It would be foolish not to at least consider what happens when your system comes under heavy load The BBC has specific platform teams dedicated to carrying out load tests before any major software release The benefit of this is to ensure that a new piece of software can run safely while handling x number of requests per second The load test results help to indicate what an applications threshold will be My team typically uses a command line tool called Siegehttpmanpagesubuntucommanpageshardyman1siege1html to stress test our applications before requesting a full load test from our platform team as this can help weed out any silly mistakes before going through official channels The BBC also utilizes Chaos MonkeyhttpsgithubcomNetflixSimianArmywikiChaosMonkey a tool developed by Netflix to help verify the resiliency of its systems Chaos Monkey itself has been instrumental in how we have started to look at designing and architecting our systems and applications The purpose of Chaos Monkey is to bring down at random a server instance that is currently running This sounds like a crazy thing to do within your production environment but the reality is it forces you to think long and hard at the design stage of your applications to ensure they are resilient and capable of being brought back up automatically At any moment Chaos Monkey could bring down one of our servers running a public facing and highly critical application This helps bring about an important realization that you need to start considering solutions before writing any code By really thinking about the design up front you avoid issues where you may find your existing software just isnt resilient enough and is going to cause unacceptable service outages and yet the existing system may well be in such a state that it doesnt facilitate an easy path to becoming scalable eg a monolithic application with many responsibilities and domain areas will introduce a massive single point of failure and is going to take a long time to refactor Solutions to the problem of resiliency and scalability at the design stage could for example involve building microservices to help isolate single points of failure As well as implementing statelessimmutable servers where by you can bring up a new instance and not worry that critical state has been lost along with service discovery mechanisms such as Consulhttpswwwconsulio or etcdhttpsgithubcomcoreosetcd Infrastructure as code is another important concept and can be achieved using technology such as AWS CloudFormationhttpawsamazoncomcloudformation which well discuss in more detail later on in this post Scaling a system to accommodate more users isnt a free lunch doing so can cause the introduction of inherent complexity as it needs to consider many different mechanisms for facilitating an increase in traffic Typically the first thing developers do before considering more extreme architectural changes is to try running their code in parallel or support more concurrent operations There is nothing necessarily wrong with that unless your code isnt thread safehttpenwikipediaorgwikiThreadsafety Fixing the issue of thread safety isnt always straight forward and if youre new to the concept of multithreaded code then youll likely hit a few stumbling blocks as you slowly start to understand the problem space If youre interested in understanding thread safety and the different concurrency abstractions in more detail then I recommend you take a read of the following article Thread Safe Concurrencyhttpwwwintegralistcoukpoststhreadsafeconcurrency Along side the process of trying to speed up code by multithreading youll nearly always want to have your application scale automatically based upon the current needs of the system If youre application is running on an infrastructure that is designed to scale horizontally eg it dynamically creates new server instances running your application then you will quickly start to introduce the problem of data consistency Data consistency is where the CAP theorem comes into play CAP states it is impossible for a distributed computer system to simultaneously provide all three of the following guarantees consistency availability and partition tolerance What this means in practice is Consistency all nodes in your system should have the same view of the world Availability a guaranteed response whether a success or fail message Partition tolerance the system as a whole continues to function even when a part of the system completely fails or messages between decoupled systems are lost The problem of growth can also affect the decisions made from both back and frontend development perspectives For example when building realtime data processing applications you have the option of either long polling and handling back pressure vs Web Sockets vs ServerSent Events all with varying browser support and each of these options opens up a different set of concerns regarding how the backend system is architectured The problem of growth and scaling applications can be simplified by splitting up your application into well defined isolated functional units If youre working with a single monolithic application try to imagine what it would look like if split up into multiple isolated services Where each part of the application communicated with another service via HTTP or TCP or via some other low level mechanism What would the system look like Dont you think it would be easier to analyze and scale individual services than one massive monolithic application This leads us nicely onto the topic of Microservices The micro fallacy As of 20142015 microservices is a bit of a buzzword Lots of organizations and companies are talking about designing microservices and using tools like Dockerhttpswwwdockercom to enable them to more easily build and scale their growing set of microservices I felt it was important to make a very quick mention of the fact that people tend to see a monolithic application as bad and a system of small services as good While I would tend to agree I would state that Ive sat on both sides of the fence and each one has their pros and cons If youre working on a small scale system then a microservice design can be extra overhead and unnecessary complexity you dont need Just because all the cool kids are drinking the microservices koolaid doesnt mean you have to or should Always take a critical eye to your design and evaluate things based on your own situationrequirements With that being said I personally see microservices as the way we will and should be designing and building large scale systems moving forward as overall it offers many more benefits compared to old school monolithic applications Its important when designing services that you split up the responsibilities in your application appropriately and try to find a sweet spot between separation of concerns and domains vs fragmentation Decoupling systems When considering the design of a complex system we ideally want to decouple our code by individual domain areas The reason for this is that it allows us to scale any particular part of the system that becomes a bottle neck rather than carrying out a blanket scale up of the entire system which is not only expensive but impractical Using the following diagram as a basis lets consider an application where the user uploads an image 1 and the server resizes the image 2 and places the newly created image into an AWS S3httpawsamazoncoms3 bucket 3 The server is expected to return an auto generated URL to the user 4 which when shared with and visited by another user will show the resized image Bad Architectureimagesbadarchitecturepng This architecture will not scale very well nor very easily Note for brevity Ive left out some details from this architecture such as persisting URLs Instead the process should be more decoupled like so see below diagram user uploads an image 1 the server stores the image into an S3 bucket in its original form 2 and sends a message to an AWS SQShttpawsamazoncomsqs queue 3 The server then returns a message to the user to inform them the image is being processed along with the auto generated URL 4 In the mean time a separate service is polling the queue for messages 5 The service will read the message which can include the location of the source image in S3 and then retrieves the relevant image from S3 6 so it can resize the image 7 and replace the image in S3 with the resized version 8 or whatever needs to happen at that point Better Architectureimagesbetterarchitecturepng So this design might not be perfect but whats better about this architecture is that weve decoupled the various parts of the overall system that previously wouldve made scaling the application much harder Were now in a position to scale up the backend service the service that polls the queue and resizes the images separately from the frontend system which handles the user upload If the user visits the auto generated URL before the image has been processed then a message can be displayed to indicate the image is still being processed Again this isnt perfect but it has freed up the user to go off and do other things theyre not chained to the browser window watching a message that says Processing for the next few minutes or longer depending on how much load your system is under One practical improvement here is that the new system is much more fault tolerant than the original In the original system if the server crashed then the user would likely be returned a 500 HTTP Status CodehttpenwikipediaorgwikiListofHTTPstatuscodes500 whereas with the new system the user can continue to use the website theyll see the message Image waiting to be processed until a new backend server instance can be brought up where by itll continue to process messages off the image queue Note fault tolerance is often referred to as partition tolerance I mentioned this earlier when discussing CAP Theorem In the above example weve used queues to help decouple the individual parts of our software system similar in spirit to creating microservices but there are other mechanisms for decoupling code such as using a message bushttpenwikipediaorgwindexphptitleMessageorientedmiddlewareredirectno Best to research different techniques to see how your architecture could be designed to utilize them to avoid problems with scaling Note depending on the purpose of the above application you might decide that displaying the unoptimized image would be better than displaying a message to the user to say the image is still being processed The reason I didnt do that here was because of performance reasons the size of the image could be very large and not something you want a mobile user to have to download especially if theyre traveling with a poor network connection BrokerRenderers At BBC News in London my team have released an opensource framework written in Ruby called AlephanthttpsgithubcomBBCNewsAlephant which abstracts a common pattern we find useful for decoupling our data driven applications Weve used this particular framework on quite a few projects over the past year and a half such as the Scottish Referendum the local and general elections an upcoming redesign of BBC Newsbeat as well as the World Service Kaleidoscope project dynamic serving of image based content to devices with poor support for nonlatin fonts Note Id like to give a shout out to Robert Kennyhttpstwittercomkenturamon formerly of BBC and now working at the Guardian as the original inspiration and developer for the Alephant framework Although it has changed quite significantly since its inception it was his solid work that helped to support some very important and high traffic events The pattern is effectively a broker ie mediator and a renderer With this pattern user requests are routed through to the relevant broker who then decides where the requests should be directed On the other end of the design are a bunch of renderer services and their role changes depending on the type of model we use push or pull Lets explore this a little further Broker a service which accepts requests requests are handled differently depending on the model Renderer a service which gets data from an endpoint and renders it into HTML or whatever format is required Models push and pull well see what each of these are in the next section Push The push model is similar to the image processing architecture we discussed in the previous section in that were using a queue to decouple our services In the push model messages are placed on a queue These messages contain data we want rendered into HTML to use a real world scenario the election results are pushed into a queue We then have a renderer service running and this service polls the queue for messages When a message eg election result is received we then process that message render it into HTML and store that rendered content in a storage area this could be any cloud based storage system such as AWS S3 for example The renderer service is easily scalable because we can set alarm thresholds that indicate whether there are too many messages on a queue ie if there are lots of messages waiting on the queue then our renderer service isnt processing them quick enough and so we need to scale up more renderer instances to handle the message load AWS SQS doesnt guarantee a delivery order this isnt true of all queues so if delivery order is important to you then there are other queue types available and so care needs to be taken to ensure messages dont get overwritten We found this type of contention could happen when scaling up our Renderer service To give you an example imagine we have two Renderers R1 and R2 Both of them take a message off the queue R1 gets version 1 of message A while R2 gets an updated version 2 of message A If R2 finishes first then R1 will finish last and subsequently the older version of the message will be stored and used a real and practical example of this is getting election results pumped onto a queue the vote count for a particular party needs to be the latest numbers To avoid this contention we use a document store AWS DynamoDB to track the version of a message and when we come to store the rendered content of our data in our storage facility we make sure the key we need to lookup that rendered content also includes its version number Note this is something we did before DynamoDB added its Conditional Put feature The broker in the push model receives a request for a component and is able to use the information it is provided to lookup the latest rendered version of a message It does this by constructing a key that determines the location of the latest version within our storage facility These lookups are also heavily cached to allow us to handle as much load as possible The following diagram gives you a toplevel view of this architecture Broker Renderer Pattern Push Modelimagesbrokerrendererpatternpushmodelpng Note for brevity in the above diagram Im not demonstrating either the caching of broker requests or the sequencing requirements ie the storing off the version of a message into DynamoDB As mentioned before some queues have different guarantees and so I didnt want the diagram to be too tightly coupled to DynamoDBs implementation There is another concern that weve accounted for but Ive left out for brevity which is AWS S3s eventual consistency model But I think for now this explanation should be enough to give you an idea of how the pattern works Pull The pull model is simpler in that it doesnt rely on a message queue for its data In this model our renderers are more connected to our broker By this I mean the broker will receive a request for a component and it will use the information that has been passed with the request to lookup the relevant renderer it needs to contact service discovery in order to retrieve the requested component The renderer service has a remote endpoint built in that it uses to make requests to to retrieve the data required for a successful render to happen so no storing of rendered components into a storage facility were dynamically renderering data upon request The reason we created this model was because we had certain domain models where the data changes were vast and would require a large amount of upfront rendering that might potentially never be seen by ie requested from an end user The BBCs Market Data pages were an example of this where some business data would rarely be viewed The following diagram gives you a toplevel view of this architecture Better Architectureimagesbetterarchitecturepng Note for brevity in the above diagram Im not detailing the complexity of how you feed information to a broker so it knows which renderer to interrogate to satisfy the users request There are some pitfalls to this model though The main one being were coupling our data to our templates For example if a change is required within the template lets say a HTML class attribute is added to an element but all other structure of the content is the same then it would require a complete rerender of the component We had experimented briefly with rendering the data itself and then letting a layer further downstream handle the template composition but this resulted in other complications With looming deadlines and concerns regarding forcing responsibility for the templates onto some other part of the system we decided to postpone Design and architecture The reason we find the brokerrenderer pattern so useful is because of how weve designed the individual libraries that make up the framework They allow for consumers of the libraries to provide custom configuration to support their own specific requirements We also use the strategy design pattern for the pull model which allows a consuming application to inject the logic the library should use to determine how to construct remote data endpoints in case the storage location path is different for your setup This is the essence of composability its like lego bricks slotting together to form a large piece Although you can utilise individual aspects of the frameworks that fit your needs because each library is self sufficient in providing isolated functionality the framework works best when utilized to form the brokerrenderer pattern we primarily designed the framework around Its an interesting mix of SRP Single Responsibility Principle and the composability of FP Functional Programming How low can you or should you go For any performance critical and scalable application its important to consider the overhead of certain technologies For example when it came to building the Alephant framework discussed in the previous section we decided that Sinatrahttpsinatrarbcom a very popular Ruby web framework that is preferred over a monolithic framework such as Ruby on Railshttprubyonrailsorg was still too bulky for our needs By that I mean it had all sorts of web related features that we didnt need for our application So we opted for the lower level almost but not quite bare metal Rack interface Depending on your requirements it may even be worth considering whether you drop down a level from HTTP to TCP or even UDP socket protocols if you genuinely dont need the extra overhead of HTTP for communicating between services eg you may have internal services running on your server instances that arent publicly exposed Its important to realize that stripping back the layers of complexity can help to better reveal the intent of your code Making it easier to understand and to reason about Language agnostic templates At the BBC we discovered the hard way that our approach for managing the storage and deployment of frontend components wasnt scalable The problem stemmed from a more traditional method of structuring frontend applications which meant separating JavaScript CSS and images into separate folders Lets for the moment consider the JavaScript folder within this folder we would have multiple modules a module being a single JavaScript file and represents an isolated piece of functionality A single HTML component could well have multiple functional requirements meaning it would be dependent on multiple JavaScript modules This shouldnt appear strange to any one as the majority of developers separate their concerns in this way The downside though to this approach is that when your system begins to expand and evolve you will start to notice the problem of fragmentation and being tightly coupled to the application the components currently resides within By that I mean if you have a HTML component and it relies on JavaScript modules X and Y as well as CSS files A and B then you cant easily reuse that component within another system without either replicating the directory structure that the current application utilizes or making code changes to reflect the location of dependencies within the new system On top of that the dependency tree for each component was getting larger and harder to visualize and maintain as more and more functionality was being added to certain components In this classic architectural model you lose or at least complicate maintainability reusability portability and even the ability to isolate components for easier testing The ability to share components across teams who work on different platforms was also proving difficult as we were being forced to duplicate content and this led to the decision to create a specification that describes how to build truly atomic components that can be easily consumed by varying services Enter Peter Chamberlinhttpstwittercompgchamberlin and Liam Wilkins to take inspiration from both Brad Frosts atomic designhttpbradfrostcomblogpostatomicwebdesign and Ian Feathers Rizzohttpianfeathercoukamaintainablestyleguide and helped to resolve this divide by creating the opensource project ChintzhttpsgithubcomBBCNewschintz which combines the best aspects of both the former projects Note this project is still WIP work in progress but we encourage the community to get involved and create an open discussion around how the specification evolves The driving force behind this specification was for it to be language agnostic This was a fundamental requirement in allowing different language platforms to consume these components The processing of components has a few simple requirements define a folder structure that will contain the components define a manifest that describe the dependencies for a given component implement a client parser that resolves the dependencies within the manifest build an app that consumes the client parser In the following diagram we show how two separate platforms PHP and Ruby consume a parser specific to their language to resolve the dependencies for a given component and serve up the components in the most appropriate way for their platform Better Architectureimageschintzspecificationpng There are currently two WIP client parsers weve opensourced PHPhttpsgithubcomBBCNewschintzphp RubyhttpsgithubcomBBCNewschintzruby Note we also hope to implement one utilizing JavaScriptNode A manifest file could look something like name exampleElement dependencies css basebasecss basetypographycss js exampleElementsomeOtherFilenamejs elements anotherThing This manifest file makes it simple to understand the complexity of a single component The hope moving forward is for our internal component pattern library built on top of Chintz to be utilised across teams working on different platforms For example the BBC News core team work on a traditional PHP platform while the Elections Presentation team develop cloud based components generated with Ruby that are consumed by the core teams platform Automation and Duplication When dealing with large software systems youll need to ensure youre automating things and reducing the amount of repeatable work you have This is essential when managing software of any substantial scale There are a few ways my team automates and reduces duplication youll likely find similar concepts and processes in many other teams and products Continuous Delivery AWS CloudFormation Custom library abstractions Containerisation via Docker Continuous Delivery Deploying software within the BBC can be a complex process as we have lots of moving parts to take code from a developers laptop and into a working release thats deployed to our cloud infrastructure To make deploying software as simple as possible we have since developed a complicated deployment pipeline to try and help achieve the end goal of having a simple release process Notice I said complicated We currently use the Jenkinshttpjenkinsciorg continuous integration server to support our deployment process Jenkins is an industry standard piece of software and no organization should be releasing software without some form of CI When we commit code into GitHub we trigger a build job on Jenkins that builds an RPM that well want to deploy onto our distributed cloud servers But before that deploy can happen we need to make sure that our software is safe to release This means that we trigger another job that verifies this via different unit and integration tests for our integration environment If all is well we then trigger the next set of jobs that do the same for our test environment and once again for our live environment If any one of these jobs fail then the deployment to that environment will be marked as failed Now the reason I used the word complicated earlier rather than the more ideal complex see the start of this blog post for the difference between them is because weve fundamentally reached the limits of what the Jenkins software can handle and this has resulted in a solution that may work most of the time but is far to complicated to be maintainable If a build breaks then it can be very hard to follow the trail because Jenkins wasnt really designed with Continuous Delivery in mind Although Jenkins provides plugins to help extend its functionality so it can support more complex deployment pipeline processes the various plugin options available arent as good as they could be and most dont offer a clear visibility of the status of a particular group of jobs This is where something like GOCDhttpwwwgocd which aims to streamline the buildtestrelease life cycle would come into the equation Jenkins has served its purpose and weve found that its no longer a complex piece of software but has moved into being more of a complicated one instead If you find yourself in a similar situation then this should be an indication that you need to be constantly monitoring your processes and evaluating their effectiveness Our Jenkins setup works so its good enough for the moment But dont fall into the trap of accepting the current system as being good enough The moment you start feeling any pain setting up or implementing a piece of software then much like a good code smell indicates the need to refactor you should start reevaluating how the situation can be improved This is something were actively doing at the moment Remember not to make any rash decisions eg lets implement a whole new CICD system that could potentially leave you with more tech debt than you initially realized especially if your team has very tight deadlines already Adding to your workload isnt a sensible decision Changing your deployment process is an incredibly important decision and so you must be absolutely certain youve exhausted all other options first AWS CloudFormation If youre unfamiliar with CloudFormation then Ill refer you to the official definition AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources provisioning and updating them in an orderly and predictable fashion The AWS CloudFormationhttpawsamazoncomcloudformation service is great for two primary reasons 1 It automates the ability to start up and provision a consistent and reproducible infrastructure 2 It facilitates the ability to version control your infrastructure With regards to the first point this also facilitates a seamless setup of different deployment environments This means we can configure each specific environment using CloudFormation parameters to allow as an example an integration and test environments to have very small EC2 server instances while stage and production environments have larger server instances helping to keep costs down in our noncritical environments such as integration and test Depending on your experience with AWS you may feel like CloudFormation falls into the complicated category This is very much a subjective opinion though as learning CloudFormation may well initially feel quite confusing and complicated the end result is a much simpler quicker and easier way to buildautomate a fully provisioned system There are also tools available that can help ease writing raw CloudFormation which is just a JSON file You could go one abstraction level up and start writing CloudFormation stacks using YAMLhttpyamlorg or you could use a language specific DSL such as CFNDSLhttpsgithubcomstevenjackcfndsl which is a Ruby DSL Custom library abstractions Most of our projects are built up from a baseline of abstractgeneric libraries RPMs that provide us with specific default functionality These abstractions make it very easy for us to construct a complex software stack without having to constantly repeat ourselves For example consider the following three libraries 1 Component Base 2 JRuby Application 3 Puma Init Each one of these libraries will add their own profile to homecomponent Each library will load their profile which will add a hook for another downstream consuming library to utilize For example Component Base creates homecomponentbashprofile The profile tries to source homecomponentcustomprofile JRuby Application creates homecomponentcustomprofile The profile tries to source homecomponentcomponentprofile Puma Init creates homecomponentcomponentprofile The profile tries to source homecomponentpumaprofile This allows our libraries to work in a similar fashion to the Template Method PatternhttpenwikipediaorgwikiTemplatemethodpattern where by subsequent inheriting layers can add their own behavior while overriding specific upstream configuration As well as the above profile files each library adds its own additional behaviors For example the first library Component Base also sets up the following items a component user account sets up the server instance to utilize our custom logging service defines an app directory with all the correct permissions defines an init script that runs a daemon set via APPDAEMON APPDAEMON can be overridden by an upstream library The second library JRuby Application loads the Component Base as a dependency then loads the JRuby binary This means a service that utilizes the JRuby Application library can develop an application that is either userfacing eg has an exposed web service or is an internal application that does data processing Its up to the developer to decide what they build The third library Puma Init loads the JRuby Application as a dependency creates a homecomponentcomponentprofile file this adds Puma specific configuration and overrides APPDAEMON to reference Puma Pumahttppumaio is a popular multithreaded Ruby web server The following diagram provides a basic visualization of the inherited layers Better Architectureimagesrpmlibrariespng Each library acts as a layer that builds upon the last and provides specific functionality that extends the overall behavior Docker Chances are youve already heard the hype around Dockerhttpwwwdockercom and containerization by now The simple fact is we use Docker because it helps to keep applications small focused think Single Responsibility Principle and most importantly isolated think decoupling code Docker also helps to make installation of specific lowlevel software much easier with standardized communication between running containers helping these services to interact We use Docker in a variety of different situations Continuous Integration using a standard masterslave setup and each build job is its own container Prototyping super fast to get up and running using new software Monitoring and Logging solutions Tooling eg Spurioushttpsgithubcomspuriousio see the following section is built using Docker containers All that being said Docker isnt a panacea It doesnt fit all project requirements and in some cases its not the right solution at all As with all things consider the pros and cons and use where and when appropriate Tooling The tools you utilize during and post development can be fundamental to the success of your project I wont cover all the various tooling solutions we have in place as there are just far too many but there is one tool in particular I want to focus on Spurioushttpsgithubcomspuriousio What Spurious offers is a set of AWS services that you can run locally ie without costing you anything The AWS services it currently works with CloudFormation and SNS to follow at some point in the future S3httpawsamazoncoms3 SQShttpawsamazoncomsqs DynamoDBhttpawsamazoncomdynamodb ElastiCachehttpawsamazoncomelasticache Currently there are helpers libraries that will configure the AWS SDK to utilise Spurious making for an almost seamless integration into your application code These libraries support Rubyhttpsgithubcomspuriousiorubyawssdkhelper ClojurehttpsgithubcomIntegralistspuriousclojureawssdkhelper and JavaScripthttpsgithubcomspuriousiojsawssdkhelper Now this by itself is very useful We can spin up an instance of Spurious on our machine and start writing application code that interacts with a queue SQS a record store DynamoDB and a data store S3 along with caching requests via ElastiCache But on top of that is the Spurious Browserhttpsgithubcomspuriousiobrowser which allows us to peek inside each of these services using a standard web browser Meaning rather than having to waste time writing code to filter down a long list of results from S3 I can instead open Spurious Browser and click on a few links to drill down into the content Im interested in and when I find it I can open the content to view it Spurious was born from the need to rapidly prototype new features for our BrokerRenderers but to also avoid the whole deployment process Its still in development and has some rough edges there is a rewrite planned that will change the implementation language from Ruby to Go but ultimately weve been using Spurious on quite a few projects now and it has become indepensible I highly recommend you take a look FP OOP and MVC So far in this post weve been discussing toplevel system and architectural design tooling and other associated processes In this section I want to take it down a level and briefly express my love for Functional Programming FP which will require me to make some comparisons to ObjectOriented Programming OOP OOP and FP are two styles of development that have somewhat apposing interests OOP is focused more around encapsulation of data whereas FP likes to filter data OOP consists of classes and creating objects where data is hidden inside them but you can manipulate the data from exposed methods Whereas FP is generally a collection of pure functions that help to enforce referential transparencyhttpenwikipediaorgwikiReferentialtransparency28computerscience29 and data is passed through these functions manipulating it as it goes One of the big selling points for most FP languages is support for immutability Immutability is a way of distinguishing state identity and value From a practical standpoint if you modify data then the changes will result in a modified copy of the original data as apposed to mutating it Weve seen a recent spurt of interest around immutability and FP in general in the JavaScript community as of late with Morijshttpsswannodettegithubiomori and immutablejshttpsfacebookgithubioimmutablejs as a couple of examples but there have been others and theyve been around for much longer Immutability can help to eradicate a whole host of bugs that can catch you out in a language as mutable as JavaScript and even more so in languages where code can be multithreaded Languages like Clojure for example also implement persistent data structures that make immutability easy and inexpensive I recommend visiting the Clojure websitehttpclojureorg and finding out more about the underlying implementation details as its quite an interesting read Typically youll find a mixed bag of opinions some organizations are OOP based others prefer FP Whats worth being aware of is that this isnt an either or situation You can find some benefits from having the structuralencapsulation benefits of OOP while implementing certain features in a functional way but I think thats a topic for another day The OOP with FP methodology is quite easy with a language like Scalahttpwwwscalalangorg which seamlessly incorporates both styles within their language design Youll also find the Clojure programming language which isnt a strict FP language although it offers many of the features youll expect to find actually incorporates certain OOP principles in both the language API as its built on top of the JVM and so it applies some idiomatic lisp syntax to creating Classes and Interfaces and in some of its popular libraries developed by the opensource community One example of this is the popular Componenthttpsgithubcomstuartsierracomponent library that allows for easier management of the lifecycle of software components by encapsulating data The reason for a discussion around using OOP vs FP is because there are different pros and cons to both styles of development when it comes to high traffic distributed and concurrent software systems Although were primarily an OOP based organization and through good code design have not suffered any illeffect from using OOP some members of the team myself included have since been exposed to the greater simplicity and reduction of bugs that inherently occurs when utilizing FP and languages that natively support immutability Immutability is the key to avoiding complex mutexes and semaphores when writing multithreaded code To find out more on the subject please take a read of Thread Safe Concurrencyhttpwwwintegralistcoukpoststhreadsafeconcurrency One last point Id like to cover is the abuse of the MVC pattern First of all there is no official design pattern called MVC if anything it could be considered an architectural pattern but most people seem to refer to it as a design pattern It is merely a collection of smaller patterns that have commonly been grouped together to form what is known as Model View Controller lest we get caught up discussing the confusion around why so many developers design their software around a single global MVC architecture as apposed to multiple smaller MVC structures within their applications but thats probably a discussion for another day Regardless of its apparent popularity MVC isnt always the right tool for the job Its worth being vigilant and recognising when youre about to cargocult a particular technology or design pattern because in a lot of cases MVC can actually be seen as overkill and add additional complexity and complication to your code that you dont need Lets be clear here MVC may well be the right choice for you but I would urge you to investigate the merits of using it against an alternative set of patterns first that could help you simplify your software design for example a simple observer pattern can suffice for many applications build up the layers of complexity as theyre needed dont go and jump straight in with an MVC framework The abuse of MVC has also become standard fare within the realm of frontend JavaScript libraries Again Im not saying MVC is bad or wrong but what I am saying is do you need it Remember to consider what it is youre signing up to when you utilise a particular opensource or even commercial library within your application Simplicity is the key Inherent complexity There are some pieces of software and tools that are inherently complex by design As an example I want to demonstrate this by taking a look at our application deployment pipeline which utilizes Jenkinshttpjenkinsciorg and provides us with a full circle continuous deployment process The process can roughly be broken down into the following steps Merge code into our version control system GitHubhttpswwwgithubcom A webhook is setup for our repository that alerts Jenkins Jenkins starts building our first job pulls codes and installs dependencies If that job passes our job is configured to call another Jenkins job The next job runs our unit tests and when they pass it again calls another job The next job builds and deploys our application to our integration server If the deploy is successful we call another job that runs our integration tests If the tests fail then we mark the job as failed If the tests pass then we trigger the next job which deploys to our test server Once deployed to test we again run integration tests and mark the deploy as either successful or failed If were good up until this point then we will deploy to live Now even breaking down this process into individual simplified steps Im sure you can appreciate that this deployment process will look fairly complicated in practice and it is Weve got multiple jobs that have been abstracted to a level where we can reuse them across multiple different projects but this process of abstraction makes understanding whats happening quite complicated because you have to context switch between Jenkins and GitHub One method of abstraction we used was to take advantage of IoC inversion of control In our jobs to help us keep a record of changes to the build scripts we decided the best thing to do was to pull down bashhttpenwikipediaorgwikiBash28Unixshell29 scripts from GitHub and execute them to implement specific functionality that had been abstracted away behind a reusable function This is what allows us to create generic jobs that can be reused across many different projects This happened because we tried to approach our build jobs the same way we would application code and although we have a nice DRY set of build jobs its a difficult thread to follow especially when there is an error within a section of your deployment pipeline it can become quite hard to locate and debug This is one of the primary reasons we feel our current solution is complicated Its because Jenkins wasnt designed with this type of complex workflow in mind and so although there are plugins that can help visualize and construct full deployment pipelines theyre not ideal I wont go into the details of why as thats a little outside the scope of this article but trust me its a bit of a hack to get continuous delivery pipelines working really smoothly in Jenkins In a bid to simplify our deployment pipeline we have planned to take the following approach to resolve our issues with Jenkins note this approach might not work for you and dependent on your team your mileage may vary Reduce our multiple connected generic jobs down into a single job meaning well manually pass in the environment we wish the component to be deployed to every time the job is run this is a temporary measure as it gives us a sane baseline from which we can build upon We will then investigate alternative methods for making Jenkins work with complex deployment pipelines such as Jenkin TemplateshttpswikijenkinsciorgdisplayJENKINSTemplateProjectPlugin Then when we get capacity within the team well look to setup GOCDhttpwwwgocd which we setup as a tech spike previously and found it worked very well with our teams workflow Containerization As of 20142015 the tech world is buzzing about Dockerhttpswwwdockercom If you want to find out more about what Docker is and how it works then I recommend going through their website as it has a wealth of good information But in summary a docker container is an isolated set of processes that can be connected with other containers to create different types of software applications but in a more manageable way One of Dockers biggest selling points is the mantra build once run anywhere This means you can define your requirements and be confident that itll run the same whether your platform is AWS ECS Google Cloud Compute or Tutum A simple example usually given to demonstrate how Docker might be used in a traditional web development workflow is create a Docker container that runs your web app then create another container that runs your database and in another container you might have a caching service eg Redis or Memcache All of these containers can be linked together by Docker so they can communicate safely with each other while providing decoupled isolation This description doesnt do justice to what Docker can offer though and if youve not played around with it yet then I strongly urge you to try it out To give you some ideas Ive listed below a few different ways we utilize Docker and raise some small points about with using it as well Here are some use cases we apply Docker to Continuous Delivery each Jenkins job is a fresh container with an isolated set of dependencies this allows us to avoid the issue of developers wanting to upgrade specific software versions and being told they cant because an updated version will cause issue for other products Dynamically building sandboxes at runtime which host specific GitHub branches for sharing with stake holders Tech spikes Docker allows us to focus on features and not get caught up with installationconfiguration issues Helping us to better structure and develop software Spurioushttpsgithubcomspuriousio uses Docker to run each of the faked AWS services Containerizing your applications will likely become the standard software architecture design pattern in the very near future so its worth your time playing around and getting to grips with the tools available and how best to break down your potential monolith into decoupled microservices But with that said what are the current pain points for containerizing all your software Are there times when you wouldnt want to use it Well there are a couple of things worth considering Tooling The tooling available to debug containers is still considered to be quite immature Docker does offer both a log command as well as an exec command which lets you jump into a running container using a bash shell but there can be instances where neither of those options will yield success For example a container wasnt logging any thing and on top of that the application which when run outside of the container would run forever in an infinite loop would run initially and then stop when run inside the container and so the exec command was not an option as I couldnt jump inside a container that was no longer running Note in the latest binary Docker has provided additional stats that are exposed via the docker stats containerhttpsdocsdockercomreferencecommandlineclistats command Scaling Scaling containers requires a new approach that might not fit your current infrastructure model Also the question of scalability could potentially be simplified by the upcoming release of Dockers Swarm feature which offers the ability to control a cluster of machines running Docker via a single endpoint But how easily this makes actually scaling a cluster of running docker instances within a single host remains to be seen in practice Traditional vertical vs horizontal scaling is quite straight forward with vertical scaling you increase the size of the box running your application whereas with horizontal scaling you increase the number of boxes instead The ability to horizontally scale a cluster of docker containers might not be so straight forward Imagine I have an EC2 server instance running with an auto scaling grouphttpawsamazoncomautoscaling that creates a new server instance when the CPU reaches 70 On this instance I have a web service running inside a Docker container and its maxing out its resources Wed have to use custom metrics fired off from our Docker container to AWS CloudWatchhttpawsamazoncomcloudwatch which would allow us to define an alarm that triggered a new server instance to be created Otherwise our ASG wouldnt catch the Docker container failing Docker Swarm could help here its not released yet and weve not had a chance to try an early prerelease But it appears to only work by having a precreated pool of servers rather than dynamically generated server instances Maybe once we see an official release and get the opportunity to see it in action well find it handles the issue of scaling perfectly But until then its just something to consider before going too far ahead with designing your infrastructure around Docker Orchestration Running Docker containers can result in very long commands that are difficult to visually parse and take a long time to retype This is the primary problem orchestration tools such as the soontobe released Docker Composehttpsblogdockercom201412dockerannouncesorchestrationformulticontainerdistributedapps are aiming to solve Ive used an early release of Docker Compose and it seems promising It definitely helps to document the infrastructure setup much like AWS CloudFormation does and allow for easily reproducible environments Layers This is actually quite a specific concern and one that might not really have any practical basis or realistic solution from an implementation point of view or because there are already work arounds but Ive found building images that need to utilise private security authentication credentials to be more awkward than it feels like it should be When building a Docker image youre constructing it from multiple layers this allows Docker to create an image using cached layers and greatly improving the build performance but this means if you add your security credentials then theyll be baked into the resulting image I would prefer it if it was possible to remove layers so that you could add the credentials pull down further resources within the Docker image and then remove the layer that included your credentials The work around to this is to build a new image from an already running container which has already mounted your credentials Now there is nothing necessarily wrong with this approach it just doesnt feel very clean either I may be complaining about nothing but its something thats stuck with me Id be interested to know how other organizations and developers are handling building images with private authentication An alternative suggestion from Steven Jackhttpstwittercomstevenjack85 was that maybe Docker could add build time environment variables These would allow you to provide your security credentials at the point of building your image rather than having them baked inside of the image For example docker build t someimage e MYSECRETKEY Optimizing user experience I said at the start of this post that I wouldnt be covering very much frontend technologies because they were vast complex and the recommended patterns seem to change on a regular basis But this isnt to say there arent things you should be doing when designing your user facing applications Below is a short list of topics that you should spend some time investigating Performance since 2007 Steve Soudershttpstevesouderscom has been the godfather of frontend performance Read his books and his blog posts Youll see a massive improvement from following his best practices which are pretty much industry standard nowadays Reacthttpfacebookgithubioreact is a JavaScript library for building user interfaces I wont go into the details of the library here but suffice to say it offers a lot of interesting features such as implementing a Virtual DOM for high performance no templates for building interfaces and avoiding twoway data binding in favor of reconciliation diffing Immutability this is a tenet of Functional Programming and is finally starting to see a rise within the JavaScript community There are a few different libraries available to support immutable data such as immutablejshttpfacebookgithubioimmutablejs and Morihttpswannodettegithubiomori I would recommend trying out these libraries as immutability can help to reduce a whole host of hard to reason about bugs BEM Block Element Modifier is a predescribed notation for writing CSS that allows it to reduce complexityspecificity while increasing maintainability and reusability Harry Robertshttpcsswizardrycom has written extensively about it and in fact I would recommend reading anything he has to say on the subject of CSS and how to write it to support large scale applications RWD Responsive Web Design is neither new or an easy topic There is a myriad of techniques for implementing RWD properly and in a progressively enhancedperformant fashion I would recommend reading anything Brad Frosthttpbradfrostcom has to say See his Pattern Libraryhttpwwwpatternlabinfo for lots of good code examples Where have we succeededfailed Admitting failure is never easy and so you dont often see blog posts describing how some new feature or technology didnt quite work out as planned Im personally of the opinion that recognizing situations where weve failed and aiming to fail fast is a healthy way to react Much like in the world of distributed computing you have to accept the fact that services are going to fail and instead focus on improving the situation if it were to arise With this in mind I can say that our current deployment pipeline using Jenkins hasnt worked out that well for us as described earlier in this post Its a bit convoluted and definitely feels too complicated to be used for much longer Making changes to our deployment process is not easy and can take quite some time for an engineer to build up a mental model of the pipeline in their head We shouldnt have to be following a thread in the manner we do at the moment Also although we mark a release as failed if any of the unitintegration tests fail we still have to manually revert the release which is not good at all Its hard to explain why this hasnt been solved yet without getting lost in technical details that are very specific to our infrastructure This is a problem we are still very keen to resolve in order for us to get a true CD experience in place With that being said were aware of this problem and are proactively looking to resolve the problem by following these steps 1 Simplify the problem get a baseline that works 2 Carry out a technical spike for GOCD understand if itll fit our needs 3 Implement new solution based upon results of technical spike ie GOCD or Jenkins Template Continuous Delivery can be a hard problem to solve in a noncomplicated manner So you have to be practical with the approach for finding a solution Something else Ive realized is that were slowly cargoculting our Alephant frameworkhttpsgithubcomBBCNewsalephant By that I mean were finding ever more ways of fitting the framework to suit the project rather than designing a solution around the problem space This should not happen Each project has a unique set of requirements that need to be considered and if the problem can be solved nicely and efficiently using the predescribed brokerrenderer pattern the Alephant Framework implements then great But we shouldnt feel required to use it you know just because With regards to our abstracted RPM libraries even though weve separated certain related functionality into individual libraries we could still extract certain configuration into their own isolated libraries For example our Component Base includes setting up logging on a server instance as well as defining app specific configuration Some of our components literally only need logging provided out of the box and nothing else so extracting this into a logging library means those components can require it as a dependency while also allowing Component Base to consume it as well This gives us even more flexibility and granularity of control When we originally were building our monitoring solution we were able to prototype it really quickly using Docker We had containers linked together that were using Graphitehttpgraphitewikidotcom and Grafanahttpgrafanaorg but we soon realized this setup wouldnt scale when along came Chaos Monkey and wiped out our instance in one hit We needed these services to be decoupled to improve the resiliency This was an instance where Docker wasnt necessarily the best tool for the job This helped us to improve our solution by changing to use InfluxDBhttpinfluxdbcom instead and thus reduced the moving parts as Graphite is actually made up of three separate applications Whisper Carbon and Graphiteweb We were then able to run InfluxDB and Grafana as separate services using traditional applications rather than Docker In some places weve succeeded by catching ourselves trying to over engineer specific solutions As an example we recently had a need to archive some components that our Alephant Framework was used to render We needed to move the rendered components from one S3 bucket to another S3 bucket This archiving process would only happen once after the event the components were associated with so this wasnt a service that needed to run 247 We initially started setting up an EC2 server and all that goes along with deploying it via our own infrastructure which is an abstraction on top of AWS only to find that really the best solution was to utilize the new as of 2015 AWS Lambdahttpawsamazoncomlambda service which was built specifically for these isolated event based scenarios It helped to simplify the solution itself as well as reduce overall costs Plus we had the opportunity to play around with some new technology which is always fun Summary So here we are finally at the end of my whirlwind tour of software design and tooling Weve covered mainly backenddevops related technologies and ever so briefly considered some pointers for the frontend Its difficult to reduce many many years of frontend and backend experience into a single blog post You may as well write a book or two or three instead The idea behind this post was to provide a taster for what the current thought process is with regards to our entire tech stack as of 2015 Some of these ideas will become standard practice others will quietly fade away But we must always be open to change and experimentation and finding ways to simplify our processes rather than over engineering them and promoting unnecessary complexity I hope some of what youve read here was enlightening Feel free to get in contact with me via GitHub or Twitter using the handle integralist "},{"title":"Dev Environments Within Docker Containers","tags":["docker","go","python","vim"],"href":"/posts/dev-environments-within-docker-containers","content":" Introduction1 Python2 Go3 Mounting Volumes4 Introduction Youre a software engineer with a new laptop Youre going to be writing code in multiple languages Youre going to have projects of varying dependencies But you want to avoid the issues youve had in the past Messy installations of lots of different software packages Clogging up your system with programming language version managers You decide to use Dockerhttpswwwdockercom Its a dependency sure but youve got to install something With Docker it allows your environment to stay clean So lets see two simple examples 1 Pythonhttpswwwpythonorg 2 Gohttpsgolangorg Note I dont claim this to be the way to do this This is just a setup that works well enough for me Im also a Vim user so your mileage may vary Python You have a Python project you need to work on I wont explain how Python works Ill just presume youre a Pythonista Heres the folder structure were dealing with fooproject Dockerfile Makefile apppy requirementstxt We have a Dockerfile naturally along with a Makefile to allow us to more easily build our docker image We also have an application script a set of dependencies Nice and simple So heres what the Dockerfile looks like FROM python361 WORKDIR tmp RUN aptget update y RUN aptget install y git ncursesdev RUN git clone httpsgithubcomvimvimgit cd vim configure make make install ADD requirementstxt apprequirementstxt RUN pip install r apprequirementstxt COPY vim rootvim COPY vimrc rootvimrc WORKDIR app As you can see were building our Docker image from an official Python base image at the time of writing its the latest Python version We jump into a tmp directory so that we can install some dependencies required to get our setup just how we want it So this means installing git so we can clone down the latest build of Vim and we also install ncursesdev which is necessary in order to compile Vim After that we copy our requirementstxt file into the image and install the packages specified inside that file We also add in our vim directory and vimrc file to the image Note the Makefile has a command for copying vimvimrc into the current project directory as docker build has a specific context environment that is effectively the location of the Dockerfile Next we have the Makefile copyvimfiles if d vim then cp r HOMEvim vim fi if f vimrc then cp HOMEvimrc vimrc fi removevimfiles rm rf vim rm vimrc build copyvimfiles docker build t pythoncontainerwithvim run build docker run it v pwdapp pythoncontainerwithvim binbash clean removevimfiles docker rmi f pythoncontainerwithvim devnull true rebuild clean run Theres lots going on in there but the important thing to know is that to start up your docker container with Python and Vim preinstalled is to use make run This will copy over the host vimvimrc directoryfiles then build a new image and then call docker run where itll mount the project directory as a volume into the running container Once youre inside the container just execute vim apppy and off you go writing code Just for completion here is our application script printhi food is a thing if linting is installed properly this will error def hellomessage My summary line starts capitalized and ends with a period my bigger description is going here so pay attention to what it says printmessage The above script has some linting issues so if our packages see below are installed correctly then we should see Vim highlight the issue to us flake8321 flake8deprecated11 flake8docstrings103 flake8mock03 flake8quotes090 mypy0501 pep8naming041 pylint164 pytest305 We can see from the requirementstxt file that weve installed a few different linters along with the MyPy static analysis tool Thats it really You can reuse the Dockerfile and Makefile for all your projects as they dont do anything specific to this project Just setup the docker imagecontainer so you can execute make run and start developing Go You have a Go project you need to work on I wont explain how Go works Ill just presume youre a Gopher Heres the folder structure were dealing with barproject Dockerfile Dockerfilecompile Godeps Makefile appgo So heres our main Dockerfile FROM golang18 RUN aptget update y RUN aptget install y wget git ncursesdev time WORKDIR tmp RUN git clone httpsgithubcomvimvimgit cd vim make make install WORKDIR gosrc COPY vim rootvim COPY vimrc rootvimrc COPY Godeps gosrc RUN wget httpsrawgithubusercontentcompotegpmv140bingpm chmod x gpm mv gpm usrlocalbin RUN gpm install RUN cp r githubcom githubcom backup packages to root to prevent volume mount removing it Install Go binaries that are utilised by the vimgo plugin httpsgithubcomfatihvimgoblobmasterplugingovimL9 We dont manually install them we let vimgo handle that We use vims execute command to pipe commands This helps avoid Press ENTER or type command to continue RUN time vim c execute silent GoUpdateBinaries execute quit Again were not doing anything too crazy not until the end which Ill explain Were building a new image from an official base image then were installing dependencies that allow us to manually compile the Vim editor Next we copy over our vim files and the Godeps dependencies file and we install our dependency manager gpmhttpsgithubcompotegpm and install the packages we want to use within our application Next we back up the installed depedencies githubcom to another directory The reason we do that is because when we mount our host project directory into the running container we will end up accidentally replacing the installed packages Finally we run vim and pass it a script to be executed once Vim has loaded What this does is allow the statically built image to include the updated set of depedencies that the vimgohttpsgithubcomfatihvimgo plugin requires I could have installed them manually but then using the built in command provided by vimgo means I dont have to ensure my list of go tools still matches up to what vimgo is using The downside to this is that when you build the image youll see for 2030 seconds Vim started and you wont be able to interact with it at all during that time This is because its installing the dependencies it uses But after that Vim will close and youll be placed at the containers shell prompt From there you can run Vim and start coding Heres the Go Makefile it works similarly to the Python one bin usrlocalbinfastly vimdir vim vimrc vimrc containerenv gocontainerwithvim containercompiler gocompiler copyvimfiles if d vimdir then cp r HOMEvim vimdir fi if f vimrc then cp HOMEvimrc vimrc fi removevimfiles rm rf vimdir devnull true rm vimrc devnull true removecompiledfiles rm fastlydarwinlinuxwindowsexe devnull true clean removevimfiles removecompiledfiles docker rmi f containerenv devnull true docker rmi f containercompiler devnull true uninstall clean rm bin devnull true build copyvimfiles docker build t containerenv dev build removevimfiles docker run it v pwdgosrc containerenv binbash rebuild clean run compile docker build t containercompiler f Dockerfilecompile docker run it v pwdgosrc containercompiler true copybinary cp fastlydarwin bin install compile copybinary removecompiledfiles One thing I did this time was change the make run for make dev as I feel thats more indicative of what were doing the run suggests were running our application when were really just wanting to drop into a development environment Theres a few more steps in the Go Makefile and thats just for the purposes of having a separate Dockerfile for compiling our application The following is the other Dockerfile we have in our project FROM golang18 WORKDIR gosrc COPY Godeps gosrc COPY compilesh gosrc RUN aptget update aptget install wget RUN wget httpsrawgithubusercontentcompotegpmv140bingpm chmod x gpm mv gpm usrlocalbin RUN gpm install RUN cp r githubcom githubcom backup packages to root to prevent volume mount removing it CMD compilesh This Dockerfile does much the same thing get dependency file install those specified dependencies then back them up to another directory This time though when the container is run we use a separate script as the CMD value as our script was getting quite long as youll see Here is the contents of compilesh Note make sure you chmod x compilesh from your host binsh copy packages back into our source code directory cp fr githubcom githubcom compile application for the major operating systems gox osarchlinuxamd64 osarchdarwinamd64 osarchwindowsamd64 outputfastlyOS run the relevant compatible compiled binary for this containers OS fastlylinux The tasks we run are 1 copy the backed up dependencies back into the mounted project directory 2 build the app using the default compiler for the OS 3 execute the compiled binary to show it can run correctly inside the container this means our compiled binary will be a linux based binary so you cant run it on your host machine if its not linux based eg Im using macOS Youll see that to allow me to compiled my application for multiple OSs Ive installed Goxhttpsgithubcommitchellhgox Now here is our Go application it simply uses the logging dependency weve installed and thats it Nothing too fancy necessary for this example package main import fmt log githubcomSirupsenlogrus func init logSetLevellogDebugLevel otherwise would not be shown func main fmtPrintlnHello World logger logWithFieldslogFields name helloworldapp loggerDebugthis is my debug log message loggerInfothis is my info log message loggerWarnthis is my warn log message loggerErrorthis is my error log message loggerFatalthis is my Fatal log message loggerPanicthis is my Panic log message we dont actually reach here Here is our dependency files content where we can see the Logrushttpsgithubcomsirupsenlogrus dependency weve specified as well as Gox for the purposes of our container responsible for compiling our application for multiple OS githubcommitchellhgox c9740af9c6574448fd48eb30a71f964014c7a837 githubcomsirupsenlogrus 10f801ebc38b33738c9d17d50860f484a0988ff5 Mounting Volumes Just remember that when making changes inside the container because youve mounted your host project directory as a volume if you make a change or add a new file or compile something inside of the container then itll be available on your host machine This is all fine but you might want to look at setting up a gitignore file to ensure you dont accidentally commit any unwanted items into your git repository "},{"title":"Engineer to Manager","tags":["engineer","leads","management"],"href":"/posts/engineer-to-manager","content":" Youre a software engineer with many years of technical experience and youve decided you want to kickstart the process of working towards becoming an Engineering Manager So what do you need to know Lets discuss Rolesroles Responsibilitiesresponsibilities Communicationcommunication Team Leaderteamleader Whywhy Incremental Stepsincrementalsteps Manager Cheat Sheetmanagercheatsheet Conclusionconclusion Roles As a software engineer you have a few different paths available to you for career progression Senior Software Engineer Staff Software Engineer Principal Software Engineer Architect Tech Lead Engineering Manager Note the role hierarchy can change per organisation You may notice some of these roles are not like the others Its worth recognizing that there is a slight separation between the last two roles Tech Lead Engineering Manager and the rest of the roles in that the last two are more closely related to management than the other roles Ive grouped these roles together like this because they tend to be the progression path for a lot of engineers They exhaust all possibilities as far as engineering as an IC individual contributor before finally residing themselves to a management role This doesnt have to be the case though You might decide to make that shift to a more management role before progressing through the more senior engineering roles but you may find some organisations will expect a certain level of engineering experience When moving up the career ladder youll typically find that between roles there is a fair amount of crossover responsibilities but for the purposes of this post lets focus in on the last role Engineering Manager and what that entails Responsibilities Below I have listed a few different areas of responsibility as I see them associated with an Engineering Manager role and of course these responsibilities can change depending on the culture of where you work Meaning its a good idea when interviewing for an Engineering Manager role to ask clarifying questions in order to help you understand the expectations of that role in the specific context of that organisation Note to learn more about what to talk about during an interview read my post Interview Topicspostsquestionswheninterviewing For example some companies have dedicated Project Managers meaning an Engineering Manager doesnt have to worry too much with that aspect of the job and can instead focus on technical leadership responsibilities Another example is the role of a Tech Lead some organisations use this name and Engineering Manager interchangeably while others treat them as distinct roles and in some cases a less managementmore code oriented role than an Engineering Manager Note see also my post Project Management in Five Minutespostsprojectmanagementinfiveminutes which discusses Lara Hogans venn diagram of various leadership roles as well as a project management checklist Motivation A good engineering manager must be committed to building great software great teams creativity careers and general welfare of the team members With that in mind the following list feels like to me a reasonable breakdown of the various jobs you as an Engineering Manager will be expected to look after Team Leader 11s read my post on better 11 practicesposts11 Mentoring Support Career development Technical Lead Define and communicate with nontechnical stake holders Guide the architecture and technical design Resolve conflicts around technical discussions Write code and work with the engineering team Project Management Define technical requirements Work with nontechnical stake holders to define user stories Breakdown large tasks into smaller tasks Prioritise work and revise priorities when necessary Establish schedules estimates and milestone deliveries Run and guide the technical aspect of the project Communicator Up teamproject status and health to upper management Down updatesdecision making to your team and direct reports Across identify engage and collaborate with other teams Recruitment Work with recruiters to define required engineering roles Screen resumes Interview candidates Assist onboarding new recruits Communication The most important skill youll need if you are to be effective as a manager of people eg your direct reports your managers your stake holders both technical and nontechnical is communication You need to be able to build strong and trusted relationships with these people You need to be able to challenge them and push them to do better for their own success not the business To elaborate further helping people to reach their goals to be happy and to help them find doing what they enjoy doing while still challenging them will ultimately yield better value from the work they do and as a sideeffect the business will benefit more greatly than if you push someone down another path designed with the business in mind This means caring personally about the people you are interacting with and doing what you can to bring out their best It also means recognising when someone isnt a good fit for your team and in extreme cases letting them go can actually work out better for both them and the business Patty McCord discusses this topic indepth in her book on culture building at scale Powerful building a culture of freedom and responsibilityhttppattymccordcombook In essence lets help people work on things that will realize their ambitions but not at the expense of the business more in cooperation with the business But if your ambitions dont fit with the business then maybe its best for you and the business to separate ways I like that as a healthy way for both sides to either work together for a common good or separate ways for an equally better outcome When building relationships you should be mindful of not falling too heavily on one side or another with regards to professional vs personal Find a balance and if unsure err on the side of building a respected professional relationship as friendship and a greater sense of personal appreciationunderstanding in some cases will naturally occur Below Ive listed some useful resources for learning how to communicate effectively these are all highly recommended Authentic Communicationhttpswwwsoundstruecomstoreauthenticcommunicationhtml by Fred Kofman Radical Candorhttpswwwradicalcandorcom by Kim Scott SBI Situation Behaviour ImpacthttpsgistgithubcomIntegralist24c8a9ce570d78d37ed0cf9967594e0e a methodology developed by The Center for Creative Leadership The Hard Thing About Hard ThingshttpswwwamazoncoukHardThingAboutThingsBuildingdp0062273205refasapbc by Ben Horowitz Note Authentic Communication is actually taken from a larger work called Conscious Business How to build value through valueshttpswwwsoundstruecomstoreconsciousbusiness4036html Team Leader As someone who is going to be mentoring and supporting a team of engineers it is important you understand the motivations and what drives your team In doing so youll be better equipped to find them the right roles within the organisation that brings out their best work and allows them to grow in the areas that matter the most to them Its also important to sometimes think as an engineer again When you design and develop large scale systems you identify bottlenecks and ways of working that reduce complexity and support certain services to scale Now think about that when youre handling management duties Do you find yourself handling tasks or using processes that take up a lot of time This might be ok for now while youre responsible for a small team but how will that work out for you once you start taking on more teams and more responsibilities Be mindful of the tools and processes you use and refine them constantly Why The single biggest question you should now be asking yourself is Why do I want to become an Engineering Manager This is really important and requires you to be very honest and open with yourself not only will you suffer the consequences of a bad decision but so will everyone else who is relying on you If you answered the question with any one of the following or more then you need to seriously consider an alternative future career path simply because you truly wont be happy in this role otherwise I want more money I want a promotion I want a better job title Im bored of writing code To me a good reason to want to progress towards being an Engineering Manager is when you deep down know that your enjoyment comes from helping people and having a positive impact in their ability to do great work and helping them to progress in their career If that is your motivation then you have the right mentality for this type of role If thats not your intent then thats OK dont worry but you will need to consider alternative options and really figure out what it is about working in techengineering that you enjoy and do what you can to move more in that specific direction Ultimately if you want things like more money or a promotion or a better job title that will require you to have more impact at work and to do that means you need to lift your head up from your laptop and engage more widely with the organisation and the teams within and start demonstrating more broadly the potential positive impact that you can have Note see also my post on Architecture Interviews and other thoughtspostsarchitectureinterview where I cover what I look for when interviewing an Engineering Manager candidate Incremental Steps You might not be quite ready for a straight jump up to Engineering Manager so what can you do in the meantime to help you move towards that goal Here are some ideas Mentoring Most companies have a mentoring program and even if not it doesnt prevent you from reaching out to colleagues and offering them your help guidance and the benefits of your experience if they choose to accept it Take Responsibility Get involved with crossteam discussions Create a workinggroup for a topic you care about Support your team and reach out on their behalf if youre able to Lead Take the responsibility of leading a project Beyond that take the opportunity to lead a team Help nontechnical stakeholders understand the teams work Manager Cheat Sheet BuzzFeedhttpswwwbuzzfeedcom recently hired a new Engineering Manager Her name is Svetlana BozhkohttpstwittercomSBozhko and she kindly shared with me a cheat sheet of sorts that helps her stay ontrackfocused I know of other Engineering ManagersDirectors of Engineering some of whom work for a few of the big 4 who use a similar approach and its something I personally like Heres an example of what such a cheat sheet might look like Daily Personal Am I ready for todays meetings Can I reduce the length of those meetings Can I cancel any of those meetings People Recruiting Hiring Interviewing Candidate followups Sell the company its vision roles etc Close great candidates Observe team health Casual feedback sessions Processes Daily standups Am I ready for these standups Any important companywide updates Projects Are we on track with critical projects Any docsguides needed to be written Any PRs waiting my approvalfeedback Should I step in on any engineering tasks Weekly Personal Have I setup my goals for thisnext week How do I progress with my goals People 11s and enhancing their strengths Who needs additional feedback Any tasks I can delegate Processes Review all tasks in backlog Any designarchitecture discussions Divide tasks into smaller ones Plan team retrospectives Projects Give status reports to my managers Who else do I need to update Monthly Personal What do I need to do to be more successful in my role Any training I can do mentoring I can give books I can read People Ensure team vision is aligned Ensure team expectations are understood Any team events we should consider 11s focused specifically on career growth Anyone moved into new roles Do I need to set new expectations Any changes to the career ladder What changes would make it more fun to work here Processes Challenge and question existing processes Are the current sprint metrics still useful Projects Any strategic ideas to improve current projects Do we need a new project Dow we need to stop a project Any strategic hires needed Budget and capacity planning Conclusion So what do you think Is there anything here that you disagree with or something you feel I missed altogether Let me know on twitter "},{"title":"Fastly Varnish","tags":["cdn","fastly","varnish","vcl"],"href":"/posts/fastly-varnish","content":" Introduction1 Varnish Default VCL2 Fastly Custom VCL3 Fastly Request Flow Diagram4 State Variables41 Persisting State42 Hit for Pass5 Serving Stale6 Logging7 Conclusion8 Introduction Varnishhttpsvarnishcacheorg is an opensource HTTP accelerator More concretely it is a web application that acts like a HTTP reverseproxyhttpsenwikipediaorgwikiReverseproxy You place Varnish in front of your application servers those that are serving HTTP content and it will cache that content for you If you want more information on what Varnish cache can do for you then I recommend reading through their introduction articlehttpsvarnishcacheorgintroindexhtml and watching the video linked there as well Fastlyhttpswwwfastlycom is many things but for most people they are a CDN provider who utilise a highly customised version of Varnish This post is about Varnish and explaining a couple of specific features such as hitforpass and serving stale and how they work in relation to Fastlys implementation of Varnish One stumbling block for Varnish is the fact that it only accelerates HTTP not HTTPS In order to handle HTTPS you would need a TLSSSL termination process sitting in front of Varnish to convert HTTPS to HTTP Alternatively you could use a termination process such as nginx behind Varnish to fetch the content from your origins over HTTPS and to return it as HTTP for Varnish to then process and cache Note Fastly helps both with the HTTPS problem and also with scaling Varnish in general Varnish Basics Varnish is a state machine and it switches between these states via calls to a return function where you tell the return function which state to move to The various states are recv request is received and can be inspectedmodified hash generate a hash key from hostpath and lookup key in cache hit hash key was found in the cache miss hash key was not found in the cache pass content should be fetched from origin regardless of if it exists in cache or not and response will not be cached pipe content should be fetched from origin and no other VCL will be executed fetch content has been fetched we can now inspectmodify it before delivering it to the user deliver content has been cached or not if we had used returnpass and ready to be delivered to the user For each state there is a corresponding subroutine that is executed It has the form vcl and so there is a vclrecv vclhash vclhit etc So in vclrecv to change state to pass you would execute returnpass If you were in vclfetch and wanted to move to vcldeliver then you would execute returndeliver Note vclhash is the only exception because its not a state per se so you dont execute returnhash but returnlookup as this helps distinguish that were performing an action and not a state change ie were going to lookup in the cache The reason for this post is because when dealing with Varnish and VCL it gets very confusing having to jump between official documentation for VCL and Fastlys specific implementation of it Even more so because the version of Varnish Fastly are using is now quite old and yet theyve also implemented some features from more recent Varnish versions Meaning you end up getting in a muddle about what should and should not be the expected behaviour especially around the general request flow cycle Ultimately this is not a VCL 101 If you need help understanding anything mentioned in this post then I recommend reading Varnish Bookhttpbookvarnishsoftwarecom40 Varnish Bloghttpsinfovarnishsoftwarecomblog Fastly Bloghttpswwwfastlycomblog Fastly has a couple of excellent articles on utilising the Vary HTTP header highly recommended reading Varnish Default VCL When using the opensource version of Varnish youll typically implement your own custom VCL logic eg add code to vclrecv or any of the other common VCL subroutines But its important to be aware that if you dont return an action eg returnpass or trigger any of the other available Varnish states then Varnish will continue to execute its own builtin VCL logic ie its builtin logic is appended to your custom VCL You can view the default or builtin logic for each version of Varnish via GitHub Varnish v21httpsgithubcomvarnishcachevarnishcacheblob21binvarnishddefaultvcl the version used by Fastly Varnish v30httpsgithubcomvarnishcachevarnishcacheblob30binvarnishddefaultvcl Varnish v40httpsgithubcomvarnishcachevarnishcacheblob40binvarnishdbuiltinvcl Varnish v50httpsgithubcomvarnishcachevarnishcacheblob50binvarnishdbuiltinvcl Note after v3 Varnish renamed the file from defaultvcl to builtinvcl But things are slightly different with Fastlys Varnish implementation which is based off Varnish opensource version 215 Specifically no returnpipe in vclrecv they do pass there some modifications to the synthetic in vclerror Fastly Custom VCL On top of the builtin VCL the opensource version of Varnish uses Fastly also includes its own custom VCL logic alongside your own additions You can see Fastlys VCL boilerplate and learn more about their custom VCL implementation herehttpsdocsfastlycomguidesvclmixingandmatchingfastlyvclwithcustomvcl You can also view their generated custom VCL here in this isolated gist for reference purposes Fastlys Custom VCLhttpsgistgithubcomIntegralist56cf991ae97551583d5a2f0d69f37788 Fastly Request Flow Diagram There are various request flow diagrams for Varnish examplehttpbookvarnishsoftwarecom40imagessimplifiedfsmsvg and generally they separate the request flow into two sections request and backend So handling the request looking up the hash key in the cache getting a hit or miss or opening a pipe to the origin are all considered part of the request section Whereas fetching of the content is considered part of the backend section The purpose of the distinction is because Varnish likes to handle backend fetches asynchronously This means Varnish can serve stale data while a new version of the cached object is being fetched This means less request queuing when the backend is slow But the issue with these diagrams is that theyre not all the same Changes between Varnish versions and also the difference in Fastlys implementation make identifying the right request flow tricky Below is a diagram of Fastlys VCL request flow including its WAF and Clustering logic This is a great reference for confirming how your VCL logic is expected to behave State Variables Each Varnish state has a set of builtin variables you can use Below is a list of available variables and which states theyre available to Based on Varnish 30 which is the only explicit documentation I could find on this although you can see in various request flow diagrams for different Varnish versions the variables listed next to each state But thishttpbookvarnishsoftwarecom30VCLfunctionshtmlvariableavailabilityinvcl was the first explicit list I found Heres a quick key for the various states R recv F fetch P pass M miss H hit E error D deliver I pipe hash RFPMHEDI "},{"title":"Functional Recursive JavaScript Programming","tags":["functional","javascript","recursion"],"href":"/posts/functional-recursive-javascript-programming","content":" Introduction1 The problem2 Tail Call Optimisation3 The solution4 Trampolining5 A more advanced solution6 Explanation7 Conclusion8 Introduction This post isnt about Functional ProgramminghttpenwikipediaorgwikiFunctionalProgramming per se as there are many parts to pure FP seen in Lisp Clojure Haskell etc which we cant achieve completely in the JavaScript language for example implementing fundamental principles of FP such as immutable data is very difficult in a language designed around allowing data structures to be mutable Although immutable data would be a lot easier with ES5 getterssetters and ES6 proxies What we do cover is recursion and a problem known as tail call optimisation and how to work around it in JavaScript The problem In JavaScript if a function calls itself recursively then the JavaScript engine has to create whats called a new stack A stack is a chunk of memory allocated to help keep track of all the information related to the function at the point of execution such as its arguments and their initialised values Here in lies the problem creating stacks is expensive as the JavaScript engine can only create as many stacks as it has memory available If we write a function that recursively calls itself many times then well find that we can exhaust the memory allocation and trigger an error Lets look at some code that demonstrates this The following function sum simply adds two numbers together but for the purpose of demonstrating recursion a potentially complex topic and making it easier to understand we have written the sum function in a very convoluted way function sumx y if y 0 return sumx 1 y 1 else return x sum1 10 11 Lets quickly explain what the code does We pass in two numbers which are assigned to the parameters x and y in this instance we pass 1 and 10 We check if y is greater than 0 If y is greater than 0 which we know it is then we recursively call sum again but this time modify the arguments so that x is incremented by 1 and y is reduced by 1 When the sum function is next called were passing 2 and 9 At this point were now back inside the sum function but the first call to sum has yet to finish as we didnt reach the end of the function the focus moved to another function being executed which in this case was sum again Also at this point the JavaScript engine has two stacks One for the point in time when we passed in 1 and 10 and now as it has to remember 2 and 9 The JavaScript engine has to remember the previous arguments 1 and 10 because potentially once this second sum execution finishes well end up back in the first execution context As we can see in the above code the JavaScript engine has to create a new stack for each recursive call For a small sum of 1 and 10 this is fine but if we try to sum 1 and 100000 then thatll require more stacks than we have memory to allocate This will cause an error like so sum1 100000 RangeError Maximum call stack size exceeded Recursion in the form weve written it above requires many stacks to be created because of how the function is calling itself The first call to the sum function ie sum1 100000 doesnt complete until the very last call to sum returns the value of x ie sum0 99999 is technically the final execution When the final call to sum occurs and we discover that y is no longer greater than zero we return the accumulated value which has for each recursive call been stored inside the argument x That returned value needs to then be passed back through each function execution context effectively closing each stack until we reach the very first function execution context that was opened when we ran sum1 100000 If we create a stack deep enough such as we did with sum1 100000 then the JavaScript engine will throw a Maximum call stack size exceeded error This problem can occur very easily as weve seen in this simple code example Tail Call Optimisation In other programming languages the recursion could be rewritten in such a way that the engine would recognise a recursive execution was happening and optimise the code internally into a loop form This is called a tail call optimisation TCO Unfortunately the JavaScript language doesnt implement this optimisation Note apparently ECMAScript 6 has plans to implement TCO The following is an example of a tail optimised version of the previous code function sumx y function recura b if b 0 return recura 1 b 1 else return a return recurx y sum1 10 11 This works because the recur function ends up being the last invocation of the sum function Now the engine is able to construct internal code that executes recur inside of a loop instead of via recursion As mentioned above this optimisation has no effect in the current version of the JavaScript language fingers crossed ES6 will implement TCO so the above code would work fine For now we need an alternative solution The solution The fix to this problem in JavaScript at least is to consume fewer stacks One way we could do this is to rewrite the code to not be recursive so in other words use a loop Effectively we would be producing our own implementation of TCO remember that TCO recognises a recursion and internally implements it as a loop The problem with using a loop is that its not as elegant as the recursive style we associate with functional programming Another solution is to use a type of functional pattern called trampolining Lets take a look at one implementation of it Trampolining Note the following code and my explanation of it assumes an understanding of the this keyword and changing its context If youre unsure about what this means then read more about it herehttpsgithubcomgetifyYouDontKnowJSblobmasterthis2020object20prototypesREADMEmd function trampolinef while f f instanceof Function f f return f function sumx y function recurx y if y 0 return recurbindnull x 1 y 1 else return x return trampolinerecurbindnull x y sum1 10 11 The reason the above code works is because weve replaced our functional recursion style with a loop In the above code we dont create a deep nest of stacks because the sum function only gets called once The trampoline function also is only called once and the recur function inside of sum although called multiple times is called via a loop again no more than one stack is required at any one time The code breaks down like so We call sum1 10 Our sum function ultimately returns a value In this case whatever is returned by calling the trampoline function The trampoline function accepts a reference to a function as its argument its important to understand it needs a reference to a function doing return trampolinerecurx y wouldnt work as that would end up passing the result of calling recurx y to the trampoline function So we use Functionbind to pass a reference to the recur function we use null as the this binding because it doesnt matter what the context the function executes in as we dont use the function as a constructor When we execute sum1 10 we pass the reference to the internal recur method to the trampoline function The trampoline function checks if we passed a function and if so we execute the function and store its return value inside the f variable If what we pass into the trampoline function isnt a function then we assume its the end ie accumulated value and we return the value straight back to the sum function which returns that value as the accumulated value Inside the recur function we check to see if y is greater than zero and if it is we modify the x and y values like we did in the previous example and then return another reference to the recur function but this time with the modified x and y values Inside the trampoline function the newly referenced function is assigned to the f variable and the while loop on its next iteration checks to see if f is indeed a function or not If it is which it would be in this instance we again execute the function which is now recur2 9 and the whole process starts again Until of course we reach the point where y is set to zero Then when the trampoline function executes the function reference recur and inside the if conditional fails as y is now zero and no longer greater than zero and so we return the accumulated x value which then gets sent back and stored in the f variable inside the trampoline function On the next iteration of the while loop f is now a value and not a function and so the while loop ends and the accumulated value is returned to the sum function which returns that as its accumulated value A more advanced solution The previous code we just looked at works fine but it required us to modify our code to work with the trampoline pattern This is a bit of a pain and means if we have lots of recursive code then it means each one might need subtle changes to accommodate this pattern The following code is an abstraction around that concept and itll allow us to keep our code exactly the same with no modifications and the abstraction will handle all of the work for us Lets take a look at the implementation function tcof var value var active false var accumulated return function accumulator accumulatedpusharguments if active active true while accumulatedlength value fapplythis accumulatedshift active false return value var sum tcofunctionx y if y 0 return sumx 1 y 1 else return x sum1 100000 100001 Here weve written a tco function which simply wraps around our original code so no modification required to our code Lets now take a look at how it works Explanation Be warned that understanding this code could take a while lord knows it took me long enough to figure it out thats what prompted me to write this post so I wouldnt forget If you dont understand how the code works after the first time reading through these notes then its probably best to execute the above code via your browser of choices developer tools and use a debugger statement to step through the code whilst reading this explanation Note the above code was written by Irakli Gozalishvili an engineer at Mozilla httpsgistgithubcomGozala1697037 As you can see above it makes tail call optimising any function really easy We store the result of calling tco wrapped around our code into the sum variable The sum variable is now a function expression that when called eg sum1 10 will execute the accumulator function that tco returned The accumulator is a closure meaning when we call sum the accumulator will have access to the variables defined inside of tco value active and accumulated as well as our own code which is accessible via the identifier f When we call sum for the first time eg sum1 10 we indirectly execute accumulator The first thing we do inside of accumulator is push the arguments object an Arraylike object into the accumulated Array so the accumulated will now have a length of 1 Its worth knowing that the accumulated Array only ever has 1 item inside of it as well soon see The active variable by default is false So as accumulator is called for the first time we end up inside the if conditional and then reset active to true Now we get to a while loop were still calling functions recursively as youll see in a moment but this is a very clean loop compared to an ugly for loop with lots of operatorsoperands The while loop simply checks whether the accumulated Array has any content If it does then we call the f function and pass through the arguments that were inside accumulated0 for the first run of this function that wouldve been 1 10 When we pass the contents of accumulated0 we use the shift Array method to actually remove it from the accumulated Array so it now has a length of zero If we ignore for a moment the code inside the loop on the next iteration of this loop the condition of accumulatedlength will fail and so we would end up setting active to false and ultimately return to sum the value of the variable value This is where it gets confusing but hold tight The f function is our own code Our own code calls the sum function which indirectly calls the accumulator function The secret sauce to this entire chunk of code is coming up If our code returns x then the while loop will stop Ill explain why in a moment If our code cant return x notice our code has a conditional check to see if y is greater than zero if it is then we return x otherwise we call sum again and pass through modified arguments This time when we call sum we dont get inside of the if conditional because active is already set to true So the purpose of calling sum from inside our own code is simply to allow us to store the newly modified arguments inside the accumulated Array The sum function then returns undefined as we werent able to move into the if conditional The flow of control then throws us back into the original while loop thats still going it hasnt stopped yet and undefined is whats stored into the value variable At this point the accumulated Array has once again got some content and so the while loops condition passes once more And that is where the recursive magic happens At some point our code is going to call sum with the y argument set to zero meaning that when the accumulator function calls our code the condition y 0 will fail and so we return the value of x which has been incremented each time along the way When that happens x is returned and assigned as the value to the value variable and so our code never called sum and thus the accumulated Array is never modified again meaning the while loop condition inside the accumulator function will fail and thus the accumulator function will end returning whatever value is stored inside the value variable which in this example is the value of x Conclusion There you have it a quick run through of TCO what it means and how potentially to work around it in JavaScript Go forth and be productive Update 1 Ive since discovered a nice explanation of trampolining in the JS Drip Newsletter 65 The following code is an example that is NOT tailcall optimised function isEvenNaive num if num 0 return true if num 1 return false return isEvenNaiveMathabsnum 2 isEvenNaive10 true isEvenNaive9 false isEvenNaive99999 InternalError too much recursion The following code snippet demonstrates the first step towards trampolining which is to flatten the stack by requiring the user to manually execute the function over and over function isEvenInner num if num 0 return true if num 1 return false return function return isEvenInnerMathabsnum 2 isEvenInner8 function return isEvenInnerMathabsnum 2 isEvenInner8 true The first thing to notice about our isEvenInner function is that instead of directly calling itself again it returns an anonymous function That means each call to isEvenInner gets resolved immediately and doesnt increase the size of the stack It also means that we need a way to automatically invoke all of those anonymous functions that will get returned along the way Thats where trampoline comes in The trampoline function effectively turns this recursive algorithm into something that is executed by a while loop As long as isEvenInner keeps returning functions trampoline will keep executing them When we finally reach a nonfunction value trampoline will return the result Now finally lets see an example that automates the execution of the flatterned stack function trampoline func arg var value funcarg whiletypeof value function value value return value trampolineisEvenInner 99999 false trampolineisEvenInner 99998 true var isEven trampolinebindnull isEvenInner isEven99999 false "},{"title":"gRPC for Beginners","tags":["go","grpc","rpc"],"href":"/posts/grpc-for-beginners","content":" Introduction1 Install gRPC2 Install Proto Buffer Compiler3 Hello World Proto Definition4 Ruby Example5 Go Example6 Conclusion7 Introduction I started designing a new microservice that I wanted to write in Gohttpsgolangorg The service was to be a JSON RPC service over TCP and the expected consumer servicer I would build using Rubyhttpswwwrubylangorg I had some initial concerns regarding the nature of TCP sockets with a highly scalable and distrubuted set of services this was to be utilised within the BBChttpwwwbbccouknews so these are genuine concerns to be had for my purposes and so I decided to do some research There are a few issues that I discovered 1 TCP connections arent free so utilise a connection pool 2 Too many simultaneous requests can exhaust open portfile descriptors 3 If not careful you can end up with orphaned TCP connections eg if no timeouts configured Now these are all things that can be worked around or I could just build the Go service as a HTTP REST service and some of these problems dissapear But it seems lots of people have been talking about using Googles new opensource RPC framework called gRPChttpwwwgrpcio and I thought what better time to investigate what it can do Google define gRPC as A high performance open source general RPC framework that puts mobile and HTTP2 first How does it work One of the initial benefits is the ability to be able to define and codify your service requirements via a proto file The proto file is based around another concept Google have been working on known as Protocol Buffershttpsdevelopersgooglecomprotocolbuffersdocsoverview In essence protocol buffers are an opensource mechanism for serializing structured data Once you have your service defined you can utilise a command line compiler to generate stubs and code in multiple programming languages So you can generate a client and server with the Go programming language and then using the same proto file generate a clientserver with Ruby Google have built gRPC on top of the HTTP2 standardhttpshttp2githubio meaning you get features such as bidirectional streaming flow control header compression and multiplexing requests over a single TCP connection See herehttpwwwgrpciopostsprinciples for Googles motivation and design principles around gRPC Now the reason for this post is that I didnt find the documentation to be that intuitive I thought I might be able to help people get started more quickly by detailing the steps in a more succinct fashion than found in Googles documentation thus opening up gRPC to more users So with this in mind lets crack on Install gRPC First thing we need to do is install gRPCs C based libraries Once we have this installed we will later install plugin extensions for other programming languages such as Go and Ruby but there are other languages available One of the things I discovered further along in my research of gRPC and I wish I had known earlier was that some commands that are utilised by the language extensions are only available when installing gRPC from source So thats what well be doing now git clone httpsgithubcomgrpcgrpcgit cd grpc git submodule update init make make install Install Proto Buffer Compiler Now that we have gRPC installed we also need the compiler for the Protocol Buffer definition file This is the file that defines our service and which well get round to writing shortly In order to install the compiler youll first need to make sure you have the requisite dependencies installedhttpsgithubcomgoogleprotobuftreemastersrc For myself running this on Mac OS X I just need XCode installed sudo xcodeselect install Once you do that you can execute the following steps git clone gitgithubcomgoogleprotobufgit autogensh configure make make check sudo make install Note each Make target took 10mins each to run Hello World Proto Definition Protocol Buffers are designed by Google to be language and platform neutral and so in theory you can use it with your own RPC implementation But in reality most people will use gRPC with Protocol Buffers So with that said here is our service definition written using the latest syntax proto3 and Ive named it requesterproto syntax proto3 package requester service Requester rpc Process Config returns Response message Config string data 1 message Response string message 1 Note see herehttpsdevelopersgooglecomprotocolbuffersdocsproto3specifyingfieldtypes for full proto3 syntax documentation In summary it defines an RPC service that exposes a Process method which can be called remotely In reality its the same Hello World app provided by the gRPC docs but with some changes in identifiers Syntax Explanation You can see the package statement near the top which according to Googles docs are used to avoid name clashes between protocol messages but more specifically it effects the way code is compiledgenerated For example in Ruby itll generate a top level module that utilises that namespace As youll see shortly I have two nested modules with the same name RequesterRequester This is because the package setting is the top level and the nested module name is because thats what I named the service So be careful what you name it as the compiled code might not be what you want Note because the design has come from Google youre going to notice lots of design considerations that correlate to their opinions and choices with the Go programming language In Go the other language were using the package value is used conveniently as the name of the Go package Which makes sense as there is a closer correlation in the design of Protocol Buffers and Go vs a dynamic language such as Ruby Inside of the service statement we state that we want an RPC service that has a Process method and that method accepts something of type Config and returns something of type Response We can then define what Config and Response look like which we do using the message statement So to keep things simple Ive only used a single property setting for each message but there is a rich selection of data types you can utilise In my simple example both properties have a string type You can then access these properties from your code as a nested object fieldproperty So in Ruby for example if you accepted the message Config as an argument c to your Process method then your code would call cdata The numbers assigned to the property eg both data and message are assigned the value 1 are known as tags Effectively tags with a number between 1 and 15 take one byte to encode whereas tags between 16 and 2047 take two bytes to encode The idea is that you should reserve the tags 1 through 15 for very frequently occurring message elements But if you really want all the gory details then Ill refer you to the encoding documentationhttpsdevelopersgooglecomprotocolbuffersdocsencoding Auto Generating Service Code So at this point we have the option of autogenerating service code for any of the languages gRPC supports which is C C C Go Java Nodejs ObjectiveC PHP Python Ruby Were interested in Go and Ruby as I want to have the RPC service server side running in Go but have the consumer in Ruby So Ill first generate the Ruby client stub using the protoc compiler we installed earlier protoc rubyoutlib grpcoutlib pluginprotocgengrpcwhich grpcrubyplugin requesterproto Note execute mkdir lib if that directory doesnt already exist This will generate two files requesterrb and requesterservicesrb inside of the lib directory weve specified The content of those files looks like the following The first file being requesterrb Generated by the protocol buffer compiler DO NOT EDIT source requesterproto require googleprotobuf GoogleProtobufDescriptorPoolgeneratedpoolbuild do addmessage requesterConfig do optional name string 1 end addmessage requesterResponse do optional message string 1 end end module Requester Config GoogleProtobufDescriptorPoolgeneratedpoollookuprequesterConfigmsgclass Response GoogleProtobufDescriptorPoolgeneratedpoollookuprequesterResponsemsgclass end Here is the second file requesterservicesrb Generated by the protocol buffer compiler DO NOT EDIT Source requesterproto for package requester require grpc require requester module Requester module Requester TODO add proto service documentation here class Service include GRPCGenericService selfmarshalclassmethod encode selfunmarshalclassmethod decode selfservicename requesterRequester rpc Process Config Response end Stub Servicerpcstubclass end end Well see how to consume these stubs from Ruby in the next section But now lets move onto how to use protoc to generate some Golang stubs protoc gooutpluginsgrpcpb requesterproto So in the above example the pb reference is to a folder that has to exist before you run that command You can name the folder whatever you like obviously but pb protocol buffer made sense to me The file that is generated will be named requesterpbgo and as with the Ruby code well look at how to consume this file in a following section that demonstrates the Go code examples But for now lets see the contents of this file shield your eyes Go isnt the most concise programming language Code generated by protocgengo source requesterproto DO NOT EDIT Package requester is a generated protocol buffer package It is generated from these files requesterproto It has these toplevel messages Config Response package requester import proto githubcomgolangprotobufproto import fmt fmt import math math import context golangorgxnetcontext grpc googlegolangorggrpc Reference imports to suppress errors if they are not otherwise used var protoMarshal var fmtErrorf var mathInf This is a compiletime assertion to ensure that this generated file is compatible with the proto package it is being compiled against const protoProtoPackageIsVersion1 type Config struct Name string protobufbytes1optnamename jsonnameomitempty func m Config Reset m Config func m Config String string return protoCompactTextStringm func Config ProtoMessage func Config Descriptor byte int return fileDescriptor0 int0 type Response struct Message string protobufbytes1optnamemessage jsonmessageomitempty func m Response Reset m Response func m Response String string return protoCompactTextStringm func Response ProtoMessage func Response Descriptor byte int return fileDescriptor0 int1 func init protoRegisterTypeConfignil requesterConfig protoRegisterTypeResponsenil requesterResponse Reference imports to suppress errors if they are not otherwise used var contextContext var grpcClientConn This is a compiletime assertion to ensure that this generated file is compatible with the grpc package it is being compiled against const grpcSupportPackageIsVersion1 Client API for Requester service type RequesterClient interface Processctx contextContext in Config opts grpcCallOption Response error type requesterClient struct cc grpcClientConn func NewRequesterClientcc grpcClientConn RequesterClient return requesterClientcc func c requesterClient Processctx contextContext in Config opts grpcCallOption Response error out newResponse err grpcInvokectx requesterRequesterProcess in out ccc opts if err nil return nil err return out nil Server API for Requester service type RequesterServer interface ProcesscontextContext Config Response error func RegisterRequesterServers grpcServer srv RequesterServer sRegisterServiceRequesterserviceDesc srv func RequesterProcessHandlersrv interface ctx contextContext dec funcinterface error interface error in newConfig if err decin err nil return nil err out err srvRequesterServerProcessctx in if err nil return nil err return out nil var RequesterserviceDesc grpcServiceDesc ServiceName requesterRequester HandlerType RequesterServernil Methods grpcMethodDesc MethodName Process Handler RequesterProcessHandler Streams grpcStreamDesc var fileDescriptor0 byte 172 bytes of a gzipped FileDescriptorProto 0x1f 0x8b 0x08 0x00 0x00 0x09 0x6e 0x88 0x02 0xff 0xe2 0xe2 0x2f 0x4a 0x2d 0x2c 0x4d 0x2d 0x2e 0x49 0x2d 0xd2 0x2b 0x28 0xca 0x2f 0xc9 0x17 0xe2 0x84 0x0b 0x28 0xc9 0x70 0xb1 0x39 0xe7 0xe7 0xa5 0x65 0xa6 0x0b 0x09 0x71 0xb1 0xe4 0x25 0xe6 0xa6 0x4a 0x30 0x2a 0x30 0x6a 0x70 0x06 0x81 0xd9 0x4a 0x2a 0x5c 0x1c 0x41 0xa9 0xc5 0x05 0xf9 0x79 0xc5 0xa9 0x42 0x12 0x5c 0xec 0xb9 0xa9 0xc5 0xc5 0x89 0xe9 0x30 0x25 0x30 0xae 0x91 0x03 0x17 0x67 0x10 0xcc 0x40 0x21 0x63 0x2e 0xf6 0x80 0xa2 0xfc 0x64 0xa0 0x94 0x90 0xa0 0x1e 0xc2 0x62 0x88 0x25 0x52 0xc2 0x48 0x42 0x30 0x93 0x95 0x18 0x9c 0x8c 0xb9 0xa4 0x32 0xf3 0xf5 0xd2 0x8b 0x0a 0x92 0xf5 0x52 0x2b 0x12 0x73 0x0b 0x72 0x52 0x8b 0x11 0x0a 0x9d 0xf8 0xe0 0xa6 0x07 0x80 0x9c 0x1f 0xc0 0xb8 0x88 0x89 0x29 0x28 0x30 0x89 0x0d 0xec 0x19 0x63 0x40 0x00 0x00 0x00 0xff 0xff 0xac 0xd0 0xf7 0x73 0xdf 0x00 0x00 0x00 Ruby Example OK so weve defined what our service does its an RPC service that exposes a Process method that takes an argument But what that method returns weve yet to build thats not the responsibility of the definition file Weve used the protoc compiler to autogenerate some code stubs for us which handle the setting up of the service So lets see how we consume that from Ruby were going to need the following files Gemfile serverrb clientrb This is what the contents of those files look like Gemfile source httpsrubygemsorg gem grpc 011 We only have one dependency which is the grpc extension serverrb FilejoinFiledirnameFILE lib require grpc require requesterservices class RequesterServer RequesterRequesterService def processconfig unusedcall RequesterResponsenewmessage Hello configname end end s GRPCRpcServernew saddhttp2port000050051 thisportisinsecure shandleRequesterServer sruntillterminated So same set of dependencies pulled in like with the client But this time were creating a new instance of a class that inherits from our RequesterRequesterService autogenerated class This is similar in essence to the Template Method Pattern where were now able to define the implementation of the method type process But one thing to remember is that you need the 2nd argument unusedcall thats passed into the process method Remove it and things will break Why Ive actually no idea Ive found nothing in the documentation that explains this and Ive sifted through the source code and nothing I could grok to understand why this second seemingly pointless argument is there From here we create a new gRPC server instance GRPCRpcServer We then specify the address and port we want the server to listen to Dont be fooled in the specific nature of http2 in the method addhttp2port there is no addhttpport or alternative method Also as before the thisportisinsecure is required I dont really like the design of the code here but I guess what can you expect from lowlevel programmers designing code for dynamic languages they typically dont use Next we specify our RequesterServer to be the instance that handles any incoming requests Finally we tell the server to run until its terminated via a signal such as INT or TERM documented herehttpwwwrubydocinfogithubgrpcgrpc7131c62GRPCRpcServerruntillterminatedinstancemethod To run this program bundle install bundle exec ruby serverrb You wont see anything in the output so lets move onto the client code clientrb FilejoinFiledirnameFILE lib require grpc require requesterservices stub RequesterRequesterStubnewlocalhost50051 thischannelisinsecure msg stubprocessRequesterConfignewname Markmessage p Greeting msg Here were loading our grpc dependency and the service stub that was autogenerated for us Notice that because my protocol buffer definition file specified the package as requester and the file itself was called requester Ive now got this ugly namespace RequesterRequester Again just be aware of what youre naming things because to be honest that double named module is annoying for me to look at I left it like that to demonstrate why its important to name things well Youll notice that we pass in thischannelisinsecure to the Stubnew method This isnt an arbitrary value it needs to be exactly that value otherwise youll see errors Ive yet to look into using HTTPSTLS but if youre interested then you can find the relevant details on the authentication documentationhttpwwwgrpciodocsguidesauthhtml Once we create a new instance of our service we can now access the process method that is exposed by our RPC service Convention in Ruby is lowercase method names so although we defined it as Process its accessed as process We pass into process the expected Config type Ruby doesnt have types as part of the language so theyve provided us a modulenamespace instead to mimic this feature and finally we call the message property on the returned object remember we defined a Response in our protocol buffer definition file that had a message field To run this program bundle install bundle exec ruby clientrb Which should result in the output Greeting Hello Mark Go Example We can set up our services to use Go completely or we can mix and match But lets see how to use both the client and server from Go As with Ruby weve defined what our service does its an RPC service that exposes a Process method that takes an argument But what that method returns weve yet to build Weve used the protoc compiler to autogenerate some code stubs for us which handle the setting up of the service So lets see how we consume that from Go were going to need the following files servergo clientgo servergo package main import log net pb githubcomintegralisttestgrpccustompb golangorgxnetcontext googlegolangorggrpc const port 50051 type server struct func s server Processctx contextContext in pbConfig pbResponse error return pbResponseMessage Hello inName nil func main lis err netListentcp port if err nil logFatalffailed to listen v err s grpcNewServer pbRegisterRequesterServers server sServelis So the Go variation is fairly straight forward our main function listens on the specified port and we start up a new grpc server instance From there we take the protocol buffer pbrequesterpbgo that was generated by the protoc compiler and call a presupplied pbRegisterRequesterServer method and pass in a data structure for it to utilise along with the grpc server For the server struct type we associate the required Process method and define its behaviour In this case similar to the Ruby version we create an instance of the Response type To run this program execute go run servergo clientgo package main import log os pb githubcomintegralisttestgrpccustompb golangorgxnetcontext googlegolangorggrpc const address localhost50051 defaultName world func main conn err grpcDialaddress grpcWithInsecure if err nil logFatalfdid not connect v err defer connClose c pbNewRequesterClientconn name defaultName if lenosArgs 1 name osArgs1 r err cProcesscontextBackground pbConfigName name if err nil logFatalfcould not greet v err logPrintfGreeting s rMessage With the server program running we can now execute our client to call the servers Process method Again in summary we use gRPCs own Dial method to call the specified address The second argument disables the transport security for this particular connection If you want HTTPSTLS encryption then youll need to read the documentation for those details We create a new instance of the autogenerated client and call the Process method Passing along the autogenerated Config type with the data we want it to receive The pb identifier references the autogenerated protocol buffer package pb githubcomintegralisttestgrpccustompb and as youll probably already know this path is unique to your local setup and where you created that package To run this program execute go run clientgo Mark Which should result in the output Greeting Hello Mark Note if you leave off the argument Mark then the output will default to Hello world instead Conclusion So there you go Hopefully youve found this break down useful The principles of gRPC seem promising and although Im not keen on the design of the autogenerated code being not as idiomatic as youd expect for a language such as Ruby Im not sure what the other language implementations are like I still think this could be an interesting evolution of the microservices movement Update There are alternatives that work in a similar fashion one of which is Apache Thrifthttpsthriftapacheorg and is defined as being a software framework for scalable crosslanguage services development But unfortunately it doesnt support the Go programming language which is a requirement for me But interesting nonetheless "},{"title":"Git Merge Strategies","tags":["pr","github","git"],"href":"/posts/git-merge-strategies","content":" Introduction1 git merge2 git merge noff edit3 git reset31 Force the merge commit32 git branch contains33 Losing useful history34 git merge squash4 git rebase5 git rebase interactive6 git rebase onto7 git formatpatch8 Conclusion9 Introduction Imagine I have a master branch with one commit 75eb1cb originmaster README This is a single READMEmd file with the following content A 1 Now imagine I have a branch from master called featfoo and in that branch Ive made 3 additional commits 41d4115 Add C also revert A 9e5626c Modify A 8e7965e Add B The contents of the READMEmd file is now A 1 B 2 C 3 Just to quickly clarify youll notice throughout this post that I use the command git lg which is actually an alias I have set within my gitconfig that uses git log but modifies its behaviour with some additional git flags log graph prettyformatCredhCreset CyellowdCreset s CgreencrCreset abbrevcommit daterelative git merge So git merge is the standard workhorse for merging branches in git Itll try to resolve the differences between the two branches the best way it can If the source branch featfoo the branch you want to merge from can be merged cleanly eg there are no major diverges from the destination branch master which is the branch the changes are being merged into then git will be able to perform a simple fastforward What fastforward means is that git will change the HEAD on the destination branch to point to the new latest commit and all the other commits from your source branch will also appear in the git loghistory of the destination branch Note HEAD is an alias that points to a commit typically HEAD is the latest commit in your branch Even the branch name itself is an alias that refers to a commit most things in git do simply resolve to commits This is why when you have a long branch name instead of git push origin reallylongbranchname you can just use git push origin head and git will figure out which branch youre on If you check git lg after doing a git merge featfoo you should see something like 41d4115 HEAD master originfeatfoo featfoo Add C also revert A 9e5626c Modify A 8e7965e Add B 75eb1cb originmaster README We can see all the commits from featfoo were replayed onto master successfully Note you might not realise that there is a short cut to checking out a branch and then merging another branch into it git merge which is the same as doing git checkout followed by git merge git merge noff edit Lets say you wanted a merge commit to happen ie merge commits typically only occur if there has been a divergence between the branches which means git has to resolve the problem for you then you can force git to use a merge commit even when there is no need for one as is the case for me here Using our previous example which merged cleanly lets say that a merge commit is what we wanted to have happen Assuming youve not pushed the branch to a remote then you can safely go back to before the merge occurred using git reset hard 75eb1cb Note 75eb1cb being my first commit in master git reset Its important to understand how git reset works as it has three flags and if not used correctly could have bad side effects The flags are soft mixed hard The way reset works is that you use one of the above flags followed by the commit you want to reset the HEAD back to So in our case we used the commit 75eb1cb which was our very first commit If I had used the soft flag instead then it would have reset the HEAD back to the first commit but any other commits that happened since would have their changes staged together in our git index waiting to be committed If I had used the mixed flag instead then it would have reset the HEAD back to the first commit but any other commits that happened since would have their changes applied to the working directory ready for us to choose which changes to be added to the index ie staged and then finally committed When using hard though any of the changes that came after the commit being reset to are lost Theyre not sitting in your staging index nor are they available within your working directory either So be careful whenever using the hard flag Force the merge commit Now were back to where we were originally a separate featfoo branch and a master branch with a single commit we can look at how to force a merge commit To force a merge commit youll need to use the noff flag and then also use the edit flag to allow you to modify the default merge commit message otherwise git will provide its own commit message which is nearly always not useful or descriptive git merge edit noff featfoo Note edit doesnt work without noff unless there is a genuine merge conflict Now if I look at my git lg I can see 97f1257 HEAD master My custom merge commit message for featfoo 41d4115 originfeatfoo featfoo Add C also revert A 9e5626c Modify A 8e7965e Add B 75eb1cb originmaster README We can see all the commits from featfoo were replayed onto master successfully but now youre able to more easily distinguish the three commits came from another branch if using my git lg alias Which is one of the main reasons to force a merge commit using noff as it really helps keep a varied branch history Notice git log will also show in its output for the merge commit a field like Merge 75eb1cb 8e7965e 9e5626c 41d4115 Which helps at a glance to know more about what commits are inside the merge commit git branch contains The following command can be useful in locating where a commit has come from git branch contains 9e5626c In our case this will indicate that the commit we specified is part of our master branch Now when you use contains with a commit such as 9e5626c which was merged in from our feature branch youll see that git recognises this commit is part of multiple branches until you delete the branch eg git branch D featfoo Losing useful history Its also worth mentioning that even after the featfoo branch has been deleted git will still show via git log graph those commits from our featfoo branch as coming from an alternative pathbranch history This is a useful bit of information that can be lost when using other tools such as git rebase or git merge squash so you should discuss with your team what type of information you feel is useful to have when you look back at a projects git history before forging ahead with any one of the strategies I cover here For example some teams dont find being able to see that a set of commits actually came from another branch very useful considering all commitsfeatures should generally come in from separate branchesPull Requests So the use of rebase or squash isnt a concern for them For a team like this an aesthetically cleaner git commit history is preferred Also in teams where Ive worked and theyve utilised a squash strategy see below4 for more details weve used the following structure for our commit message so its clearer whats been squashed Closes 123 New Feature X Squashed commit of the following commit c7e4145f6e95e51fcff79d6b3476bcb19c058071 commit 3275f1805c4f82298676aa3c61db8c65ee9f3428 commit bb50fb69c2d131d0126fa9ae018377e6451678e2 commit 7ceb49c352d812a91db0e87a8ed4c4cf426c0365 commit 86d1de3c5133a403edf45343081353055c02b454 commit 8f48e5b3c43acf71e8abab4b821cfdc66447b732 commit ed857784feff091ece52d906e311ef7f64a49c3d commit a277e60c39333a55134c3e3ef6d97076f9bc8370 commit dd7e1973fe91f29887928aad9d991be24efb143a commit ff7e7dabf745ac4d73b52644c3d29ea05d5c318f commit 36f1c5bc5949f01117c1d57e6ab12f05c2a202f5 git merge squash So what if you dont want all those commits in your master You could instead squash all the commits down into a single commit using the squash command git merge squash featfoo Now what this does is take my changes from the source branch featfoo and automatically squashes those separate commits into a single change thats placed into the staging area of my destination branch These collection of changes now appear as a single change to the file They arent actually merged yet So you have the opportunity to change the commit message git commit m your own custom commit message git rebase The git rebase feature in essence is solving the same problem as git merge they both integrate a set of changes but they do them in fundamentally different ways With git merge a merge commit is utilised to resolve conflicts and so is considered nondestructive What this means is that the commits within either branch destination or source arent modified in any way With git rebase the source branch commits are placed before the destination branchs commits but also the commits themselves are from the source are recreated inside the destination branch Lets look and see what this does for us git rebase featfoo We can see that as there were no conflicts git was able to fastforward the commits So in theory this is no different right now from originally doing git merge featfoo But what if master had a new change committed to it and this change happened after we had branched off with featfoo For example Ill add a second commit to master that changes A 1 to A 9 If I run git rebase featfoo I should see we get a merge conflict and one that git doesnt know how to resolve First rewinding head to replay your work on top of it Applying A to 9 Using index info to reconstruct a base tree M READMEmd Falling back to patching base and 3way merge Automerging READMEmd CONFLICT content Merge conflict in READMEmd error Failed to merge in the changes Patch failed at 0001 A to 9 The copy of the patch that failed is found in gitrebaseapplypatch When you have resolved this problem run git rebase continue If you prefer to skip this patch run git rebase skip instead To check out the original branch and stop rebasing run git rebase abort We can see from the information git has given us that it first rewinded master back to the first commit 75eb1cb in order for it to place our featfoo commits on top of it as that initial commit is where our branch originally forked from From there we can see once git replayed our featfoo commits on top of 75eb1cb that it then tried to apply the new commit that featfoo didnt have eg Applying A to 9 and it failed to do so Git tells us that there was a merge conflict CONFLICT content Merge conflict in READMEmd Its up to us to open READMEmd and to resolve the conflict ourself When I open the file I see A to 9 So the above shows the file is split into three 1 2 merged common ancestors 3 I know that Im happy for the line A 1 which was changed in my featfoo branch commit 41d4115 to be changed to A 9 which was changed in master after I originally branched from it So I manually make that change by deleting all the added noise eg master A to 9 41d4115 originfeatfoo featfoo Add C also revert A 9e5626c Modify A 8e7965e Add B 75eb1cb originmaster README This shows that the changes from featfoo where replayed directly on top of 75eb1cb Otherwise if we didnt use gits rebase feature but a standard git merge we couldve ended up with a git history that looked like the following 41d4115 originfeatfoo featfoo Add C also revert A 9e5626c Modify A 8e7965e Add B 65553e0 HEAD master A to 9 75eb1cb originmaster README Notice the featfoo commits are on top of the A to 9 commit and that might not necessarily be what we want to have happen git rebase interactive The interactive flag is useful for letting us rewrite our git history Were able to move the order of our commits as well as squash commits down and change their recorded message So lets assume we want to squash all but the first commit in our featfoo branch By that I mean we currently have b4f9dfd HEAD featfoo Add C also revert A 7354a41 Modify A c321b40 Add B 75eb1cb originmaster README Lets say we want Add B Modify A and Add C also revert A squashed into one commit To do this we need to locate the parent commit of the earliest commit we want to squash So Add B is the earliest commit we want as part of the squash so the parent commit is README To action the rebase lets run the following command git rebase interactive 75eb1cb This drops us into an editor with the following output pick c321b40 Add B pick 7354a41 Modify A pick b4f9dfd Add C also revert A Rebase 75eb1cbb4f9dfd onto 75eb1cb 3 commands Commands p pick use commit r reword use commit but edit the commit message e edit use commit but stop for amending s squash use commit but meld into previous commit f fixup like squash but discard this commits log message x exec run command the rest of the line using shell d drop remove commit These lines can be reordered they are executed from top to bottom If you remove a line here THAT COMMIT WILL BE LOST However if you remove everything the rebase will be aborted Note that empty commits are commented out We can modify it like so pick c321b40 Add B squash 7354a41 Modify A squash b4f9dfd Add C also revert A This will result in the following combined commit details This is a combination of 3 commits The first commits message is Add B This is the 2nd commit message Modify A This is the 3rd commit message Add C also revert A Please enter the commit message for your changes Lines starting with will be ignored and an empty message aborts the commit Date Sun May 15 172932 2016 0100 interactive rebase in progress onto 75eb1cb Last commands done 3 commands done squash 7354a41 Modify A squash b4f9dfd Add C also revert A No commands remaining You are currently editing a commit while rebasing branch featfoo on 75eb1cb Changes to be committed modified READMEmd Now if we run git lg p well see the new squashed commit does indeed contain all the previous commits contents b63857d HEAD featfoo Add B 16 minutes ago diff git aREADMEmd bREADMEmd index 428f59ef2e26b6 100644 "},{"title":"Git Tips","tags":["bash","git","shell"],"href":"/posts/git-tips","content":" I thought I would get down in a blog post the different Githttpgitscmcom commands and tips that I find really useful because every now and then it seems I need to refer back to these notes which up until this point have been in a txt file in my Dropbox if Ive not used a particular command in a while Hopefully youll find them useful too 1 Show where Git is installed 2 Show the Git version installed 3 Update your global user details 4 Setup a global ignore file 5 Adding all files inc those marked as deleted 6 Writing a long commit 7 Viewing file changes while writing your commit 8 Viewing what files have been committed 9 Improving git log with git lg 10 Shorter git status 11 Finding a commit that includes a specific phrase 12 Only merging the files you want 13 Stashing changes youre not ready to commit 14 Revert all changes back to last commit 15 Unstaging files 16 Untrack a file without deleting it 17 Amend your last commit 18 Show the files within a commit 19 See any changes between current working directory and last commit 20 See changes between two commits 21 Creating a branch and moving to it at the same time 22 Deleting a branch 23 Viewing all branches of a remote 24 Checkout a remote branch 25 Remove a remote 26 Revert a specific file back to an earlier version 27 Viewing all commits for a file and who made those changes 28 Commiting only parts of a file rather than the whole file 29 Modifying your Git history with rebase 30 Push branch without specifying its name Show where Git is installed which git Show the Git version installed git version Update your global user details git config global username Your Name git config global useremail Your Email git config global applywhitespace nowarn ignore white space changes Setup a global ignore file First create the global ignore file touch gitignoreglobal Then add the following content to it this is a standard ignore file but Ive added some Sass CSS preprocessor files to it Compiled source com class dll exe o so sasscache scssc Packages its better to unpack these files and commit the raw source git has its own built in compression methods 7z dmg gz iso jar rar tar zip Logs and databases log sql sqlite OS generated files DSStore DSStore SpotlightV100 Trashes Icon ehthumbsdb Thumbsdb You can let Git know about your global ignore file by editing your global gitconfig file nano gitconfig then adding the following to it core excludesfile Usersgitignoreglobal or once the gitignoreglobal file is created you can just tell git by using this shorthand command git config global coreexcludesfile gitignoreglobal Adding all files inc those marked as deleted git add A Writing a long commit A short git commit message would look like this git commit m My short commit message but you should really be writing longer more descriptive commit messages which you do like so git commit what this does is open up the default editor for commit messages which for most is Vim Now Vim is a bizarre editor with all sorts of odd shortcuts for adding text Ive only used Vim to write commit messages nothing else so I have a very focused set of commands to write my commands Press i which puts Vim into insert mode meaning you can actually write This is my short description for this commit Here is a break down of my changes Another note about a particular change After Ive written my commit I just need to save the commit and exit Vim Press Esc Press wq the colon means you can execute more commands w write q quit Viewing file changes while writing your commit git commit v Viewing what files have been committed git lsfiles Improving git log with git lg To get a better looking git log we need to write an alias called git lg that is just made up of standard Git commandsflags but when put together along with specific colour settings means we can have a short git command that provides us lots of useful information What we need to do is open the gitconfig file and then add the following content alias lg log color graph prettyformatCredhCreset CyellowdCreset s Cgreencr Cbold blueCreset abbrevcommit daterelative Shorter git status As per the above tip we can create two extra alias which give us a shorter command to type I dont know about you but when typing really fast I seem to always misspell the word status and doesnt show us all the unnecessary crap that someone new to Git needs to see What we need to do is open the gitconfig file and then add the following content alias st status sts status sb you dont need to specify alias if its already in the file see previous tip Now typing git st will be the same as git status and typing git sts will be the same as git status sb Finding a commit that includes a specific phrase git log grep For example git log grepCSS will display all commits that contain the word CSS in the message Only merging the files you want git checkout Stashing changes youre not ready to commit If you make changes to your branch and then want to quickly change branches without first having to commit your current dirty state then run git stash To apply a stashed state git assumes the most recent stashed state if none specified use git stash apply To see which stashes youve stored on any branch use git stash list When viewing a list of stashes it can be useful if the stashes had corresponding messages so you know what each stash holds for that to happen youll need to create stashes with an associated message using the save command git stash save my message here If you have multiple stashes under a branch eg stash1 stash2 stash3 then you can reference a particular stash using git stash apply stash2 pre 20 this would work git stash apply2 If you want to stash only specific changes then use the patch mode git stash p To view the contents of a stash use git stash show p stashn where n is the numeric index of the stash you can also use git show stashn Applying the stash doesnt mean its removed from your list of stashes though so you need to run git stash drop stash eg git stash drop stash2 You can also apply and drop the stash at the same time git stash pop You can also specify an exact stash to pop git stash pop stash2 If you stash some work leave it there for a while and continue on the branch from which you stashed the work you may have a problem reapplying the work If the apply tries to modify a file that youve since modified youll get a merge conflict and will have to try to resolve it If you want an easier way to test the stashed changes again you can run git stash which creates a new branch for you checks out the commit you were on when you stashed your work reapplies your work there and then drops the stash if it applies successfully If you need to stash only specific files then first git add the files you dont want to stash then run git stash keepindex finally you can then git reset the files you originally added if you dont plan on committing them yet Revert all changes back to last commit git reset hard Note you can do a soft reset git reset soft The difference between hard and soft is with hard the specified commit hashs files are moved into the working directory and the staging area as if there were no changes since that specified commit But using soft will leave whatever changes youve made in your working directorystaging area but will restore the specified commit youve selected Unstaging files To unstage files weve added to the staging area we need to run the command reset HEAD but thats a bit ugly and awkward to remember What would be easier is if we could just say git unstage so lets create an alias to help make that easier Open up the file gitconfig and then add the following content alias unstage reset HEAD Note you dont need to specify alias if its already in the gitconfig file You can also unstage a single file using git reset If youve staged files before any commits have been set eg right at the start of your project then youll find the above wont work because technically there are no commits to revert back to So instead youll need to remove the files like so git rm cached Untrack a file without deleting it If you want to have Git stop tracking a file its already tracking then you would think to run git rm but the problem with that command is that Git will also delete the file altogether Something we usually dont want to have happen The work around to that issue is to use the cached flag git rm cached Amend your last commit If you make a commit and then realise that you want to amend the commit message then dont make any changes to the files and just run git commit amend which will open up the default editor for handling commits usually Vim and will let you amend the commit message If on the other hand you decide that after youve written a commit that you want to amend the commit by adding some more files to it then just add the files as normal and run the same command as above and when Vim opens to let you edit the commit message youll see the extra files you added as part of that commit Show the files within a commit git show nameonly See differences between files To see the difference between the current working directory and the last commit git diff If your files have been added to the staging area already then you can use the cached flag git diff cached To show specific changes use the worddiff flag git diff worddiff To see the diff between the working directory and a specific commit git diff the file name is optional To see the difference between branches git diff See changes between two commits git diff Creating a branch and moving to it at the same time git checkout b Deleting a branch git branch D Viewing all branches of a remote git branch a Checkout a remote branch What normally happens is this you clone down a repository from GitHub and this repo will have multiple branches but if you run git branch locally all you see is the master branch If you run git branch a you can see all the branches for that remote repository but you just cant access them or check them out So if you want to access the other branches within that repo then run the following command git checkout b origin this will create a new branch named whatever you called it and contains the content of the remote branch you specified Remove a remote git remove rm Revert a specific file back to an earlier version git checkout Note if youve staged your file and then started making changes to the file which you no longer want applied you can use git checkout to revert to the version of the file in the staging area Viewing all commits for a file and who made those changes git blame Commiting only parts of a file rather than the whole file If you have a file with lots of changes made you might not want to have all the changes logged under one single commit To split the single file into multiple commits you need to use Gits patch mode git add p Git will attempt to split a file into separate hunks Git terminology for a chunk of code You can then press to see what options you have available the most common being y yes n no d no to all remaining hunks s split current hunk into more hunks Sometimes you cant split a hunk into more hunks automatically you have to do it manually To do so you press e to edit and then use Vim to manually make changes So if you have a line removed that you want to keep as part of the commit then youll remove the so there is just a space instead and if you have a line added that you want to not have included as part of the commit then you remove the entire line BUT the most important part it also updating the line numbers at the top of the file so that the number of lines in the file match what you are looking to commit otherwise the commit will fail To make the edit to the hunk final precommit press esc then wq and then youll be able to commit the selected changes Modifying your Git history with rebase To change multiple commits you must use the interactive mode of the rebase command and you must tell Git how many commits back you want to go because itll start from there and keep moving through the commits until it reaches the HEAD REMEMBER when using rebase every commit in the range specified is changed whether you change the message or not So dont use rebase on commits that have already been pushed to a remote server as other users might have those commits pulled down and your changing of the commits will cause havoc for those users in the near future To amend the last 3 commits we use git rebase i HEAD3 and follow the instructions The principle is if you want to merge two commits then youll need to have a commit to merge into and then change pick to squash on the other commits that you want to have squashed into the previous commit You can also reorder commits and other things like change commits add files rename the message and remove commits completely Push branch without specifying its name If you have a long branch name then youll know how tedious it is to type out git push origin bugfixcachenodesexpiration Instead you can rely on the fact that git will retrieve the current branch name from its head tag git push origin head "},{"title":"Thinking about Interfaces in Go","tags":["dependencies","go","interfaces"],"href":"/posts/go-interfaces","content":" Interfaces in Gointerfacesingo Name Your Interface Argumentsnameyourinterfacearguments Keep Interfaces Smallkeepinterfacessmall Accept Interfaces Return Concrete Typesacceptinterfacesreturnconcretetypes Dont Return Concrete Typesdontreturnconcretetypes Use existing interfacesuseexistinginterfaces Dont Force Interfacesdontforceinterfaces Upgrading Interfacesupgradinginterfaces Standard Library Interfacesstandardlibraryinterfaces Tight Couplingtightcoupling Dependency Injectiondependencyinjection Refactoring Considerationsrefactoringconsiderations Testingtesting More flexible solutionsmoreflexiblesolutions Conclusionconclusion This post is going to explain the importance of interfaces and the concept of programming to abstractions using the Gohttpsgolangorg programming language by way of a simple example While treading what might seem like familiar ground to some readers this is a fundamental skill to understand because it enables you to design more flexible and maintable services Interfaces in Go An interface is a contract which describes behaviour not data and in Go it looks something like the following type Foo interface Bars string string error If an object in your code implements a Bar function with the exact same signature eg accepts a string and returns either a string or an error then that object is said to implement the Foo interface An example of this would be type thing struct func l thing Bars string string error Now you can define a function that will accept that object as long as it fulfils the Foo interface like so func doStuffWiththing Foo This is different to other languages where you have to explicitly assign an interface type to an object like with Java class testClass implements Foo Because of this flexibility in how interfaces are applied it also means that an object could end up implementing multiple interfaces For example imagine we have the following two interfaces type Foo interface Bars string string error type Beeper interface Beeps string string error We can define an object that fulfils both interfaces simply by implementing the functions they define type thing struct func l thing Bars string string error func l thing Beeps string string error Note this is a bit of silly example and so youll notice the method signature for each type is effectively the same Be careful when designing your interfaces because in this case we could possibly combine these two interfaces into a single more generic interface Name Your Interface Arguments Consider the following interface type Mover interface MovecontextContext string string error Do you know what the second and third arguments refer to and how the function will use them Now consider this refactored version where the arguments have names associated with them type Mover interface MovecontextContext source string destination string error Now that is better because we can clearly see what the expectations are the second argument is the source and the third argument is the destination Keep Interfaces Small Youll find in the Go Proverbshttpsgoproverbsgithubio the following useful tip The bigger the interface the weaker the abstraction The reason for this is due to how interfaces are designed in Go and the fact that an object can potentially support multiple interfaces By making an interface too big we reduce an objects ability to support it Consider the following example type FooBeeper interface Bars string string error Beeps string string error type thing struct func l thing Bars string string error func l thing Beeps string string error type differentThing struct func l differentThing Bars string string error type anotherThing struct func l anotherThing Beeps string string error In the above example weve defined a FooBeeper interface that requires two methods Bar and Beep Now if we look at the various objects weve defined thing differentThing and anotherThing well find thing fulfils the FooBeeper interface differentThing does not fulfil the FooBeeper interface anotherThing does not fulfil the FooBeeper interface Alternatively if we were to break the FooBeeper interface up into separate smaller interfaces like we demonstrated earlier then in our above example the differentThing and anotherThing would become more reusable Thats ultimately what this Go proverb is suggesting smaller interfaces allow for greater code reuse Accept Interfaces Return Concrete Types If your function accepts a concrete type then youve limited the consumers ability to provide different implementations Consider a function only accepting the concrete type osFile instead of the ioWriter interface Now try swapping out the osFile implementation in a test environment youll have a hard time vs mocking this using a struct that has the relevant interface methods Try to return concrete types instead of interfaces as interfaces have a tendendency to add an unnecessary layer of indirection for consumers of your package Interfaces ultimately do not protect your underlying API from change For example if your interface changes then the returned struct will likely need to change as well unless the interface is removing a method which your struct continues to implement but even then returning a concrete type will be better as it removes the indirection of having to go through an interface to access the underlying methods they want to use Dont Return Concrete Types This is to keep you on your toes If your function needs to return multiple types then returning an interface can be an appropriate solution The following code example highlights the principle type ItemInterface interface GetItemValue string type Item struct ID int type URLItem struct Item URL string type TextItem struct Item Text string func ui URLItem GetItemValue return uiURL func ti TextItem GetItemValue return tiText func FindItemID int ItemInterface The FindItem could be an internal library function that attempts to locate an item via multiple data sources Depending on which data source the item was found the type returned will change In this instance returning an interface allows the consumer to not have to worry about the change in underlying data types Note its possible the returned types could be consolidated into a single generic type struct which means we can avoid returning an interface but it depends on the exact scenariouse case Use Existing Interfaces Its important to not reinvent the wheel and to utilise existing interfaces wherever possible otherwise youll suffer from a condition known as interface pollution The golang toolchain offers a tool called Go Guruhttpsdocsgooglecomdocumentd1Y9xCEMj5S7rv2ooHpZNH15JgRT5iM742gJkw5LtmQeditheadingh7q1t7o2y7td3 which helps you to navigate Go code Its a command line tool but its designed to be utilised from within an editor like Atom or Vim etc Here is a list of the sub commands available callees show possible targets of selected function call callers show possible callers of selected function callstack show path from callgraph root to selected function definition show declaration of selected identifier describe describe selected syntax definition methods etc freevars show free variables of selection implements show implements relation for selected type or method peers show sendreceive corresponding to selected channel op pointsto show variables the selected pointer may point to referrers show all refs to entity denoted by selected identifier what show basic information about the selected syntax node whicherrs show possible values of the selected error variable This can be really useful for identifying for example whether a new interface youve defined is similar to an existing interface To demonstrate this consider the following example this is a duplicate of fmtStringer interface type stringit interface String string type testthing struct func t testthing String string return a test thing The stringit interface Ive defined is actually a duplication of the existing standard library interface fmtStringer So using Guru via my Vim editor I can see when I have my cursor over the testthing struct and I call Guru that this concrete type implements not only stringit but a few other interfaces maingo3363314 struct type testthing usrlocalCellargo1103libexecsrcfmtprintgo6266213 implements fmtStringer maingo2962913 implements stringit usrlocalCellargo1103libexecsrcruntimeerrorgo6666613 implements runtimestringer Now whether you continue to define a new interface is up to you There are actually quite a few places in the Go standard library where interfaces are duplicated for what I believe to be semantic reasoning but otherwise if you dont need to make an explicitsemantic distinction then Id opt to reuse an existing interface Note for more details on how to use Guru see this gisthttpsgistgithubcomIntegralist20ff7427d3df5cc02d5a619ca0cd9695 Dont Force Interfaces If your code doesnt require interfaces then dont use them No point making the design of your code more complicated for no reason Consider the following code which returns an interface Note the following example is modified from a much older post by William Kennedyhttpswwwardanlabscomblog201610avoidinterfacepollutionhtml package main import fmt Server defines a contract for tcp servers type Server interface Start type server struct NewServer returns an interface value of type Server func NewServer Server return server Start allows the server to begin to accept requests func s server Start fmtPrintlnstart called func main s NewServer fmtPrintfv Tn s s sStart The use of an interface here is a bit pointless We should instead just return a pointer to an exported version of the server struct because the user is gaining no benefits from an interface being returned by NewServer see Dont Return Concrete Typesdontreturnconcretetypes for a possible use case for returning interfaces but the above example is not one of them Upgrading Interfaces If you use have an interface thats used by lots of people how do you add a new method to it without breaking their code The moment you add a new method to the interface their existing code that handles the concrete implementation will fail Unfortunately there isnt a completely clean solution to this problem In essence the original interface needs to stay untouched and we need to define a new interface that contains the new behaviour Then the consumers of an interface will continue to reference the original interface while using a type assertion within their functions for the new interface Below is an example of this problem in action package main import fmt type foo interface bar string baz string new method added which breaks the code func doThingf foo fmtPrintlnbar fbar type point struct X Y int func p point bar string return fmtSprintfpd yd pX pY func main var pt point ptX 1 ptY 2 doThingpt In the above code we can see we have added a new method baz to our foo interface which means the concrete implementation pt is no longer satisfying the foo interface as it has no baz method Note I appreciate the example is a bit silly because we could just update the code to support the new interface but we have to imagine a world where your interface is provided as part of a public package that is consumed by lots of users To solve this problem we need an intermediate interface The following example demonstrates the process The steps are 1 define a new interface containing the new method 2 add the method to the concrete type implementation 3 document the new interface and ask your interface consumers to type assert for it package main import fmt type foo interface bar string type newfoo interface baz string We want a foo interface type but if that valid type can also do the new behaviour then well execute that behaviour func doThingf foo if nf ok fnewfoo ok fmtPrintlnbaz nfbaz fmtPrintlnbar fbar Original concrete implementation type point struct X Y int func p point bar string return fmtSprintfpd yd pX pY New concrete implementation of point struct has the new method type newpoint struct point func np newpoint baz string return fmtSprintfnp d ny d npX npY func main var pt point ptX 1 ptY 2 doThingpt var npt newpoint nptX 3 nptY 4 doThingnpt Note again the example is a bit silly in that were handling everything within a single file whereas in reality the consumer wont have access to the original interfaceimplementation code like we do here so just use your imagination The output of the above code is as follows bar p1 y2 baz np 3 ny 4 bar p3 y4 So we can see we called doThing and passed a concrete type that satisfied the foo interface and so that function called the bar method it was expecting to exist Next we called doThing again but passed a different concrete type that not only satisfied the foo interface but the newfoo interface and within doThing we type assert that the object passed in is not only a foo but a newfoo What would this look like in practice then Well if the Go standard library wanted to add a new method to the existing and very popular nethttp package ResponseWriter interface they would create a new interface with just the new behaviour defined then they would document its existence and in that documentation they would explain that if your HTTP handler required the new behaviour then you should type assert for it Imagine if the go standard library just updated the ResponseWriter with the new method Lots and lots of existing HTTP server code would break as the concrete implementation that was passed through would not support that implementation In fact this is exactly what the go standard library authors have done with the FlusherhttpsgolangorgpkgnethttpFlusher and HijackerhttpsgolangorgpkgnethttpHijacker interfaces The following code demonstrates the use of a type assertion to access the additional behaviour defined by those interfaces funcw httpResponseWriter r httpRequest ioWriteStringw This will arrive before if fl ok whttpFlusher ok flFlush timeSleep1 timeSecond ioWriteStringw this bit does Standard Library Interfaces Imagine we have a function process whose responsibility is to make a HTTP request and do something with the response data package main import fmt ioioutil nethttp func processn int string error url fmtSprintfhttphttpbinorglinksd0 n resp err httpGeturl if err nil fmtPrintfurl get error sn err return err defer respBodyClose body err ioutilReadAllrespBody if err nil fmtPrintfbody read error sn err return err return stringbody nil func main data err process5 if err nil fmtPrintfndata processing error sn err return fmtPrintfSuccess v data We can see our process function accepts an integer which is interpolated into the URL that is requested We then use the httpGet function from the nethttphttpsgolangorgpkgnethttp package to request the URL The function then stringifys the response body and returns it This is sufficient for a basic example but in the real world this function would likely do lots more processing to the response data It may not be immediately obvious but there are already many instances where interfaces are being utilised Lets break down the code and see what interfaces there are The httpGet function returns a pointer to a httpResponse struct and from within that struct we extract the Body field and pass it to ioutilReadAll The Body fields type is set to the ioReadCloserhttpsgolangorgsrcioiogos49775022L116 interface If we look at that interface well see its made up of nested interface types type ReadCloser interface Reader Closer If we now look at the ioReaderhttpsgolangorgsrcioiogos33033363L67 and ioCloserhttpsgolangorgsrcioiogos40434083L88 interfaces well find type Reader interface Readp byte n int err error type Closer interface Close error This means that for the response body object to be valid it must support the Read and Close functions defined by these interfaces the returned object will likely include other functions but it needs Read and Close at a minimum The next thing that happens in the code is that we pass httpResponseBody to an inputoutput function called ioutilReadAll If we look at the signature of ioutilReadAll well see that it accepts a type of ioReader which weve seen already and so this is another indication of why smaller interfaces enable reusability What the ioReader interface means for our code is that the input we provide to ioutilReadAll must support a Read function and because httpResponseBody implements the ioReadCloser interface we know it does implement that required function So already weve seen quite a few builtin interfaces being utilised to support the standard library code were using More importantly youll find the use of these interfaces ioReadCloser ioReader ioCloser and others are used everywhere in the Go codebase highlighting again how small interfaces enable greater code reusability Tight Coupling Now theres an issue with the above code specifically the process function and that is weve tightly coupled the nethttp package to the function What this means is that the process function has to intrinsically know about HTTP and dealing with the various methods available to that package Also if we want to test this function were going to have a harder time because the httpGet call would need to be mocked somehow We dont want our test suite to have to rely on a stable network connection or the fact that the endpoint being requested might be down for maintenance The solution to this problem is to invert the responsibility of the process function also known as dependency injection This is the basis of one of the SOLIDhttpsenwikipediaorgwikiSOLID principles inversion of control Dependency Injection If we call a function then it is our responsibility to provide it with all the things it needs in order to do its job In the case of our process function it needs to be able to acquire data from somewhere that could be a file it could be a remote procedure call it shouldnt matter The most important aspect to consider is how it acquires that data The how is not the responsibility of the process function especially if we decide later on that we want to change the implementation from HTTP to GRPC or some other data source Meaning we need to provide that functionality to the process function Lets see what this might look like in practice Note this is just a first iteration and is a poor design because although it shifts the problem slightly there will still be tight coupling Ill come back to this code later and refactor away the coupling completely The reason Ive not done that upfront is because there are learnings to be had from trying to write tests for this code which well see in a minute package main import fmt ioioutil nethttp type dataSource interface Geturl string httpResponse error type httpbin struct func l httpbin Geturl string httpResponse error resp err httpGeturl if err nil fmtPrintfurl get error sn err return httpResponse err return resp nil func processn int ds dataSource string error url fmtSprintfhttphttpbinorglinksd0 n resp err dsGeturl if err nil fmtPrintfdata source get error sn err return err defer respBodyClose body err ioutilReadAllrespBody if err nil fmtPrintfbody read error sn err return err return stringbody nil func main data err process5 httpbin if err nil fmtPrintfndata processing error sn err return fmtPrintfnSuccess vn data Refactoring Considerations Lets start by looking at the interface weve defined type dataSource interface Geturl string httpResponse error Weve not been overly explicit when naming this interface dataSource Its name is quite generic on purpose so as not to imply an underlying implementation bias Unfortunately the defined Get method is still too tightly coupled to a specific implementation ie it specifies httpResponse as a return type Meaning that although the refactored code is better it is far from perfect Next we define our own object for handling the implementation of the Get method which internally is going to use httpGet to acquire the data type httpbin struct func l httpbin Geturl string httpResponse error resp err httpGeturl if err nil fmtPrintfurl get error sn err return httpResponse err return resp nil By using this interface as the accepted type in the process function signature were going to be able to decouple the function from having to acquire the data and thus allow testing to become much easier as well see shortly but the process function is still fundamentally coupled to HTTP as the underlying transport mechanism The reason this is a problem is because the process function still knows that the returned object is a httpResponse because it has to reference the Body field of the response which isnt defined on the object weve injected meaning the function intrinsically knows of its existence How far you take your interface design is up to you You dont necessarily have to solve all possible concerns at once unless there really is a need to do so Meaning this refactor could be considered good enough for your use cases Alternatively your values and standards may differ and so you need to consider your options for how you might what to design this solution in such a way that it would allow the code to not be so reliant on HTTP as the transport mechanism Note well revisit this code later and consider another refactor that will help clean up this first pass of code decoupling But first lets look at how we might want to test this initial code refactor as testing this code allows us to learn some interesting things when it comes to needing to mock interfaces Testing Below is a simple test suite that demonstrates how were now able to construct our own object with a stubbed response and pass that to the process function package main import bytes ioioutil nethttp testing type fakeHTTPBin struct func l fakeHTTPBin Geturl string httpResponse error body Hello World resp httpResponse Body ioutilNopCloserbytesNewBufferStringbody ContentLength int64lenbody StatusCode httpStatusOK Request httpRequest return resp nil func TestBasicst testingT expect Hello World actual process5 fakeHTTPBin if actual expect tErrorfexpected s actual s expect actual Much like we do in the real implementation we define a struct in this case weve named it more explicitly fakeHTTPBin The difference now and what allows us to test our code is that were manually creating a httpResponse object with dummy data One part of this code that requires some extra explanation would be the value assigned to the response Body field ioutilNopCloserbytesNewBufferStringbody If we remember from earlier The Body fields type is set to the ioReadCloser interface This means when mocking the Body value we need to return something that has both a Read and Close method So weve used ioutilNopCloser which if we look at its signature we see returns an ioReadCloser interface func NopCloserr ioReader ioReadCloser The ioReadCloser interface is exactly what we need as that interface indicates the returned concrete type will indeed implement the required Read and Close methods But to use it we need to provide the NopCloser function something that supports the ioReader interface If we were to provide a simple string like Hello World then this wouldnt implement the required interface So we wrap the string in a call to bytesNewBufferString The reason we do this is because the returned type is something that supports the ioReader interface we need But that might not be immediately obvious when looking at the signature for bytesNewBufferString func NewBufferStrings string Buffer So yes it accepts a string but we want an ioReader as the return type whereas this function returns a pointer to a Bufferhttpsgolangorgsrcbytesbuffergos402817L7 type If we look at the implementation of Buffer though we will see that it does actually implementhttpsgolangorgsrcbytesbuffergos95649614L287 the required Read function necessary to support the ioReader interface Great Our test can now call the process function and process the mocked dependency and the codetest works as intended More flexible solutions OK so weve already explained why this implementation might not be the best we could do Lets now consider an alternative implementation package main import fmt ioioutil nethttp type dataSource interface Geturl string byte error type httpbin struct func l httpbin Geturl string byte error resp err httpGeturl if err nil fmtPrintfurl get error sn err return byte err defer respBodyClose body err ioutilReadAllrespBody if err nil fmtPrintfbody read error sn err return byte err return body nil func processn int ds dataSource string error url fmtSprintfhttphttpbinorglinksd0 n resp err dsGeturl if err nil fmtPrintfdata source get error sn err return err return stringresp nil func main data err process5 httpbin if err nil fmtPrintfndata processing error sn err return fmtPrintfnSuccess vn data All weve really done here is move more of the logic related to HTTP up into the httpbinGet implementation of the dataSource interface Weve also changed the response type from httpResponse error to byte error to account for these movements Now the process function has even less responsibility as far as acquiring data is concerned This also means our test suite benefits by having a much simpler implementation package main import testing type fakeHTTPBin struct func l fakeHTTPBin Geturl string byte error return byteHello World nil func TestBasicst testingT expect Hello World actual process5 fakeHTTPBin if actual expect tErrorfexpected s actual s expect actual Now our fakeHTTPBinGet only has to return a byte array Conclusion Is there more we can do to improve this codes design Sure But well leave a new refactor iteration to another post Hopefully this has given you a feeling for how interfaces are used in the Go standard library and how you might utilise custom interfaces yourself "},{"title":"Golang Reverse Proxy","tags":["go","golang","proxy"],"href":"/posts/golang-reverse-proxy","content":" Introduction1 Example Python Origin Code2 Example Golang Proxy Code3 Demonstration4 Explanation5 NGINXLite notreally6 Conclusion7 Introduction I was struggling to find a good or just simple reverse proxy solution written in Gohttpsgolangorg so I decided to take what I had learnt from a work colleague of mine and put together a simple example for others to build upon if they needed a quick reference point In this example I have an origin server written in Python for no other reason than to have a clearer distinction between the proxy and the origin and which supports the endpoints foo and bar where the wildcard glob means we support multiple variants of that such as barbaz Each origin handler will print the http request headers followed by sending a response body that correlates to the handler name so for example the FooHandler class will respond with FOO while the BarHandler class will response with BAR Example Python Origin Code Here is our Python code using the Tornadohttpwwwtornadoweborg web framework import tornadoioloop import tornadoweb class MainHandlertornadowebRequestHandler def getself printMAIN HEADERSnn selfrequestheaders selfwriteMAIN class FooHandlertornadowebRequestHandler def getself printFOO HEADERSnn selfrequestheaders selfwriteFOO class BarHandlertornadowebRequestHandler def getself printBAR HEADERSnn selfrequestheaders selfwriteBAR def makeapp return tornadowebApplication r MainHandler rfoo FooHandler rbar BarHandler if name main app makeapp applisten9000 tornadoioloopIOLoopcurrentstart Example Golang Proxy Code There are two versions of the code a simple version and a more advanced version that aims to handle more specific use cases The simple version uses just the Go standard library whereas the advanced version uses the standard library as well as a few a few external packages such as httprouterhttpsgithubcomjulienschmidthttprouter and logrushttpsgithubcomSirupsenlogrus for routing and logging respectively One difference between them thats worth mentioning is that in the simple version we use the httputilReverseProxy http handler directly whereas in the advanced version we use httputilNewSingleHostReverseProxy to construct this for us The advanced version also tries to normalise the paths by stripping trailing slashes and joining them up with the base path if there was one although ironically I dont define one in the advanced example Simple package main import log nethttp nethttphttputil neturl func main origin urlParsehttplocalhost9000 director funcreq httpRequest reqHeaderAddXForwardedHost reqHost reqHeaderAddXOriginHost originHost reqURLScheme http reqURLHost originHost proxy httputilReverseProxyDirector director httpHandleFunc funcw httpResponseWriter r httpRequest proxyServeHTTPw r logFatalhttpListenAndServe9001 nil Advanced package main import nethttp nethttphttputil neturl os strings githubcomSirupsenlogrus githubcomjulienschmidthttprouter func singleJoiningSlasha b string string aslash stringsHasSuffixa bslash stringsHasPrefixb switch case aslash bslash return a b1 case aslash bslash return a b return a b func main logrusSetFormatterlogrusTextFormatter logrusSetOutputosStdout logrusSetLevellogrusInfoLevel logger logrusWithFieldslogrusFields service goreverseproxy router httprouterNew origin urlParsehttplocalhost9000 path catchall reverseProxy httputilNewSingleHostReverseProxyorigin reverseProxyDirector funcreq httpRequest reqHeaderAddXForwardedHost reqHost reqHeaderAddXOriginHost originHost reqURLScheme originScheme reqURLHost originHost wildcardIndex stringsIndexAnypath proxyPath singleJoiningSlashoriginPath reqURLPathwildcardIndex if stringsHasSuffixproxyPath lenproxyPath 1 proxyPath proxyPathlenproxyPath1 reqURLPath proxyPath routerHandleGET path funcw httpResponseWriter r httpRequest p httprouterParams reverseProxyServeHTTPw r loggerFatalhttpListenAndServe9001 router Demonstration In order to run this example you should follow these instructions 1 run the tornado application eg python tornadooriginpy 2 run the go application eg go run maingo 3 make http requests shown below curl v httplocalhost9001 curl v httplocalhost9001foo curl v httplocalhost9001foo curl v httplocalhost9001barbaz You should see output from the Python server that looks something like this FOO HEADERS Host localhost9001 UserAgent curl7540 Accept XForwardedHost localhost9001 XOriginHost localhost9000 Explanation OK so lets step through the main function of the advanced example code to see whats going on The core reverse proxy code and its concepts are effectively the same between the advanced and simple versions First we set up our basic logging configuration logrusSetFormatterlogrusTextFormatter logrusSetOutputosStdout logrusSetLevellogrusInfoLevel logger logrusWithFieldslogrusFields service goreverseproxy Next we create a new httprouter instance we define the origin host httplocalhost9000 and the pattern we want httprouter to look out for catchall which is a special syntax that represents a catchall wildcardglob router httprouterNew origin urlParsehttplocalhost9000 path catchall Next we create a new reverse proxy instance passing it the origin host httplocalhost9000 reverseProxy httputilNewSingleHostReverseProxyorigin Followed by configuring the director for the reverse proxy The director is simply a function that modifies the received incoming request while the response from the origin is copied back to the original client In this example we attach a few common proxy related headers to the incoming request and then modify its SchemeHost to reflect the origin we wish to proxy it onto Next we change the request path to the origin What we do is ensure the path we request from the origin is whatever the base origin path is the requested path ie not just directing the request to the rootentrypoint of the origin In our example our origins path is just whereas the client will be requesting things like foo and barbaz so these would be appended to the origins defined But we also make sure that when joining the origins path with the incoming request path that we avoid double slashes in the middle Lastly we ensure that any trailing slash is removed as well reverseProxyDirector funcreq httpRequest reqHeaderAddXForwardedHost reqHost reqHeaderAddXOriginHost originHost reqURLScheme originScheme reqURLHost originHost wildcardIndex stringsIndexAnypath proxyPath singleJoiningSlashoriginPath reqURLPathwildcardIndex if stringsHasSuffixproxyPath lenproxyPath 1 proxyPath proxyPathlenproxyPath1 reqURLPath proxyPath Finally we setup the handler for the catchall httprouter path In this case we dont do anything other than call the reverse proxys ServeHTTP method and pass it the original ResponseWriter and http Request We then kick start the httprouter using ListenAndServe routerHandleGET path funcw httpResponseWriter r httpRequest p httprouterParams reverseProxyServeHTTPw r loggerFatalhttpListenAndServe9001 router NGINXLite notreally Below is an example that demonstrates using httpbinhttpshttpbinorg as our origin Specifically we use its anything endpoint which allows you to provide any value as the final path segment So for example anythingfoo or anythingbeep both work with the httpbinorg service package main import fmt log nethttp nethttphttputil func main proxy httputilReverseProxyDirector funcreq httpRequest originHost httpbinorg originPathPrefix anything reqHeaderAddXForwardedHost reqHost reqHeaderAddXOriginHost originHost reqHost originHost reqURLScheme https reqURLHost originHost reqURLPath originPathPrefix reqURLPath fmtPrintffinal requestnn v nn req httpHandleFunc funcw httpResponseWriter r httpRequest proxyServeHTTPw r logFatalhttpListenAndServe9001 nil Now lets elaborate on this example a little bit and ensure that our reverse proxy has a client timeout specified see this articlehttpsmediumcomnate510dontusegosdefaulthttpclient4804cb19f779 for details as to why you would want to do this We also use gorillamux as it supports utilising regular expression path matching we could do this ourselves but using a library in this case helps to keep the code we have to write down One last thing youll notice is that were using a configuration object that allows us to configure override behaviour For example if our request includes a HTTP header of XBFTesting and its value is integralist then well proxy the request to a different endpoint You can do more complex things if necessary but this gives you a good idea of how to replicate something like NGINX with very little code obviously to replicate something like NGINX is waaay beyond the scope of this post package main import log net nethttp nethttphttputil time githubcomgorillamux type override struct Header string Match string Host string Path string type config struct Path string Host string Override override func generateProxyconf config httpHandler proxy httputilReverseProxyDirector funcreq httpRequest originHost confHost reqHeaderAddXForwardedHost reqHost reqHeaderAddXOriginHost originHost reqHost originHost reqURLHost originHost reqURLScheme https if confOverrideHeader confOverrideMatch if reqHeaderGetconfOverrideHeader confOverrideMatch reqURLPath confOverridePath Transport httpTransport Dial netDialer Timeout 5 timeSecond Dial return proxy func main r muxNewRouter configuration config config Path pathanythingfoobar Host httpbinorg config Path anythingfoobar Host httpbinorg Override override Header XBFTesting Match integralist Path anythingnewthing for conf range configuration proxy generateProxyconf rHandleFuncconfPath funcw httpResponseWriter r httpRequest proxyServeHTTPw r logFatalhttpListenAndServe9001 r Conclusion Thats all there is to it You could also wrap the function passed to routerHandle in a middleware function so that youre able to do extra processing A common example of this is to authenticate the incoming request before it is proxied to the origin meaning you can reject the request if necessary "},{"title":"Hashing, Encryption and Encoding","tags":["bash","encoding","encryption","gpg","hash","keybase","keys","openssh","openssl"],"href":"/posts/hashing-and-encryption","content":" Introduction Ive written previouslypostssecuritybasics and indepth on the subject of security basics using tools such as GPG OpenSSH OpenSSL and Keybase But this time I wanted to focus in on the differences between encryption and hashing whilst also providing a slightly more concise reference point for those already familiar with these concepts Before we get started lets see what well be covering Terminology1 Hashing vs Encryption2 MAC vs HMAC21 Base64 Encoding3 Random Password Generation4 Hash Functions5 shasum hashlib cksum OpenSSH6 OpenSSL7 Generating a key pair Encrypting and Decrypting Randomness GPG8 Generating a key pair Automate Asymmetrical Encryption and Decryption Symmetrical Encryption and Decryption Signing keys Signing encrypted files Keybase9 Terminology OK so using the correct terminology is essential and helps us to be explicit and clear with what we really mean hash function calculates a deterministic irreversible fixedsize alphanumeric string based on input message a message is the data eg the input provided to a hash function digest the hexidecimal output generated by a hash function contextually referred to as checksum or fingerprint symmetric algorithm a cryptographic algorithm that uses the same key to encrypt and decrypt data asymmetric algorithm a form of encryption where keys come in pairs what one key encrypts only the other can decrypt integrity the message transported has not been tampered with or altered confidentiality the communication between trusted parties is confidential authenticity the communication is with who you expect it to be not a maninthemiddle For a longer Security Glossary please see this Google dochttpsdocsgooglecomdocumentd1qs3jEIQvocdVhSxCSPLF1BoLnp91aLnuUIasvlmaYoedituspsharing I created Hashing vs Encryption In essence hashing provides integrity encryption provides confidentiality Often cryptographic primitives need to be combined For example publickey cryptographypostssecuritybasicspublickeycryptography uses RSA a slow but very secure algorithm for communicating securely while internally using AES a faster but less secure algorithm for encrypting data with a shared key while using a hash function for generating a message digest to ensure both parties can verify the integrity of the payload sentreceived less secure in the sense that you have to share a secret key with the person you wish to communicate with but thats what publickey cryptography helps to secure Why use a hash function Hash functions or more specifically their output digests can be used for many things like indexing data in a hash table fingerprinting ie detecting duplicate data or uniquely identifying files or as a checksum ie detecting data corruption Message authentication ie message integrity involves hashing the message to produce a digest and encrypting the digest with the private key to produce a digital signature In order to verify this signature the recipient of the encrypted message would need to compute a hash of the message then decrypting the signers public key and comparing the computed digest against the decrypted digest sent within the encrypted message If the digest you generated is the same as the decrypted digest then we can be sure the message was delivered unmodified whilst in transit eg maninthemiddle Base64 Encoding Base64 is a way of taking binary data and transforming it into a textbased format It is commonly used when there is a need to transfer the binary data over a medium that only supports textual data eg you can Base64 encode images so they can be inlined into HTML How it works Base64 encoding takes three bytes each consisting of eight bits and represents them as four printable characters in the ASCII standard Note Base64 encoded strings are NOT secure Remember it encodes data not encrypt it MAC vs HMAC A MAC Message Authentication Code uses symmetrical cryptography with an encryption algorithm such as AES to verify the integrity of a message whereas a HMAC will use a hash function such as SHA256 internally instead of an encryption algorithm encryption algorithms AES Advanced Encryption Standard Blowfish DES Data Encryption Standard Triple DES Serpent and Twofish Below is an example HMAC written in Bashhttpswwwgnuorgsoftwarebash and using the OpenSSLhttpswwwopensslorg commandline tool function hmac digest1 data2 key3 shift 3 echo n data openssl dgst digest hmac key The way you would use it is as follows hmac sha256 message to be hashed secretkey Note you can swap sha256 for any supported digest algorithm see openssl dgst h for details Which would generate the digest output 44db14fe496c4bc4af5e8e6e3683e5db7acffa555897cf4b2b4345abaaf1ace3 Now because the implementation is using the openssl command you can also choose to convert the hexidecimal output into binary and then Base64 encode that binary output like so hmac sha256 message to be hashed secretkey binary base64 Which outputs RNsUklsS8SvXo5uNoPl23rPlVYl89LK0NFq6rxrOM You dont have to use an abstraction around the command obviously you can just use cat plaintexttxt openssl dgst sha512 binary base64 Note base64 could be replaced with openssls base64 encoding command openssl enc base64 A Random Password Generation Generating random passwords that are complex enough to make automated attacks difficult can be a bit tedious yet important But if you install a program such as pwgen brew install pwgen youll be able to generate random and complex passwords very easily Once installed add the following alias to your shell alias pswpwgen sy 20 1 Now when you execute psw youll get output that looks something like the following 93 Hash Functions There are many different ways of accessing a hash function two options well look at will be using the executable shasum provided by macOS and the hashlib package provided by the Pythonhttpswwwpythonorg programming language shasum Lets generate a hexidecimal digest of the message foobar using the SHA512 hash algorithm echo n foobar shasum a 512 Note see shasum h for all available algorithms Which outputs 0a50261ebd1a390fed2bf326f2673c145582a6342d523204973d0219337f81616a8069b012587cf5635f6925f1b56c360230c19b273500ee013e030601bf2425 hashlib Lets again generate a hexidecimal digest of the message foobar using the SHA512 hash algorithm now using Python import hashlib message hashlibsha512 messageupdatebfoobar printmessagehexdigest Which outputs the same digest as the shasum command produced 0a50261ebd1a390fed2bf326f2673c145582a6342d523204973d0219337f81616a8069b012587cf5635f6925f1b56c360230c19b273500ee013e030601bf2425 cksum Remember hash functions generate a digest of some message input and one such use of that digest output is data corruption ie a checksum The macOS also provides a cksum command which lets you generate a checksum like so echo foobar cksum Which outputs 857691210 7 The first number is the checksum and the second number is the amount of data in bytes OpenSSH OpenSSH provides secure and encrypted tunneling capabilities and is typically used to enable secure shell connections from your machine to external servers In order to generate a cryptographically secure key pair execute the following command sshkeygen t rsa b 4096 C youremaildomaincom This uses the RSA algorithm which is the default so the t can be omitted along with a key size of 4096 bits the default is 2048 The output of this command will be a public and private key pair Its usually best to generate these keys or at least move them when generated within the ssh directory SSH Agent One thing that catches me out all the time is when I open a new terminal tab or shell instance and I go to push up some code changes to a remote server only to discover an error saying Im not authenticated This is because the new terminalshell instance doesnt have the SSH agent running which is what makes my SSH key pair available This happens so often Ive created an alias to make starting up the SSH agent and loading my SSH private key very quick and easy alias sshagenteval sshagent s sshadd K sshgithubrsa Note the use of the K flag is macOS specific it means itll add the key into the macOS keychain program OpenSSL OpenSSL is designed to provide a method for securing web based communication think HTTPSTLSSSL Note for a full list of commands see openssl h and openssl h Key Exchanges There are two popular key exchange algorithms 1 RSA 2 DiffieHellman For the specific details of each I recommend you read this post on the differenceshttpstechnetmicrosoftcomenuslibrarycc962035aspx In short RSA uses the persons public key to encrypt the secret while DiffieHellman uses a mathematical function to ensure only those two people communicating can calculate the secret based on the information thats publicly available Generating a key pair In order to generate a RSA based publicprivate key pair execute the following commands generate a private key openssl genrsa out privatekeypem 4096 generate a public key from the private key openssl rsa pubout in privatekeypem out publickeypem Encrypting and Decrypting The following examples use symmetric encryption and so youll be asked for a secret key when encrypting and decrypting although you could also use the pass flag like so pass pass yeah the syntax is odd and its the same for decrypting symmetric encryption youll be asked for a key echo foobar openssl enc aes256cbc out messageenc decrypt that encrypted message openssl enc aes256cbc in messageenc d Note enc is a commonly used format to indicate a file is encrypted asc is specifically used for asymmetric encryption Im passing in the message via stdin when encrypting but specifying a file for the output when decrypting but you could use a file for both by explicitly specifying the in and out flags to provide a text file instead Annoyingly with openssl the same thing can be done a million different ways so for example you might also find that you can do the above without the enc portion of the command and thus removing the prefix from the selected algorithm symmetric encryption echo foobar openssl aes256cbc out messageenc decrypt that encrypted message openssl aes256cbc in messageenc d Encoding You can also generate Base64 output of the encrypted data by using the a flag like so echo foobar openssl aes256cbc a U2FsdGVkX19L0WtkvCNlpMiQnvD1SWGM19lm4m6xK4 Note see man enc for details Salts Its also worth mentioning that the default behaviour for OpenSSL is to use a salt when using encrypting the message A salt is random data appended to your already hashed message and then that is hashed itself In pseudocode it would look like this pwd hashhashpassword salt You would then store the value of pwd in your database along with the salt itself The security doesnt come from obfuscating the salt but more that a rainbow table attack cant now automatically loopcheck its collection of hashed passwords An attacker would need to incorporate your peruser unique salt value into their check against a predetermined list of hashes and they also wouldnt know if the salt was prefixed or suffixed to the password itself Making it computationally very expensive and time consuming to attempt You can also see that a salt is used by trying to read an encrypted file cat messageenc SaltedMJinMrandomrandomW5 Asymmetrical Encryption If you need to you can use a public key to encrypt data with ie asymmetrical encryption by utilising the openssl rsautl command which stands for RSA Utility and is commonly used to sign verify encrypt and decrypt data using the RSA algorithm In the following example we have a file plaintexttxt we encrypt using a public key It will now only be possible to decrypt the secretenc file if you have the corresponding private key encrypting openssl rsautl encrypt pubin inkey publickeypem in plaintexttxt out secretenc decrypting openssl rsautl decrypt inkey privatekeypem in secretenc Randomness OpenSSL also offers a way to generate random binary data which you can then export as either hexidecimal or base64 formats Note in the following examples 64 is the number of bytes to be generated openssl rand 64 RRwKq5VrdMj8Ty7fileIntegralistMBPrtmp openssl rand 64 hex 660baf33c189ced722a07c6a29d35a7e4584bb954c8c86f2cfd4ea8d892bff32fc188b0c56cbe0a56d60b628cdee697308b0cf3806cd95052b743bec5ccc5240 openssl rand 64 base64 JIPU5SiCgKP3XVrnef1gYPxjBvjdQgSNOJoBAdWmCacRvDdFl01GQiSwFimQ5 1lVa7hfYIK6Z5jjHNauaQ GPG GPG is a tool which provides encryption and signing capabilities and supports both symmetrical and asymmetrical encryption digital signing of your encrypted content to ensure the integrity Generating a key pair To generate a new GPG key pair you would execute the following command and interactively fill in the details gpg genkey Automate If you prefer to automate this you can create a file to contain the details and pass that into the commandline instead The following code generates a new batchfile that will contain the information we would otherwise have to enter manually cat batchfile EOF echo Generating a basic OpenPGP key KeyType RSA KeyLength 4096 SubkeyType Default NameReal Your Name NameComment Integralist testing NameEmail fooexamplecom ExpireDate 0 Passphrase foobar commit echo done EOF Once we have this file we can pass it along with the genkey command gpg genkey batch batchfile gpg Generating a basic OpenPGP key gpg key 4BCAEAAD199B5FE8 marked as ultimately trusted gpg directory UsersIntegralistgnupgopenpgprevocsd created gpg revocation certificate stored as UsersIntegralistgnupgopenpgprevocsdCFE96536285D83C990567BF64BCAEAAD199B5FE8rev gpg done Now if we check our list of keys well see the new one we just generated gpg listkeys UsersIntegralistgnupgpubringgpg "},{"title":"Hitchikers Guide to Go","tags":["go"],"href":"/posts/hitchikers-guide-to-go","content":" Introduction1 Private Repo Access2 Build and Compilation3 Build Time Dynamic Variables31 Dependency Information4 Dependency Management5 Documentation6 Testing7 Logging8 Godo9 Import Race Conditions10 New vs Make11 Custom Types12 Custom Errors125 Function Types13 Enumerator IOTA14 Struct Var vs Type15 Embedded Structs16 Reference vs Value17 See all methods on 18 Convert Struct into JSON19 Pretty Printing JSON String20 Convert Struct into YAML21 Sorting Structs22 Read Users Input23 HTTP Middleware24 Sessions25 HTTPS TLS Request26 HTTP GET Web Page27 Custom HTTP Request Methods28 Pointers29 Type Assertion30 Line Counting31 Reading File in Chunks32 Time33 Starting and Stopping things with Channels34 Channel Pipelines35 Templating36 Error handling37 Socket Programming38 Comparing Maps39 Zip File Contents40 Shell Commands41 New Instance Idiom42 JSON Connection Draining43 Writing your own MarshalUnmarshal functions44 Introduction A few years ago when I was learning the Go programming languagehttpsgolangorg I created a gist and updated it on a regular basis as a sort of cheat sheet I stumbled across this gist recently and decided Id try and port it over to some form of semicoherent blog post Note the code will not be updated in any way so YMMVhttpdictionarycambridgeorgdictionaryenglishymmv What this isnt is a walkthrough of how to write Go code Id suggest trying the official Go Tourhttpstourgolangorgwelcome1 which is really good and covers a lot of ground Instead Im going to provide lots of example code in the vein of a resource like Go by Examplehttpsgobyexamplecom What this is is a barren wasteland of old code Passers by are forewarned to tread carefully Note theres also not much in the way of code explanation to the extent that some of the testing examples are long and very context specific youve been warned Most of this Go code is old so you may find some packages or information possibly out of date as in not the latest awesome thing or maybe not that great quality either Take this as what it is a sharing exercise Take what you need and leave the rest I wont be offended Private Repo Access go get uses HTTPS so to be able to pull dependencies from a private repository youll need to force it to use SSH so it can access your keys and authorise the connection git config global urlgitgithubcominsteadOf httpsgithubcom You can also restrict this to a single specific organisation if you prefer git config global urlgitgithubcomfooinsteadOf httpsgithubcomfoo So when you want a private dependency like gitgithubcomfooprivategit go get githubcomfooprivate Build and Compilation As of Go 15 you can use GOOSdarwin GOARCH386 go build foogo Heres a quick reference of the values you can specify GOOS GOARCH darwin 386 32 bit MacOSX darwin amd64 64 bit MacOSX freebsd 386 freebsd amd64 linux 386 32 bit Linux linux amd64 64 bit Linux linux arm RISC Linux netbsd 386 netbsd amd64 openbsd 386 openbsd amd64 plan9 386 windows 386 32 bit Windows windows amd64 64 bit Windows You can get a full list with go tool dist list Gox Gox is an alternative build tool One time only commands for purpose of downloadsetup go get githubcommitchellhgox gox buildtoolchain only necessary for Go 14x and lower Compilation example gox osarchlinuxamd64 osarchdarwinamd64 osarchwindowsamd64 outputfoobarOS This will generate three files 1 foobardarwin 2 foobarlinux 3 foobarwindowsexe Other information Use the a flag when running go build In short if you dont use go build a v then Go wont know if any packages are missing you can find the gory details herehttpsmediumcomfelixgewhyyoushouldusegobuildaorgbc469157d5c1bjf5orcwrj Build Time Dynamic Variables Imagine you have a global variable called version in your main package and you want to update that value at build time go build ldflags X mainversionfoobar A more realistic example would be to use some form of revision number of commit hash go build ldflags X mainversiongit revparse HEAD Another approach would be to have separate files for different environments all under the main package You would then use a code comment to indicate what the environment was and at build time you would tell the compiler which version of the file to compile build prod package main func init version 123 You would compile the above version variable using go build tags prod Package Naming In Go the name of the package is used to refer to the exported item fmtPrintln httpRegisterFunc etc Because the package name is so visible the package name should describe what the exported items are Meaning we shouldnt have packages named util as a common example of bad package naming because utilJSONMarshal isnt as efficient and effective as jsonMarshal Another example of this that I found in my own code was utilsCreateUser Later on during the project I had added utilsCreateLegacyUser When I discovered what I had done I went back and made two separate packages legacy and aws so that I could have a consistent CreateUser function within both eg awsCreateUser and legacyCreateUser Dependency Information To see a list of dependencies for a given Go package you can utilise the go list command go list json strconv Which returns Dir usrlocalCellargo152libexecsrcstrconv ImportPath strconv Name strconv Doc Package strconv implements conversions to and from string representations of basic data types Target usrlocalCellargo152libexecpkgdarwinamd64strconva Goroot true Standard true Root usrlocalCellargo152libexec GoFiles atobgo atofgo atoigo decimalgo docgo extfloatgo ftoago isprintgo itoago quotego IgnoredGoFiles makeisprintgo Imports errors math unicodeutf8 Deps errors math runtime unicodeutf8 unsafe TestGoFiles internaltestgo XTestGoFiles atobtestgo atoftestgo atoitestgo decimaltestgo exampletestgo fptestgo ftoatestgo itoatestgo quotetestgo strconvtestgo XTestImports bufio bytes errors fmt log math mathrand os reflect runtime strconv strings testing time unicode If you dont specify the json flag then the default behaviour is to filter out the ImportPath field from the above JSON output For example go list strconv Will return just the import path strconv Documentation go help list less You can also utilise Gos templating functionality on the returned JSON object by adding the f flag go list f join Deps strconv Which filters out the Deps field joins up all items it contains using whitespace and subsequently returns errors math runtime unicodeutf8 unsafe You can do more complex things such as go list f ImportPath join Imports compress Which will return something like compressbzip2 bufio io sort compressflate bufio fmt io math sort strconv compressgzip bufio compressflate errors fmt hash hashcrc32 io time compresslzw bufio errors fmt io compresszlib bufio compressflate errors fmt hash hashadler32 io Dependency Management Update August 2017 there is an official tool now called dephttpsgithubcomgolangdep There are many dependency management tools these are the few that Ive tried in this order godeps gb glide Lets review each of them to see how they work Godeps When running go get locally Go will stick the dependency in the folder defined by your GOPATH variable So when you build your code into a binary using go build itll bake the dependencies into the binary ie the binary is statically linked But if someone pulls down your repo and tries to do a build from your code then theyll need to have a network connection to pull down the dependencies as their GOPATH might not have those dependencies yet unless the user manually executes go get for each dependency required Also the dependencies they subsequently pull down could be a more recent and untested version of each dependency So to make this situation better we can use Godephttpsgithubcomtoolsgodep to stick all your dependencies within a Godeps folder inside your project directory You can then use godep save r to automatically update all your references to point to that local folder Note you might need to remove the Godeps folder and run go get if you get strange conflicts The means to target all go files This way users who clone your repo dont need an internet connection to pull the dependencies as they already have them But also theyll have the correct versions of the dependencies This acts like a Gemfilelock as you would typically find in the Ruby world Gb go get u githubcomconstabularygb gb vendor fetch gb build all Youll need the following structure src foo maingo vendor manifest src The vendor directory is autogenerated by the gb vendor fetch command Glide This is now my preferred dependency management tool as it works just like existing tools in other languages eg Rubys Bundler or Nodes NPM and so consistency is a plus It also provides the ability like gb to not commit dependencies but have specific versions vendored when running a simple command go get githubcomMastermindsglide export GO15VENDOREXPERIMENT1 or use 16 glide init generates glideyaml glide install installs from lock file creates it if not found glide update updates dependencies and updates lock file glide list shows vendored deps go test glide novendor test only your package not vendored packages Note to add a new dependency glide get Documentation Godoc is the original implementation for viewing documentation Previous to Godoc there was go doc but that was removed and then added back with totally different functionality The syntax structure for go doc is as follows go doc go doc go doc Here are some examples of using go doc go doc json same as go doc encodingjson go doc jsonNumber go doc jsonNumberFloat64 Here is the same thing but using godoc where the syntax structure is godoc godoc encodingjson unlike go doc json godoc json doesnt work as its not a fully qualified path godoc encodingjson Number godoc src builtin make less Unlike with go doc godoc doesnt allow filtering by It only goes as far as You can use and the method will be included in the results but youll need to search for the method manually godoc src nethttp Request ParseForm less here is a similar result using go doc go doc httpRequestParseForm less The purpose of go doc was to provide a simplistic cli documentation viewer whereas Godoc has many more features available The go doc command also works not only with Gos own librarys but your own custom packages as well There are some differences in what is returned though between godoc and go doc mainly the latter is more succinctcompact so you can find the functionstypes youre after and then you can expand into those once youve found them godoc is harder to sift through on the command line godoc encodingjson Encoder type Encoder struct contains filtered or unexported fields An Encoder writes JSON objects to an output stream func NewEncoderw ioWriter Encoder NewEncoder returns a new encoder that writes to w func enc Encoder Encodev interface error Encode writes the JSON encoding of v to the stream followed by a newline character See the documentation for Marshal for details about the conversion of Go values to JSON go doc encodingjson Encoder type Encoder struct Has unexported fields An Encoder writes JSON objects to an output stream func NewEncoderw ioWriter Encoder func enc Encoder Encodev interface error Notice the functions dont have their documentation notes printed with go doc One other thing godoc has over go doc is the ability to view the source code using the src flag godoc src builtin make less The godoc tool also has a full browser documentation suite available and allows you to generate HTML documentation for your project Full Browser Documentation Start a local documentation server and allow indexing which takes a few minutes you have to just keep trying the search until its done godoc http 6060 index You can then open a new terminal pane and search via cli if you prefer rather than open up a browser to httplocalhost6060 godoc q tls less You can also have the playground available if you need it in the browser but it does require an internet connection to compile godoc http 6060 play Testing Note see also examples herehttpsgistgithubcomIntegralistcf76668bc46d75058ab5f566d96ce74a Test files are placed in the same directory as the filepackage being tested The convention is to use the same file name but suffix it with test So foogo would have another file next to it called footestgo Run the tests go test v You can also run a specific test like so go test v commandfootestgo Note remember that your test file should have the same package name as your code being tested This means the test file will have access to all the public functions and variables of that package and so subsequently itll have access to the code being tested Here is a simple test example package foo import testing func TestBasicst testingT expect abc actual def if actual expect tErrorfexpected s actual s expect actual The output from running this test will be RUN TestBasics "},{"title":"Host Methods vs Native Methods","tags":["javascript"],"href":"/posts/host-methods-vs-native-methods","content":" Introduction1 What they are2 How to detect them3 When is it OK to modify them4 Introduction This was intended as a short and overly simplified post about Host methods and Native methods What they are Native methods are builtin functions provided by the ECMAScript core specification So things like Object methods eg Objectcreate Array methods eg ArrayforEach etc Host methods are functions provided by the host environment most of the time when working in web development the host environment will be the users web browser So things like the DOM API and the Events object are host objectsmethods eg attachEvent is a host method and addEventListener is a host method How to detect them Detecting Native methods is relatively straight forward The real problem comes when you need to determine whether the objectmethod youre detecting actually works the way the specification dictates it should work So just checking it is available to use isnt good enough Detecting host methods is similar but a lot more problematic because the ECMAScript specification states that the host environment can implement certain methods however they like and so there is no guarantee that your checks for certain host methods which may work today will work in future Well give an example of each so you can get an idea of what I mean To detect a Native method such as ArrayforEach you should be able to do the following if ArrayprototypeforEach polyfill for missing forEach method Note polyfill is a term that Remy Sharp coined which means a shim that mimics a future API see httpremysharpcom20101008whatisapolyfillhttpremysharpcom20101008whatisapolyfill But the issue you could encounter in this example is if youre inheriting a project from another developer and they have already extended the Native Array object with a forEach method and their polyfill version of the missing forEach function doesnt work how the specification has dictated it should then you could find your code errors at hard to debug stages because of the difference in implementation where youre passing parameters into a polyfilled method and that method hasnt been implemented properly so the extra parameters either throw an error or potentially worse silently fail This is where you either suck it and see which is a bad idea but not always unavoidable or you attempt genuine feature detection which means in this example you create a test Array and test the forEach method works how you expect it to The downsides to this approach although it is the most robust and futureproof way of writing your code is that all these checks are a performance penalty If youre sure the method youre checking for is going to work how you expect it to then should you waste timeeffort writing additional checkstests to ensure the method works how the specification dictates What happens if you do the full feature detection and discover the method doesnt work how you expect it Youll still then need to implement some kind of fallback or lose the functionality that relies on that method These are important decisions that need to be made and ones that are outside the realms of this post Im afraid simply because there are no easy answers Now detecting Host methods is actually worse because they can be implemented in any fashion the host environment chooses So far it has been noted that checking the typeof result for a Host method will normally result in either function object or unknown so if you get one of these back as a result then its a good chance the host object youre checking for is available to use but as you should be able to tell by now this is a flawed process fun heh Again this isnt a reliable assumption to make because in a futurenew host environment they might have a typeof result that is none of the above Literally you could check the typeof for a method and its result could be spacecraft there are no rules as far as the Host environment is concerned But for testing a host method exists the following function has become the defacto standard Feature Testing a Host Method Because a callable host object can legitimately have any typeof result then it cant be relied upon notes The reason for the objectproperty is because in ECMAScript version 3 a null object has typeof result object which is considered a bug In future versions ECMAScript 6 the typeof result will be null as it should be reference httpmichauxcaarticlesfeaturedetectionstateoftheartbrowserscripting function isHostMethodobject property var type typeof objectproperty return type function This is the result were expecting as the test is for a method type object objectproperty Protect against ES3 null typeof result being object type unknown For IE 9 when Microsoft used ActiveX objects for Native Functions were checking property of ActiveX object So lets take a quick recap of whats going on here function For most browsers the typeof operator will result with function when passed a callable host object object objectproperty Because were dealing with host objects we cant expect function to be returned and in most cases as far as ECMAScript 3 ES3 implementations are concerned the result will normally be object which is incorrect but allowed as far as the ES3 spec is concerned So first of all we check for object If that matches we then check to make sure the property coerces to true The reason for this is that ES3 allows the host to return whatever they like so if the property youre checking for is actually null the ES3 typeof result for most browsers will still be object even though the result is null So to work around this issue we coerce the result into a boolean so if null is the result it will coerce to false and thus this whole expression will return false otherwise itll return true unknown In older versions of IE less than 9 it implements some of its host objects not as Native functions but as ActiveX objects admittedly this is deep browser implementation talk and normally you dont need to know this stuff but in this instance its important to understand what the heck is going on with IE So in IE calling the typeof operator with properties of an ActiveX Object will result in unknown When is it OK to modify them Modifying builtin Native objects isnt as dangerous as host objects as already noted by Kangax httpperfectionkillscomextendingbuiltinnativeobjectsevilornothttpperfectionkillscomextendingbuiltinnativeobjectsevilornot but care needs to be taken to ensure the augmented object works as the spec dictates something that isnt possible all the time for example like with Objectcreate As far as host objects are concerned never ever ever ever modify or augment them just too dangerous "},{"title":"HTTP/2","tags":["HTTP","HTTP2","go","network","nginx","performance"],"href":"/posts/http2","content":" Introduction1 Persistent Connections2 Multiplexing3 Compression4 Prioritization5 SSLTLS6 Server Push7 Implementations8 Nginx9 Go10 References11 Introduction This is a super quick post for demonstrating how to utilise the new HTTP2 protocol If youre unfamiliar with it then let me spend a brief few moments discussing some of the highlights Single persistent connection2 Multiplexing3 Header compression4 Prioritization5 Encryption6 Server Push7 If none of these features make sense then allow me to clarify further Persistent Connections When using HTTP1x each resource your web page specified would need its own connection If you had three images on a page then that would be three separate connections With HTTP2 the situation is improved by utilising a single connection which supports the concept of a stream A stream is effectively a two way channel so information flows up and down it and a single connection will be able to manage as many streams as necessary This removes the need for previous performance techniques such as domain sharding a way to side step the problem of browsers only being able to parallelize a limited number of connections to the same domain image spriting combining multiple images into one to reduce multiple connections to the server concatenating cssjs combining multiples stylesheets or javascript files into a single file to reduce multiple connections to the server This also means that the browser is able to more precisely cache resources as there is no need to have to bundle all your static assets together This also avoids the user downloading assets for a page that they will never visit Multiplexing This simply means that multiple resources can be loaded in parallel over a single connection Just to be clear this is a very good performance boost and facilitates the ability to transfer lots of resources in a much more efficient manner than HTTP1x Compression Header information will no longer be sent over the wire in plaintext format Itll now be compressed making it smaller and the responses subsequently quicker to receive although admittedly this is only a marginal gain This also means we should be less concerned about having to serve static assets from a cookieless domain which was a problem because the size of the static resources would all become larger due to cookie data being associated with them Prioritization Because all connections are multiplexed into a single connection we need a way to prioritize certain requests over others in order to ensure the fastest possible overall response HTTP2 supports the concept of weighting each stream see Persistent Connections above for details of what a stream is I wont dive into the specifics of how this has been designed suffice to say if you want the gory details then I recommend you read the specification document here http2githubiohttp2spechttphttp2githubiohttp2specrfcsection532 SSLTLS The above highlights also suggest a reduction in the overall time cost associated with the SSLTLS handshake process Heres why A single connection will minimize SSL handshaking back and forth between the clientserver Multiplexing allows requests to be handled asynchronously Compressing the HTTP headers will make the connection smaller and subsequently faster Prioritized connections means allowing relevant requests to be handled in an appropriate order Server Push In HTTP2 the server now has the ability to send additional information along with the initial HTTP request made by the client Now its important to realise that the concept of server push isnt the same thing as ServerSent EventshttpsdevelopermozillaorgenUSdocsWebAPIServersentevents ServerSent Events allows the server to push updates to the client and as long as the client is listening for the relevant event the client will be able to receive the pushed notification Server Push isnt the same thing and was designed to solve a different use case With HTTP2 the server is able to send the client additional resources even though the client hadnt explicitly requested them A typical example given is when the client requests a HTTP page and that page has some static resources like CSS and JavaScript In HTTP1x the client would request a web page and then start parsing it only to discover the page includes CSS and JavaScript resources The client would then have to make additional requests for those static resources But with HTTP2 the server can save the client from making multiple requests by sending all the other static resources in parallel for the clients initial request for the main pagedocument Implementations Im not sure exactly how many implementations are available for the HTTP2 specification out in the wild but there are two that well look at here in this article nginxhttpswwwnginxcom and Gohttpsgolangorg If youre interested in other implementations then you can find a list of alternative options here githubcomhttp2http2specwikiImplementationshttpsgithubcomhttp2http2specwikiImplementations Nginx The latest release of nginx both its opensource and paid for models has good support for HTTP2 but for the moment at least it doesnt support Server Push Im going to presume that youre already familiar with nginx and how it works so I wont bother explaining things a basic nginx user would already know Below is a snippet from a nginxconf file that has enabled HTTP2 support http server listen 443 ssl http2 servername integralistcouk sslcertificate etcnginxcertsservercrt sslcertificatekey etcnginxcertsserverkey ssltrustedcertificate etcnginxcertscacrt As you can see the listen directive specifies http2 In essence this is all you would need to enable HTTP2 using nginx The reason were restricting nginx to listening on port 443 and enabling ssl specifying SSL certificates is because the majority of web browsers require TLS in order to support HTTP2 and also nginxs implementation relies upon TLS see below for details Note currently Opera and Safari 9 supports HTTP2 without TLS Nginx is a reverse proxy and so because the client doesnt have direct access to the backend servicesapplications nginx is able to translate HTTP2 into HTTP1x which also allows those services to not have to be rearchitected When a client communicates with nginx itll typically pass a list of protocols it supports along with the request Nginx will attempt to identify the h2 protocol within that list which indicates HTTP2 support specifically nginx implements the Application Layer Protocol Negotiationhttpstoolsietforghtmlrfc7301 extension for TLS If HTTP2 isnt supported then nginx falls back to HTTP1x instead Go If youre not using a load balancer or a reverse proxy such as nginx then you might still be able to implement HTTP2 support via your application server One such example is with the Go programming language Below is an example application which demonstrates how to enable HTTP2 support Because were not utilising a reverse proxy we dont have SSL termination handled automatically for us and so our application will need to handle the TLS handshake process package main import fmt html log nethttp golangorgxnethttp2 func main var server httpServer http2VerboseLogs true serverAddr 8080 http2ConfigureServerserver nil httpHandleFunc funcw httpResponseWriter r httpRequest fmtFprintfw URL qn htmlEscapeStringrURLPath ShowRequestInfoHandlerw r logFatalserverListenAndServeTLSlocalhostcert localhostkey func ShowRequestInfoHandlerw httpResponseWriter r httpRequest wHeaderSetContentType textplain fmtFprintfw Method sn rMethod fmtFprintfw Protocol sn rProto fmtFprintfw Host sn rHost fmtFprintfw RemoteAddr sn rRemoteAddr fmtFprintfw RequestURI qn rRequestURI fmtFprintfw URL vn rURL fmtFprintfw BodyContentLength d 1 means unknownn rContentLength fmtFprintfw Close v relevant for HTTP1 onlyn rClose fmtFprintfw TLS vn rTLS fmtFprintfw nHeadersn rHeaderWritew Note the above code has been slightly modified from an example originally conceived by Kim Ilyonghttpsplusgooglecom111824860449692850794posts If you stick the above code into a file called http2go run the program and visit httpslocalhost8080 in your browser using one that supports HTTP2 obviously then you should see the following output or something similar URL Method GET Protocol HTTP20 Host localhost8080 RemoteAddr 163555 RequestURI URL urlURLScheme Opaque UserurlUserinfonil Host Path RawPath RawQuery Fragment BodyContentLength 0 1 means unknown Close false relevant for HTTP1 only TLS tlsConnectionStateVersion0x303 HandshakeCompletetrue DidResumefalse CipherSuite0xc02f NegotiatedProtocolh2 NegotiatedProtocolIsMutualtrue ServerNamelocalhost PeerCertificatesx509Certificatenil VerifiedChainsx509Certificatenil SignedCertificateTimestampsuint8nil OCSPResponseuint8nil TLSUniqueuint80xf6 0xb 0xf8 0x95 0x6f 0x73 0x4f 0x26 0x8f 0x72 0x26 0xab Headers Accept texthtmlapplicationxhtmlxmlapplicationxmlq09imagewebpq08 AcceptEncoding gzip deflate sdch AcceptLanguage enUSenq08 CacheControl maxage0 Cookie chartbeat2CAgQSrCqRzJnCmxa4b143498342731714349837590851 Dnt 1 UpgradeInsecureRequests 1 UserAgent Mozilla50 Macintosh Intel Mac OS X 10111 AppleWebKit53736 KHTML like Gecko Chrome460249071 Safari53736 Youll notice the Protocol HTTP20 which indicates were handling HTTP2 now If youre using the Chrome web browser you can also download an extension called HTTP2 and SPDY indicator which will display a blue lightning bolt on any site that is serving content via the HTTP2 protocol References HTTP2 FAQhttpshttp2githubiofaq HTTP2 Implementationshttpsgithubcomhttp2http2specwikiImplementations Go HTTP2 Demo Pagehttpshttp2golangorg Go HTTP2 Example Codehttpsgithubcombradfitzhttp2treemasterh2demo "},{"title":"Key Architecture","tags":["key","gpg"],"href":"/posts/key-architecture","content":" Introduction1 Visual2 Breakdown3 Vulnerabilities4 Conclusion5 Introduction Just a quick post to cover the key architecture Im using currently Im very interested to know how others are doing things in the hope that I can improve the security of my setup Note Im not a security paranoia nut so Im not looking for the most concrete solution But definitely want to be sure Im not missing anything obvious either Visual Here is a highlevel view of what I have currently Breakdown Lets start at the top Laptop The laptop is password protected and the hard drive is automatically encrypted Password Data Store The laptop contains a password data store consisting of GPG keys for every record in the data store In order to access the data store or any of its records you need a private GPG key The data store is backed up to a private online git repository Private GPG Key The private key is itself protected by a secure passphrase ie one that is almost impossible to crack by todays standards and recommendations and Ive not written it down but memorised it entirely The privatepublic key pair is stored on the laptop but as the key is protected by a secure passphrase I feel without going over the top on security this is as good as Im probably going to get with it The private key is also backed up onto a remote USB drive in case I lose my laptop and I need the key in order to access the data in the password data store Private Git Repository The private git repo has two routes of access The account itself with the relevant service provider and an SSH key that exists on my laptop to allow it to gain access to the contents of the repository but not the contents of the files in the data store itself as they are protected by my private GPG key The passphrase for both the account and the SSH key are secure by todays standards and recommendations and are stored inside the password data store If my laptop was lost then Id need to rely on the service providers ability to reset my account password via email where by I could then remove the existing SSH key and replace it with a new one I mention replacing the SSH key because I dont back it up I feel this SSH key doesnt need to be a long lasting key unlike my private GPG key which I intend to keep as safe as possible the offline backup as a fallback Vulnerabilities There are a few layers to this architecture and so Im hoping this makes it harder for a compromise to be effective If someone compromises my laptop ie gets access to it then they cant access the password data store as they cant access my private key Although a compromise could result in a key logger being installed and record what I type for my passphrase The only way to get my private key is to locate my remotely stored and safe USB that homes the raw private key contents If someone compromises the private git repository they again cant do anything with the contents without my private GPG key Note when exporting a private key from GPG it is by default encrypted with its passphrase its not the raw key Conclusion So what do you think Is this good bad or just plain terrible Please let me know your thoughts on twitter integralisthttpstwittercomintegralist "},{"title":"Load Testing Guidelines","tags":["load-testing","performance"],"href":"/posts/load-testing-guidelines","content":" Introduction1 Use real datasets2 Identify collateral damage3 Stub services if necessary4 Distribute your traffic5 Automate for reproducible runs6 Consider disabling authentication7 Dont immediately scale dependencies8 Send traffic from different geographical regions9 Decide if your tool should support reusing connections10 Start testing a single instance before moving onto clusters11 Verify load using multiple strategies12 Reset your environment between test runs13 Document the results14 Tools15 Introduction This post should serve as a guideline for running load tests It is designed to be a concise list of notes and tips For a list of load testing tools including their reviews and benchmark results I refer you to the Tools15 section at the end Use real datasets Parse your access logs for real user requests then replay those requests within your load test Also be mindful of different time periods for more accurate distribution of transactions Look for realistic worst case situations there may be more than one Identify collateral damage What users or upstreams will be affected by this additional load Are there any vulnerable dependencies that should be notified of the load test or protected from it see next section4 not all services are completely isolated Consider what happens when your services is using an external API service If the API is doing its job right it should start rate limiting you or denies you access for exceeding your thresholds But that still doesnt necessarily mean you have to be a bad web citizen and start hammering their service on a side note you should probably be considering caching Stub services if necessary Rather than hit a real upstream try creating a mock version The benefit of doing this is that you can your mock service to allow for controlling the latency andor throughput restraints thus mimicking different failure scenarios and seeing how your service behaves under different conditions Distribute your traffic When your service response is fairly big its easy to hit a network capacity limit so if youre seeing that your service doesnt scale with addition of new instances this may be the root cause To help avoid this use distributed testinghttpsgithubcomtsenartvegetausagedistributedattacks heres an example wrapperhttpsgistgithubcomIntegraliste4b4e53dd09745b645e10e89fc133f63 to simplify the process Automate for reproducible runs Write simple scripts that let you coordinate a fresh load test run Such as configuring mock services and defining specific strategies eg running a soakhttpsenwikipediaorgwikiSoaktesting test vs a shorter duration load test that mimicks a slow rampup in traffic pattern vs a load test that mimicks a thundering herd approach Consider disabling authentication It can be hard to include auth tokenskeys in load testing tools and as such it is often easier to use a network secured backend and a custom code branch that either allows for auth bypass or has no authentication Note although being able to load test with authentication is important as it could highlight important problem points in your architecture design Dont immediately scale dependencies If you have a dependency such as redis or another type of data store then leave it configured to the size it currently is If you have a stage environment it can be tempting to configure that stage resource to be identical to your production environment But it would be better to first verify if that resource is even a problem point by analysing the results of an initial load test with all resources configured normally for that environment Once youve run your load test if you find youre having no issues then sure you could consider increasingscaling up the resource in question But ultimately if it wasnt a problem before then it is unlikely to be an issue once it has even more computememorynetworketc Send traffic from different geographical regions It can be beneficial to setup load testing instances in multiple regions and availability zoneshttpdocsawsamazoncomAmazonRDSlatestUserGuideConceptsRegionsAndAvailabilityZoneshtml This is ideal for a highly dynamic application expected to be globally utilized as it means you can accurately simulate traffic with different latency expectations Although if you have a simplestatic application maybe the use of CDN edge cachinghttpswwwfastlycom is enough to mitigate concern Decide if your tool should support reusing connections A tool such as SiegehttpsgithubcomJoeDogsiege doesnt support reusing existing http connections This can be interesting as far as identifying how your system behaves under those conditions But reusing existing connections is mostly preferred so that throughput can be properly verified Tools such as Vegetahttpsgithubcomtsenartvegeta and Wrkhttpsgithubcomwgwrk support connection reuse and have better performancefeatures Start testing a single instance before moving onto clusters We should ideally identify the threshold of a single instance before moving onto testing a cluster of instances This is so that we can make appropriate tweaks to our applications based on the initial load testing results and should then help improve the initial cluster results as well Verify load using multiple strategies There are multiple types of load testing strategies constant rampup soak test and more Research and identify which is the most appropriate for your service under test It may be that youll want to execute multiple strategies Reset your environment between test runs Ensure your system is back to normal that means different things to different services before starting another load test Otherwise your test results will be skewed by the system being in a hot state and could also mean putting your upstream services under DoShttpsenwikipediaorgwikiDenialofserviceattack like scenarios Document the results It is beneficial for all ie your team and others to be able to review the load test results Ensure each test is documented in a consistent known and obvious location For example store them in a docsloadtests folder inside your service code base and include any key fixes eg changes made that resolved a problem in your service performance Tools There are various loadstress testing tools available The following for me are the top three opensource choices Siegehttpswwwjoedogorgsiegehome Vegetahttpsgithubcomtsenartvegeta Wrkhttpsgithubcomwgwrk For a review of these tools and many more opensource options please read Load Testing Tools Reviewhttpblogloadimpactcomopensourceloadtestingtoolreview Load Testing Tools Benchmarkshttpblogloadimpactcomopensourceloadtestingtoolbenchmarks "},{"title":"Logging 101","tags":["logging","logs"],"href":"/posts/logging-101","content":" Logs1 Levels2 Quality3 Logs Applications should record informationevents to help make debugging and understanding what a program is doing easier Typically this information is recorded into a log filehttpsenwikipediaorgwikiLogfile But in some cases it is preferred to send this information to stdouthttpsenwikipediaorgwikiStandardstreams because it then becomes the responsibility of the operating systemenvironment to determine where and how those logs are stored To quote 12factorhttps12factornetlogs A service never concerns itself with routing or storage of its output stream It should not attempt to write to or manage log files Instead each running process writes its event stream unbuffered to stdout Applications should be conscious of the volume and quality3 of the logs they emit Logging isnt free and high volume logs arent necessarily more useful than fewer thoughtful log messages Include important context with your log messages to be able to better understand the state of the system at that time You may discover that some log information is better off not logged but recorded as time series metrics so they can be provided and analysed by tools such as Datadoghttpswwwdatadoghqcom The benefit of doing this is that you can more easily measure those metrics and gain more insights over time It can also depending on the tools you use to analyse your log data eg external log analysis system such as Papertrailhttpspapertrailappcom be better to log data in a more structured format in order to provide better contextual information and to make filtering logs easier For more information on structured logging I recommend this articlehttpskartarnet201512structuredlogging inspired from The Art of Monitoring Levels When recording log data there are various levels you can utilise that indicate different severitieshttpsenwikipediaorgwikiSyslogSeveritylevel Below are some common log levels along with a short description that describes when you wouldshould expect to use that particular level Note I saw these descriptions in a tweet a long time ago and saved them off and then months later when Ive come to write this post I have since lost the link to the original tweet If someone knows what it is could you ping me on twitter and Ill get the author credited appropriately Fatal Wake me up at 4am on a Sunday Error Apologize to the user and raise a ticket Warn Make a note in case it happens again Info Everythings fine just checking in Debug Fill my harddrive with stack traces Quality There are many logging best practice articles below is a short bullet list pulled from SplunkhttpdevsplunkcomviewloggingSPCAAAFCK I recommend reading through their articlehttpdevsplunkcomviewloggingSPCAAAFCK for the full details Use clear keyvalue pairs Create events that humans can read Use timestamps for every event Use unique identifiers Log in text format eg log an images attributes but not the binary data itself Use developerfriendly formats eg json Log more than just debugging events Use categorieslevels eg info warn error debug Identify the source Keep multiline events to a minimum "},{"title":"Message Passing in Object Oriented Code","tags":["patterns","ruby"],"href":"/posts/message-passing-in-object-oriented-code","content":" Introduction1 Quick example2 The Proxy Design Pattern3 How Ruby handles method calls4 Implementing methodmissing5 Conclusion6 Introduction In my previous posthttpwwwintegralistcoukpostsobjectorienteddesign I quoted the following description of objectoriented design ObjectOriented Design is about the messages that get sent between objects and not the objects themselves The reason I felt this quote was important for good code design was because it helped focus our attention on improving our objects interfaces Since then Ive been reading through Design Patterns in Rubyhttpdesignpatternsinrubycom by Russ Olsen and in the chapter on the Proxy design pattern he reiterates thinking about objects more from the perspective of messages and how that can help improve the Proxy pattern implementation His comments really nailed home for me the design benefits of thinking more about messages being passed to objects and its that point which I want to elaborate on below Quick example Imagine the following code example accountdeposit50 When thinking about a statically typed language object methods are generally considered to be more baked into the objects in the sense that running the above code example suggests you are calling the deposit method found on the account object But in a dynamically typed language such as Ruby this doesnt make a lot of sense because the account object might not actually contain a method called deposit statically typed languages are compiled and so we can be assured that if we call a method on an object it will be there otherwise the program would fail to compile so talking about calling a method on an object is not as accurate as describing it like so were sending a deposit message to an account object The Proxy Design Pattern The Proxy design pattern is where we place an object between the user and the actual object the user wishes to interact with There are a few different types of proxy object Protection proxies Remote proxies Virtual proxies The reason message passing came up in the Proxy design pattern specifically when developing a virtual proxy which is where we create a proxy object to prevent an expensive object instantiation operation from happening until the user actually interacts with one of the methods on the real object was because the author wanted to avoid the situation where we would need to implement a stub method for each method found on the real object This isnt necessarily an issue for all types of objects But if you look at builtin objects such as the Array object that has approximately 118 maybe more methods So for us to implement a proxy for that object wed theorectically need to implement 118 stub methods each of which would simply forward on the request to the corresponding method on the real object to handle That would not only be tedious but an inefficient way to implement our proxy object How Ruby handles method calls In Ruby if you pass a message eg call a method to an object and that method doesnt exist then Ruby will try to find another method on that object methodmissing If methodmissing doesnt exist then Ruby will try to lookup the method on the parent object and will keep moving up the inheritance chain until it reaches the core Object object which does implement methodmissing and which simply raises a NoMethodError exeception Implementing methodmissing If you implement methodmissing on your proxy object then you can pass on the message to the real object more efficiently than stubbing the method So instead of this class AccountProxy def initializerealobject realobject realobject end def depositamount realobjectdepositamount end ad infinitum end account Accountnew proxy AccountProxynewaccount proxydeposit50 we should really take advantage of the dynamic nature of the Ruby language to avoid having to manually write out these methods by hand like so class AccountProxy def initializerealaccount subject realaccount end def methodmissingname args subjectsendname args end end You can see from the above example that were using the sendhttprubydocorgcore20Objecthtmlmethodisend method to pass the message ie the method invoked by the user on the proxy object directly to the real object Conclusion As you can see focusing on passing messages not only helps inform us of better interfaces when designing our application but also makes us more efficient by utilising features unique to dynamically typed languages "},{"title":"Mocking in Python","tags":["tests","testing","mocking","python"],"href":"/posts/mocking-in-python","content":" Introductionintroduction unittestmock or mockunittestmockormock Decoratordecorator Resource locationresourcelocation Mock returnvalue vs sideeffectmockreturnvaluevssideeffect Mock Nested Callsmocknestedcalls Verify Exceptionsverifyexceptions Clearing lrucacheclearinglrucache Mock Module LevelGlobal Variablesmockmodulelevelglobalvariables Mock Instance Methodmockinstancemethod Mock Class Methodmockclassmethod Mock Entire Classmockentireclass Mock Async Callsmockasynccalls Mock Instance Typesmockinstancetypes Mock builtin open functionmockbuiltinopenfunction Conclusionconclusion Introduction Mocking resources when writing tests in Python can be confusing if youre unfamiliar with doing such things In this post I am going to cover various aspects of mocking code which will hopefully be a useful resource for those who are a bit stuck Note in the code examples Im using pytesthttpsdocspytestorgenlatest but for the most part that shouldnt matter unittestmock or mock In order to mock a resource well first need the mock module and this is our first stumbling block which version do we need ie theres two and they both look to be official mock and unittestmock The mock module is a backwards compatible library you can download from PyPy where as unittestmock is the same thing but only compatible with the version of Python youre using So in almost all cases youll want to import it like so import unittestmock as mock For more examples see this reference guidehttpwwwvoidspaceorgukpythonmockexampleshtml Decorator The most common way to mock resources is to use a Python decorator around your test function mockpatchthing def teststuffmockthing mockthingreturnvalue 123 In this case what were patching thing can be a variable or a function When you do this youll need to pass an argument to your function you can name it whatever you want which will be a MagicMockhttpsdocspythonorg37libraryunittestmockhtmlunittestmockMagicMock This means if you dont do anything else then calls to thing will in the example above at least result in the value 123 being returned convention is to name the variable mock If youre mocking multiple things then youll stack the mock decorators ontop of each other and pass them along in order to the test function mockpatchthird mockpatchsecond mockpatchfirst def teststuffmockfirst mocksecond mockthird Resource location Its important to know that when mocking you should specify the location of the resource to be mocked relevant to where its imported This is best explained by way of example Imagine I have a module appfoo and within that module I import another dependency like so from appbar import thing You might think that when you call mockpatch that you pass it a reference to the resource like appbarthing That would only be relevant if the resource was being called with that full path within the appfoo module eg if appfoo called appbarthing If the full namespace path isnt referenced which it isnt in the above example notice we import just the thing resource It means we need to specify the reference namespace to mock as where its imported mockpatchappfoothing So even though thing exists within appbar we specify appfoothing as appfoo is where weve imported it for use This catches people out all the time Mock returnvalue vs sideeffect If your function has a tryexcept around it then you can use sideeffect to cause the calling of the function to trigger an Exception as the returned value mockpatchappawssdkconfirmsignup sideeffectExceptionwhoops Note if you had used returnvalueExceptionwhoops then the mock would return the string representation of the Exception rather than raising an exception like sideeffect does Otherwise if you just need a static value returned so its evaluated at the time its defined not when its called then you can use returnvalue instead mockpatchappsecuritysecrethash returnvalue Mock Nested Calls Calling a property on a mock returns another mock so in order to mock very specific properties youll need to nest your returnvalue or sideeffect m mockMagicMock mreturnvaluegetsideeffect 1 2 mreturnvaluepostreturnvalue foo x m xget 1 xpost foo xget 2 Verify Exceptions If we want to verify that some piece of code throws an Exception type when we need it to we can mock specific resources to throw an exception and then use pytestraises as a context manager around the caller of our code to be verified In the following example our code in appaccountconfirm catches a generic Exception and reraises it as exceptionsCognitoException We can catch and make assertions against this expected behaviour by first mocking the resource we want to throw an exception and get it to throw our own fake exception using the sideeffect parameter Next we specify the exact exception type were expecting to be raised using pytestraisesT mockpatchappawssdkconfirmsignup sideeffectExceptionwhoops def testaccountconfirmfailuremocksignup with pytestraisesexceptionsCognitoException as excinfo appaccountconfirm123 foo assert True is True this will never be executed assert excinfotypename CognitoException assert strexcinfovalue SIGNUPCONFIRMATIONFAILED Note dont make the mistake of putting any assertions within the with context manager Once the Exception is raised by the function being called within the with context manager all code after it inside the block is skipped Clearing lrucache If a function you wish to test has the functoolslrucache decorator applied then youll need to be mindful of mocking the response of that function as itll be cached in one test and the cached result will be returned when calling the function again to test some other behaviour and might likely confuse you when you see the unexpected response To fix this issue is very easy because lrucache provides additional functions when decoratoring your functions it provides cacheinfo cacheclear The latter cacheclear is what you would need to call This is demonstrated below lrucache5 def foo printExecuting foo foo Executing foo foo foocacheinfo CacheInfohits1 misses1 maxsize5 currsize1 foocacheclear foo Executing foo notice the side effect of print is executed again Note debugging this isnt always obvious Later on I demonstrate how to mock the builtin open functionmockbuiltinopenfunction and in that scenario I stumbled across this issue because although I wasnt mocking the top level function itself I was mocking the call to open within the contents of the file being opened was what was returned and being cached Mock Module LevelGlobal Variables With a module variable you can can either set the value directly or use mockpatch In the following example we have the variable clientid which is a global variable inside the appaws module which we import to reference elsewhere in our code import appaws def testaccountconfirmsuccessful appawsclientid 456 used internally by confirm mockpatchappawsclientid 456 def testaccountconfirmsuccessful In the mockpatch example there are two key things to notice 1 we dont use returnvalue 2 there is no mock instance passed to the test function This is because were modifying a variable and not a direct function or callable so theres no need to pass a mock into the test function if you want to change the value a few times within the test itself then you would mock the variable but not immediately assign a value in the decorator Mock Instance Method When needing to mock a class instance method you can take advantage of the fact that a Mock will return a new mock instance when called mockpatchfoobarSomeClass def teststuffmockclass mockclassreturnvaluemadeupfunctionreturnvalue 123 The reason the above example works is because were setting returnvalue on our mock Because this is a MagicMock every attribute referenced returns a new mock instance a function or property you call on a mock doesnt have to exist and so we call madeupfunction on the returned mock and on that newly created mock we set the final returnvalue to 123 Mock Class Method To mock a class method is a similar approach to mocking an instance method in that you mock the entire class but you have one less returnvalue to assign to mockclassClassMethodNamereturnvalue 123 Mock Entire Class To mock an entire class youll need to set the returnvalue to be a new instance of the class mockpatchmyappappCar def testclassself mockcar class NewCarobject def getmakeself return Audi property def wheelsself return 6 mockcarreturnvalue NewCar See other class related mocking tips herehttpschaseseibertgithubioblog20150625pythonmockingcookbookhtml Mock Async Calls Mocking asynchronous code is probably the most confusing aspect of mocking My go to solution Ill explain first but after that Ill share some alternative methods Ive seen and tried in the past First consider this asynchronous code inside of a appfoo module import appstuff async def dothingx return await appstuffsomeconcurrentfunctionx If we need to mock the coroutine appstuffsomeconcurrentfunction then we can solve this by creating a function that acts as a coroutinehttpsdocspythonorg37libraryasynciotaskhtmlasynciocoroutine and allow it to be configurable for different types of responses Note the example uses tornadohttpswwwtornadoweborgenstable for running an asynchronous test def makecoroutineresponse You could pass response as a mock or a raw data structure doesnt matter async def coroutineargs kwargs args will be whatever is passed to the original async function Meaning you could have a conditional check that lets us change the response to be anything we need return response return coroutine class TestThingtornadotestingAsyncTestCase mockpatchappstuffsomeconcurrentfunction tornadotestinggentest def testasyncfuncself mockthing mockthingsideeffect makecoroutinesome response result yield appfoodothingxyz assert result some response If you do include an if statement within makecoroutine you could pass in a MagicMock as a simple way of having a single input give you multiple different values back def makecoroutineresponse async def coroutineargs kwargs if args0 x return responsex elif args0 y return responsey else return responsedefault return coroutine m mockMagicMockx1 y2 default3 coro makecoroutinem When dealing with sideeffects that need to sometimes trigger an Exception and other times suceed you could use a slightly modified mock implementation that checks if the given response object is callable or not count 0 def makesideeffectcoroutinesideeffect Side effect friendly mock coroutine In some tests we need to have a mocked coroutine return a different value when its called multiple times but a mock sideeffect cant trigger a raised exception when given an iterator and so we have to construct that behaviour ourselves async def coroutineargs kwargs return sideeffectargs kwargs if callablesideeffect else sideeffect return coroutine mockpatchappthing def testconfirmemailchangefailureself mockthing def sideeffectsargs kwargs Use global var to control mock side effects global count if count 0 raise Exceptionwhoops count 1 return dont raise an exception the first time around mockthingsideeffect makesideeffectcoroutinesideeffects If the above approach doesnt work for you here are some alternatives Monkey Patch allow mock to be used as an await expression async def asyncresponse return namedtuple bodystate success def mockasyncexpressionourmock return asyncresponseawait mockMagicMockawait mockasyncexpression MagicMock Subclass class AsyncMockMagicMock async def callself args kwargs return superAsyncMock selfcallargs kwargs class TestHandlerstestingAsyncTestCase mockpatchapphandlerstriggersoftcdnpurge newcallableAsyncMock mockpatchapphandlersapi testinggentest async def testupdatecacheself apimock triggersoftcdnpurge response mockMagicMock responsecode 200 apimockbuzz AsyncMockreturnvalueresponse Async Inline Function mockpatchappbuzzapiapigateway testinggentest async def testbuzzapiself clientmock async def geturl kwargs return clientmockgetsideeffect get Mock Instance Types When mocking an object youll find that the mock replaces the entire object and so it can cause tests to pass or fail in unexpected ways Meaning if you need to make a mock more like the concrete interface then there are two ways to do that 1 spec 2 wrap We can use mocks spec feature to mimic all methodsattributes of the object being mocked This ensures your mocks have the same api as the objects they are replacing Note there is a stricter specset that will raise an AttributeError This is best demonstrated with an example import unittestmock as mock import tornadosimplehttpclient from tornadohttpclient import AsyncHTTPClient httpclient AsyncHTTPClient typehttpclient tornadosimplehttpclientSimpleAsyncHTTPClient isinstancehttpclient tornadosimplehttpclientSimpleAsyncHTTPClient True isinstancemockMagicMock tornadosimplehttpclientSimpleAsyncHTTPClient False m mockMagicMockspectornadosimplehttpclientSimpleAsyncHTTPClient isinstancem tornadosimplehttpclientSimpleAsyncHTTPClient True The wrap parameter on the other hand allows you to spy on the implementation as well as affect its behaviour In the following example I want to spy on the builtin datetime implementation pytestmarkparametrizeinputdate inputurl valid 20170617T000000000000Z foo True 20170618T000000000000Z bar False mockpatchapphandlersdatadatetime wrapsdatetime def testvalidvideomockdatetime inputdate inputurl valid mockdatetimenowreturnvalue datetime2017 6 18 00 00 00 000000 assert validvideoinputdate inputurl is valid Mock builtin open function Pythons mock library provides an abstraction for mocking the builtin open function a lot simpler def testloaduimessagessuccessful Verify ui message YAML file can be read properly filecontent foo bar with mockpatchbfauthutilityopen mockmockopenreaddatafilecontent createTrue as mockbuiltinopen assert utilsloaduimessagespathtononexistingfileyaml foo bar The createTrue param set on mockpatch means that the mockMagicMock returned will automagically create any attributes that are called on the mock this is because the open function will attempt to access lots of different things and its easier for mock to mock out all of that for you Conclusion There well end Hopefully this list of mocking techniques will be able to see you through even the most complex of code needing to be tested Let me know what you think on twitter "},{"title":"Modern JavaScript: Webpack and Babel","tags":["babel","eslint","javascript","js","node","npm","webpack"],"href":"/posts/modern-js","content":" Introduction This post will explain how to setup and configure the various tooling necessary in order to be able to write crosscompatible modern ES2015 JavaScript code Note if youre unsure of what modern JavaScript looks like then Ill refer you to these compatibility tableshttpkangaxgithubiocompattablees6 The tools well be using Babelhttpsbabeljsio transpiler of modern JS into ES5 compatible code Webpackhttpswebpackgithubio a js module bundler Note webpack is actually capable of transforming bundling packaging just about anything as well see shortly To clarify you dont need webpack as babel handles transpiling modern JS code into ES5 compatible code but it makes sense to use webpack still as a way to help with the performance of your clientside services With that in mind the configuration Ill be describing will be using webpack primarily and under that Ill be using webpack loaders to utilise the babel transpiler If you werent using webpack the configuration would be different as youd be configuring babel directly instead of webpack Example Project Were going to create a very basic project Its so basic it doesnt really do anything Its the bare minimum required in order to demonstrate the setup and configuration of webpack and babel I purposely did this because learning new tech can be confusing enough without needing to understand a realworld application at the same time One thing youll need upfront though is Nodehttpsnodejsorg and NPMhttpswwwnpmjscom installed as well be installing webpack and babel from existing NPM packages Lets begin by creating our project directory mkdir modernjs cd modernjs Note I will be working exclusively from the terminal Now create an empty packagejson file npm init y Next well install all the relevant packages well be needing npm install savedev webpack webpackcli webpackdevserver babelcore babelpresetenv babelloader800beta styleloader cssloader sassloader nodesass eslint4x babeleslint8 npm install save babelpolyfill Note the dev dependencies need to be installed in the order theyre specified above otherwise npm will complainfail Your packagejson should now have the following content name modernjs version 100 description main indexjs scripts test echo Error no test specified exit 1 keywords author license ISC devDependencies babelcore 712 babelpresetenv 710 babeleslint 826 babelloader 804 cssloader 100 eslint 4191 nodesass 493 sassloader 710 styleloader 0230 webpack 4202 webpackcli 312 webpackdevserver 319 dependencies babelpolyfill 700 There are two important Babel related packages to pay attention to 1 babelpolyfill 2 babelpresentenv The babelpolyfillhttpsbabeljsiodocsenbabelpolyfill dependency will help emulate a full ES2015 environment and this means you can use new builtins like Promise or WeakMap static methods like Arrayfrom or Objectassign instance methods like Arrayprototypeincludes amongst others see the documentation for more information The babelpresetenvhttpsbabeljsiodocsenbabelpresetenv helps manage the browser environment for you so itll handle determining what additional scriptspolyfills you need By default itll setup everything to support ES5 environments but you can configure it for very specific browsers if you dont need to worry about older browsers Note babelpolyfill also provides a useBuiltIns flag which allows Babel to selectively polyfill builtin features that were introduced as part of ES6 Because it filters polyfills to include only the ones required by the environment we mitigate the cost of shipping with babelpolyfill in its entirety Now lets create all the files we need to build out our example application mkdir src dist touch eslintrc srcindexjs srccomponentjs srcstylesscss distindexhtml This should result in the following tree structure tree I nodemodules eslintrc dist indexhtml src componentjs indexjs stylesscss We can see we have one Sasshttpssasslangcom file and two JavaScript files as well as a single HTML page that will load our scriptscss Lets now look at the contents of each of these files eslintrc parser babeleslint globals console true document true window true rules bracestyle 2 1tbs allowSingleLine true camelcase 2 commaspacing 2 commastyle 2 curly 2 eollast 2 indent 2 2 keyspacing 2 newcap 2 newparens 2 nolonelyif 2 nomultispaces 2 nomultipleemptylines 2 max 2 funccallspacing 2 notrailingspaces 2 quotes 2 single allowTemplateLiterals true semi 2 semispacing 2 spacebeforeblocks 2 spaceinparens 2 spaceinfixops 2 spaceunaryops 2 arraycallbackreturn 2 blockscopedvar 2 consistentreturn 2 eqeqeq 2 guardforin 2 noarrayconstructor 2 nocaller 2 nocondassign 2 noconstassign 2 nocontrolregex 2 nodeletevar 2 nodupeargs 2 nodupeclassmembers 2 nodupekeys 2 noduplicatecase 2 noemptycharacterclass 2 noemptypattern 2 noeval 2 noexassign 2 noextendnative 2 noextrabind 2 nofallthrough 2 nofuncassign 2 noimpliedeval 2 noinvalidregexp 2 noiterator 2 noloneblocks 2 noloopfunc 2 nomixedoperators 2 groups in instanceof allowSamePrecedence false nomultistr 2 nonativereassign 2 nounneededternary 2 nounsafenegation 2 nonewfunc 2 nonewobject 2 nonewsymbol 2 nonewwrappers 2 noobjcalls 2 nooctal 2 nooctalescape 2 noredeclare 2 noregexspaces 2 noscripturl 2 noselfassign 2 noselfcompare 2 nosequences 2 noshadowrestrictednames 2 noshadow 2 nosparsearrays 2 notemplatecurlyinstring 2 nothisbeforesuper 2 nothrowliteral 2 noundef 2 nounexpectedmultiline 2 nounreachable 2 nounusedexpressions 2 allowShortCircuit true allowTernary true nounusedvars 2 nousebeforedefine 2 nofunc nouselesscomputedkey 2 nouselessconcat 2 nouselessconstructor 2 nouselessescape 2 nouselessrename 2 nowith 2 radix 2 requireyield 2 useisnan 2 validtypeof 2 wrapiife 2 any You dont have to worry too much about the contents of this file as its just the configuration Im using to define what is good JavaScript syntax In other words if your code editor is configured properly then any JS code you write that violates any of these ES linter values will be flagged as an error distindexhtml Hello Webpack This is simple enough a HTML page that loads a bundlejs script which currently doesnt exist and will be generated by webpack via babel srccomponentjs const c x y z export default c A simple script that defines an Array of items and then exports them using modern JS module syntax familiar to people who may have written Node applications before srcindexjs eslint noundef error eslintenv browser import babelpolyfill import stylesscss import c from componentjs consolelogc let a 1 let b 2 a b b a consoleloga consolelogb const root documentcreateElementdiv rootinnerHTML Hello Webpack documentbodyappendChildroot A simple script that imports from our componentjs and then logs it to prove the import code works it then demonstrates let variables and another modern JS feature known as destructuringhttpsdevelopermozillaorgenUSdocsWebJavaScriptReferenceOperatorsDestructuringassignment before finally creating a HTML element and populating it with some text and inserting it into the DOM of the HTML page At the top of the script youll notice some code comments These are used to tell our ES linter package what context this script is running in and so global references such as document or console wont trigger a linting error as weve told the linter that the context the script will run is the browser environment where those globals are expected to exist The code comment for eslintenv could be replaced by adding individual references into the eslintrc file and I have done that take a look at the globals field in the file contents shown earlier but I prefer the code comment as it can be a much clearer indicator of the expectations of the files scope Youll also notice that we import the babelpolyfill at the top of the file This module must always be the first import in the file Lastly youll notice we also import a Sass file which admittedly is a bit strange considering were dealing with a JS file but well dig into this a little bit more later on and why we do that srcstylesscss color blue body backgroundcolor color This is a simple Sass file that demonstrates how to use a variable to generate dynamic CSS output in this case the body element should have a blue background Webpack Configuration OK at this point we have a set of JS files that cant be used in some browsers due to the fact that they use features not supported by most web browsers So we want to compile this code down into something that is understandable to most browsers ie ES5 standardized code To do that well be using Babel as our transpiler and Webpack as our module bundler Webpack will take the separate JS files and combine them into a single bundlejs file which our HTML will attempt to load So lets create a webpackconfigjs file touch webpackconfigjs Well then add the following contents to that file eslintenv node const path requirepath moduleexports entry srcindexjs output filename bundlejs path pathresolvedirname dist publicPath dist module rules test js exclude nodemodulesbowercomponents use loader babelloader test scss use loader styleloader loader cssloader loader sassloader Youll notice a similar code comment at the top of the file which again is for the benefit of our ES linter In this case it wont complain when it sees certain global references such as require or module as weve told the linter that the context the script will be run in is the Node environment where those globals are expected to exist We see the entry field of the configuration is telling webpack that the main application JavaScript file is going to be found at srcindexjs Webpack will look at this file and then traverse back up its dependency tree looking for imports and it will combine each of those separate files into one single file The file that is generated and where is determined by the output field We can see we want the file to be called bundlejs and we want the file to be saved to the dist directory this is indicated by the path field The publicPath field is a bit different in that it tells the webpackdevserver package where to find the bundlejs file that the HTML is attempting to load What wont be clear yet is the fact that when running our code locally in dev well be using webpackdevserver because it allows us to utilise a web server for running our code as well as hot reloading which means if were dealing with a complex singlepage application with lots of nested state that a change in code doesnt cause us to lose the state the page is in For production well statically generate our final bundle file using the standard webpack command and so youll see shortly that we need to update our packagejson to include two npm run commands that let us use either webpackdevserver or webpack depending on where our code needs to run The module field tells webpack that for every file it finds before adding it to the final bundlejs to run it through a loader script for additional processing In this case all js files are passed through babel and so their modern JS code is transpiled into ES5 code first before being added to bundlejs Finally we look for any Sass scss files and tell webpack to pass those files through multiple loaders sassloader transforms Sass into CSS cssloader parses the CSS into JavaScript and resolves any dependencies styleloader outputs our CSS into a tag in the document Loaders are executed in a nested fashion meaning the above order of loaders would evaluate to something like styleLoadercssLoadersassLoadersource Where the source file is passed into the Sass loader and so transforming the Sass into CSS That CSS is then passed into the CSS loader which allows it to be parsed by JavaScript Finally the Style loader places our CSS into a tag within our HTML page Packagejson Update As mentioned earlier we want to modify the packagejson so that we have two npm run commands for letting us run our code in development mode ie locally or compile our code ready for use in production The following snippet shows the changes needed to be made scripts build webpack modeproduction dev webpackdevserver modedevelopment config webpackconfigjs Now we can run either npm run dev for local dev or npm run build to compile our bundle for production Youll notice that we pass a specific mode flag to both webpack and webpackdevserver and this indicates to both tools what to do to the files its configured to interact with In the case of modeproduction the webpack tool will make additional modifications that means the bundlejs output is as efficient as possible such as minifiying and obfuscating the code Whereas modedevelopment will allow for the generation of webpack source map files to aid you in debugging Conclusion This should be the basics covered of how to use babel with webpack and hopefully is enough to help you kickstart your exploration of new JavaScript features "},{"title":"Observability and Monitoring Best Practices","tags":["instrumentation","monitoring","observability"],"href":"/posts/monitoring-best-practices","content":" This post aims to discuss key monitoring discussion points and to summarise the relevant best practices when instrumenting application performance monitoring Below are some of the areas well be focusing in on Terminology1 Understand the different types of monitoring2 Data collection methods3 Frontend monitoring4 Make it useful then actionable5 Focus on user impact51 Favour organic changes over static thresholds6 Send critical and noncritical alarms to different channels7 Give context8 Think about data aggregation9 Know your graphs10 Map your graphs101 Choosing between a metric or log11 Reference material12 Note we primarily work with Datadoghttpswwwdatadoghqcom so youll see them mentioned a lot throughout this post Terminology There is a lot of confusion around the difference between certain terms such as observability monitoring instrumentation and telemetry Lets start with defining what each of these mean Observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs Wikipedia In that context observability is the word you use when talking about how well your systems are doing in a broad overarching sense is the system observable Beneath the umbrella term observability well then find monitoring and instrumentation Monitoring is the translation of IT metrics into business meaning Wikipedia In that context monitoring is the word you use when talking about tools for viewing data that has been recorded by your systems whether that be time series data or logging etc These monitoring tools are supposed to help you identify both the what and the why something has gone wrong Instrumentation refers to an ability to monitor or measure the level of a products performance to diagnose errors and to write trace information Wikipedia In that context instrumentation is the word you use when talking about how youre recording data to be viewed and monitored Telemetry is the process of gathering remote information that is collected by instrumentation MSDN In that context telemetry is the word you use when talking about the mechanisms for acquiring the data that has been gathered by your instrumentation eg tools like FluentDhttpswwwfluentdorg or SysloghttpsenwikipediaorgwikiSyslog Understand the different types of monitoring Although most of this document is based around one specific type of monitoring APMhttpsenwikipediaorgwikiApplicationperformancemanagement its good to be aware of the various types of monitoring available across an entire system Server monitoring monitor the health of your servers and ensure they stay operating efficiently Configuration change monitoring monitor your system configuration to identify if and when changes to your infrastructure impact your application Application performance monitoring look inside your application and services to make sure they are operating as expected also known as APM tooling Synthetic testing real time interactions to verify how your application is functioning from the perspective of your users hopefully to catch errors before they do Alerting notify the service owners when problems occur so they can resolve them minimizing the impact to your customers Data collection methods There are fundamentally two methods for data collection 1 Push sending metrics to an analysis tool 2 Pull configuring a health check endpoint that a centralised tool pulls data from When dealing with the pull model youll hear people suggest that rather than a simple 200 OK response you should add extra information that gives humans more understanding of the overall state of the service So this could be things like a successful database connection was opened But a possible concern would be the performance overhead that might need to be accounted for remember health endpoints are generally pinged every few minutes There are also various metric types you can collect data as Two common ones are Counter an ever increasing value Gauge a pointintime value can arbitrarily go up and down Histogram samples observations and counts them in configurable buckets For more information see these Datadog articles Metric Typeshttpsdocsdatadoghqcommetrictypes and DogStatsDhttpsdocsdatadoghqcomguidesdogstatsd Frontend monitoring There are two main approaches to frontend monitoring 1 Real User Monitoring RUM 2 Synthetic The difference between them has to do with the type of traffic that is triggering the data collection For example with RUM the requests being processed are from real users whereas with synthetic monitoring the requests are fake ie they are generated by your own software Synthetic monitoring causes data to be collected for analysis thus allowing you to identify the availability and performance of your system by constructing very specific test cases Make it useful then actionable Lets start with a quote from Charity Majors author of Database Reliability Engineeringhttpshoporeillycomproduct0636920039761do and CEO of honeycombiohttphoneycombio Dont attempt to monitor everything You cant Engineers often waste so much time doing this that they lose track of the critical path and their important alerts drown in fluff and cruft When a monitor triggers an alarm it should first and foremost be useful Secondly it should be actionable There should be something you can do to resolve the alarm and also be a set of steps postresolution to prevent that alarm from triggering again If the alarm isnt actionable then it just becomes noise Focus on user impact Below is a quote from Mike Julian author of Practical Monitoringhttpshoporeillycomproduct0636920050773do and Monitoring Weeklyhttpsweeklymonitoringlove Go as deep and wide with your instrumentation as you want but always be asking yourself How will these metrics show me the user impact Or put another way Who is your customer Depending on the services you build your customers might be people who pay money to use your service or they might be other engineers within your organisation who are consuming an API youve developed Either way your customer is the most important thing to you and your service You need to keep them happy For most users whether they be a nontechnical paying customer or an engineer they will have certain criteria that will make them happy You could imagine the sorts of questions theyll ask to be something like 1 I want the service to work as expected ie you should monitor whatever is determined to be a success rate for your service 2 I want the service to be fast ie you should monitor the service latency 3 I want to use this much of the service ie you should monitor things like requests per second do you have SLAhttpsenwikipediaorgwikiServicelevelagreements defined In essence you should start by creating monitors for things that have a direct impact on your users For example measuring OS level metrics such as CPU and memory consumption is great for diagnostics performance analysis as they help to highlight the underlying system behaviour But you should probably not be using these stats for alarming as their values can have relative meaning in different situations and they dont necessarily offer much in the way of understanding the root cause of the problem Instead try monitoring 5xx errors and very slow latency times These metrics are much more useful indicators of problems with the system and real negative user experiences Ultimately the deeper you go the more specific your alarms become and the less useful they are at identifying trends and patterns Favour organic changes over static thresholds Static thresholds such as the number of errors reported has exceeded N have a habit of raising false alarms due typically to unexpected spikes in data anomalies This happens so frequently that most monitoring toolsservices such as nagios offer flapping detection to help prevent these deviations To help with this services such as Datadoghttpswwwdatadoghqcom offer a feature called recovery thresholdshttpswwwdatadoghqcomblogintroducingrecoverythresholds which helps to quieten monitor state changes so you can be confident when a monitor switches back to OK state that it has in fact definitely resolved itself The way it works is like so you give Datadog a threshold value that must be met to consider the monitor back to normal Once the monitor state switches to ALARM it will now never flipflop between OK and ALARM It will only ever go back to OK if the set recovery threshold goes below the specified value They also offer anomaly detectionhttpsdocsdatadoghqcomguidesanomalies which detects when a metric is behaving differently than it has in the past taking into account trends seasonal dayofweek and timeofday patterns This can be more useful for organically identifying issues as it allows for buffer zones around your static thresholds Datadog also offers outlier monitoringhttpsdocsdatadoghqcomguidesoutliers which detects when a specific member of a group eg hosts availability zones partitions is behaving unusually compared to the rest They are useful for noticing when a given group which should behave uniformly isnt doing so Note A summary of Datadogs various detection methods can be found herehttpsdocsdatadoghqcomguidesmonitors Send critical and noncritical alarms to different channels At BuzzFeed I work in the software infrastructure team and there we have two separate Slack channels for handling monitoring notifications ooinfraalarms ooinfrawarnings Only alarms that require immediate review such as a 5xx monitor goes into ooinfraalarms Everything else is sent to ooinfrawarnings because although important they arent surfacing immediately as user issues If you dont do this then youll find people become fatigued by the sheer amount of noise coming from multiple alarms Especially alarms that are firing due to unactionable anomalies To quote again Charity Majors In the chaotic future were all hurtling toward you need the discipline to have radically fewer paging alerts not more You should also consider sending a monitors warning state to a different channel for similar reasons You can define different channels in Datadog using the following template code isalert slackmychannelforseriousalarms isalert iswarning slackmychannelforjustwarningstokeepaneyeon iswarning Give context When an monitor triggers an alarm and youre oncall that night then you might be unfamiliar with the system and its dependencies One quick way to help people oncall is to provide them with additional context about the alarm and the affected system A general message template could look something like the following An example might look something like FooService 5xx FooService is serving a high number of 5xx responses these will not be cached at CDN possibly resulting in further cache misses FooService serves responsive article pages for the BuzzFeed website wwwbuzzfeedcom and is fronted by a CDN It has an upstream dependency on Redis and the internal Buzz API v2 Please refer to the monitoring for more details Logs Dashboard Timeboard Request Breakdown Alarms highlight the symptom and not the cause So if at all possible try to include information or data that might aid the oncall person in identifying the root cause of the issue Think about data aggregation When dealing with TSDBs Time Series DatabasehttpsenwikipediaorgwikiTimeseriesdatabase youll find they will start aggregating multiple data points into a single data point This is known as the roll up effect Note if you werent aware a TSDB is made up of keyvalue pairs The key is the timestamp and the value is the metric value at that point in time The problem with rolling up data is that it smooths out your data points Meaning you shift from a graph that has lots of spikes ie a graph that shows every possible false positive to a graph that covers up those false positive spikes On the plus side rolling up data like this means you get to see the data at a higher level and so patterns of use start to emerge There have been examples in the past where important spikes in CPUMemory were not captured due to the smoothing out of aggregated data and so it can be useful to look at your data closely at first and then in some instances force the disabling of rolling up data using Datadogs rollup method Ultimately youll need to find the balance that works best for you and your monitoring requirements Note you can read more about this smoothing out process herehttpshelpdatadoghqcomhcenusarticles203571289Whydoeszoomingoutatimeframealsosmoothoutmygraphs as well as the rollup method Datadog provides to allow you to control this behaviour Know your graphs We wont repeat the details here but suffice to say each graph in Datadog has a purpose and specialised use case You should review Datadogs articles on the various graphs they offer and the whyswhen of using them Timeseries graphshttpswwwdatadoghqcomblogtimeseriesmetricgraphs101 Summary graphshttpswwwdatadoghqcomblogsummarygraphsmetricgraphs101 UPDATE Ive since written a blog post about stastistics aimed at beginners and so by understanding the basics of statistics youll be able to understand more clearly what certain graphs represent and how You can read the post here Statistics and Graphs The Basicspostsstatisticbasics Map your graphs It can be useful to order your graphs within a dashboardtimeboard chronologically For example CDN LB Service This can help you mentally model the request flow through your system such that you know the request starts by hitting your CDN layer its then routed inside of your infrastructure and hits a load balancer finally that load balancer distributes the request to a specific service node It can equally be useful to collate multiple services and their graphs within a single overarching dashboard because when there is a problem in the system you can follow the request flow from start to finish and see where a bottleneck or anomaly somewhere else in the chain is causing a side effect elsewhere in the chain An alternative approach is to have a dashboard that focuses on the key metrics for a services performance and underneath that theyll have graphs that monitor their dependencies So when an engineer gets a call because of an issue that seems to be with their service theyll check the dashboard and might see theres an issue upstream of them with one of their dependencies Some companies even take that approach a step further and formalize this process and subsequently define a standardized structure for dashboards ie all dashboards are structurally the same The benefit of that approach is that people oncall can start at the beginning of a request and then follow the dashboards like a thread until they reach a service that is the root cause of the problem being reported Choosing between a metric or log In order to help individual teams identify whether they should collect data as a metric or as a log one recommended approach is to ask the following questions 1 Is it easier for your team to think about metrics or logs 2 Is it more effective for the thing in question to be a log entry or metric 3 What questions do you typically ask when debugging We cant answer these questions for you but we have generally found the following approach works reasonably well as a generic pattern YMMVhttpsdictionarycambridgeorgdictionaryenglishymmv Collect an exception as both a log and an error The log helps add additional context not available in the metric The metric helps with monitoring and triggering alarms Log only errorsexceptions No news is good news Control other log calls using log levelspostslogging101 so they can be enabled when necessary Include unique identifiers with your logs This helps to quickly figure out what the log is possibly associated with when looking from a centralized distributed log system which contains many logs aggregated from many distinct services Mostly everything else well record as a metric so we can monitor pattern changes Other useful tips Datadog tagshttpsdocsdatadoghqcomguidestagging are useful for splitting metrics by type eg status codes Datadog eventshttpsdocsdatadoghqcomapieventspost are useful for capturing additional info eg exception message 99 of the time you want a Timeboard not a Screenboardhttpshelpdatadoghqcomhcenusarticles204580349WhatisthedifferencebetweenaScreenBoardandaTimeBoard Timeboards allow for tracking data points across multiple graphs at once Let people know where the dashboards are Slack pinned Runbooks etc For latency use 95th percentile standard deviationhttpsenwikipediaorgwiki68E2809395E28093997rule not just the mean average Because the mean can miss important slow requests Load balancer metrics can also be useful to monitor especially if service is falling over Reference material Practical Monitoring Effective Strategies for the Real Worldhttpshoporeillycomproduct0636920050773do book Observability and Understanding the Operational Ramifications of a Systemhttpswwwinfoqcomarticlescharitymajorsobservabilityfailure Datadog Timeseries graphshttpswwwdatadoghqcomblogtimeseriesmetricgraphs101 Datadog Summary graphshttpswwwdatadoghqcomblogsummarygraphsmetricgraphs101 "},{"title":"Multigrain Services","tags":["architecture","macro","micro","mini","nano","services","system"],"href":"/posts/multigrain-services","content":" It seems most people are either talking about monolith applications or micro services But recently Ive noticed yet another new buzzword bubbling up across the internet mini services Its worth mentioning that this isnt a post about which pattern is the most appropriate to use as theyre all appropriate to use when applied in the right context and scenario Below is an image that visualises the concept and makes understanding these various patterns much easier at a glance source thenewstackiohttpsthenewstackiominiservicesarealisticalternativetomicroservices A rose by any other name To me mini services isnt anything new nor do I think it necessarily needs a new term to be applied to it For me mini services are effectively just SOAhttpsenwikipediaorgwikiServiceorientedarchitecture Service Oriented Architecture but done right People complain about SOA and say it has lots of pain points that make it unusable I disagree From what Ive read it simply comes down to designing your SOA correctly so that it has proper fault tolerance and resiliency Not just throwing together some code seeing certain dependencies break and then crying that SOA is rubbish Although maybe a new services lexicon is useful at least for giving a consistent and modern terminology to these concepts we collectively understand the vocabulary of a person language or branch of knowledge Definitions Lets breakdown these concepts along with their oldnew terminology Macro Service Monolith Application the traditional architecture modelsystem design One big application handling multiple responsibilities and all of them plugged into a single data store Mini Service SOA an evolution of the monolith where domain boundaries were more clearly defined and made into separate services and data stores Although these services would still handle multiple responsibilities those responsibilties were at least related Micro Service we breakdown the domain boundaries even further so that servicesdata stores have very narrow and specific responsibilties Weve moved to feature driven services Nano Service we go to the extremes of boundary definitions and are working at the component level BBC explain their approach to this herehttpsmediumcombbcdesignengineeringpoweringbbconlinewithnanoservices727840ba015b Micro services are themselves a newish concept at 7 years of age at the time of writing The term was first used in 2011 Real micro services Martin Fowler famously defined microservices as having a very specific design application and that most micro services arent conforming to that Specifically he says that they should Have a single responsibility Be loosely coupled Be independently deployable and scalable But according to Ross Garrett VP of Marketing Cloud Elements an HTTPbuild service has to know more about whats going on around it in order to communicate So how do we achieve communication without relying on tried and tested HTTPREST APIs etc The answer is the observer design pattern or pubsub publisher subscriber model With the pubsub model the micro service publishes notifications whenever something happens and services can subscribe to those notifications This means the micro service has no awareness of anything outside of itself it just throws out a notification and theres either someone there to catch it and process it or there isnt Queue systems like RabbitMQ or SQS etc are used heavily in these cases to allow for that decoupling What do you use I tend to gravitate towards the SOA pattern as its a nice balance between loose coupling and separation of domain boundaries without descending into the coordination and infrastructure madness of micro services or mini services if thats the language you prefer although I personally dont think Ill ever be able to bring myself to use it over SOA "},{"title":"Multiple Branches in Git","tags":["branch","git","github"],"href":"/posts/multiple-branches-in-git","content":" Introduction There are times where you might be working from a particular git branch and need to quickly jump over to a different branch to do some urgent work Typically you would need to first git stash anything you were working on as its unlikely to be in a state where it can be committed and then youd have to leave your current branch to create a new branch from master and thus begin working on your new urgent task This is a fairly straightforward workflow but there is a mild annoyance which is that I happen to git stash a lot and I find when jumping over to a new branch to do some urgent work that I might end up git stashing a few more times along the way Ultimately when Im done with my urgent task and ready to go back to my other branch I then have to sift through my stash to find the relevant one I want to pop OK so not that tragic considering git stash list will indicate the branch on which the stash was taken which helps but I do then need to Google what the syntax is for popping a specific stash eg its git stash apply stashn where n is the index you want to apply Note for the life of me I wish I could remember the syntax but it just eludes me every time Oh and then you have to think about whether you actually want to use apply which leaves the stashed changes in the stack or if you meant to actually pop the stashed content git stash pop stashn so its properly removed from the stack This is where I was recently introduced to a concept in git referred to as a worktree thanks Kiran Worktree Git offers a feature referred to as a worktreehttpsgitscmcomdocsgitworktree and what it does is allow you to have multiple branches running at the same time It does this by creating a new directory for you with a copy of your git repository that is synced between the two directories where they are stored This is different to manually creating a new directory and git cloning your repo down because with the worktree model the two sub directories are aware of each other Note as youll see although this workflow is pretty cool you could argue that git stash is just plain simpler and easier for a human mind to reason about Ill leave that up to the reader to decide Example In the following example Im going to create a new git repo Ill make a change in master then create a new branch for doing some work Well then imagine that I have been given an urgent task that I must complete now and yet my current nonmaster branch is in such a state that I want to avoid just stashing everything Note I use tmux to split my terminal into multiple windows and this demonstration will require two windows or two separate terminal instances if youre not using a screen multiplexer for the sake of demonstration Create a new repo mkdir fooproject cd fooproject touch foo git add foo git commit m created foo file Create a new branch git checkout b foocontents echo 123 foo git add u git commit m added content to foo Now Ill create a new file and stage it for committing but I wont commit it this is where we pretend my branch is in some hideously complex state Create new worktree branch git worktree add foohotfix Note youll want to create the new worktree in a directory outside of your current repos directory just so theres a clear distinction At this point youll find your current terminal is still in the same foocontents but there is now a new directory called foohotfix outside your current repos directory Make changes in new worktree branch Open up a new terminal or split window and run through the following steps cd foohotfix or cd foohotfix if your new terminal is currently set to your main git repo directory git log OK so if you do a git log youll find that the worktree has a branch automatically created and named after the worktree so the branch is called foohotfix in my case The important thing to realize is that git worktree add is a bit like git branch in that it creates the new worktree from the current branch youre in Meaning that my foohotfix branch has the added content to foo commit from the foocontents branch as thats where I ran the git worktree add command from This is what git log looks like for me in this new worktree d374dcb Integralist HEAD foohotfix foocontents added content to foo 2 minutes ago 9ae3a7f Integralist master created foo file 3 minutes ago I dont want the commit d374dcb in there as its coming from a branch foocontents thats still in progress and so Ill need to rebase out that commit git rebase i 9ae3a7f Note the rebase editor opens and I change pick to drop to get rid of the commit Now at this point I have a new working directory that I can work in echo hotfix baz git add baz git commit m some hotfix Merge my hotfix back into master Im going to change into my master branch but remember Im still in the foohotfix directory so my main repo directory fooproject open in another terminal window is still in the foocontents branch git checkout master git merge foohotfix Removing the worktree OK so at this point weve merged our hotfix into master I want to go back to my original repo directory and make sure I have the latest master rebased in before continuing on with my foocontents work To remove the worktree you can either remove it using the git interface eg git worktree remove foohotfix or manually remove it eg cd rm foohotfix where git will at some point in the future internally run a prune and remove any references to this orphaned branchworking tree you could also manually trigger that prune using git worktree prune Note if I do git worktree remove foohotfix while currently residing inside the foohotfix directory Ill find that the git repository is removed from the directory Continuing working on my feature branch Presuming Im still in the foohotfix directory and thats where I ran git worktree remove foohotfix cd fooproject git rebase master why yes this does seem a bit strange considering thats what I was trying to avoid in the first place but in this case its a single stash and so a simple git stash pop will suffice to get me back to where I need to be I can now continue working on my foocontents branch Conclusion Well this was fun heh Do you think you have any uses for gits worktree feature Let me know on twitterhttpstwittercomintegralist "},{"title":"Multiple SSH Keys for Different GitHub Accounts","tags":["ssh"],"href":"/posts/multiple-ssh-keys-for-different-github-accounts","content":" Introduction1 The Problem2 The Solution3 Creating a new Key4 SSH Config5 Optional Shell Function6 Alternatives7 Another Alternative8 Introduction I recently had an issue with my GitHub setup which has since prompted me to write this post The issue I had was dealing with multiple GitHub accounts via SSH on a single laptop So I have a GitHub account under the username IntegralisthttpsgithubcomIntegralist This is a personal account and up until recently I was also using it to access my works private repos BBChttpsgithubcombbc and BBCNewshttpsgithubcomBBCNews When cloning a repo from GitHub you will typically create an SSH Key Pairhttpwwwintegralistcoukpostssecuritybasics61 and paste the contents of the public key into your GitHub account via their web site When you come to clone a repo youll also typically use the SSH variation of the path gitgithubcomIntegralistGoRequestergit The problem occurred when I had to remove my personal account from the BBCBBCNews repos and replace them with a generic BBCMarkMcDonnellhttpsgithubcomBBCMarkMcDonnell account The Problem So the first thing I did was create myself a new SSH Key upload the public key to my new GitHub account I then added the private key to my laptops SSHAgent sshadd K sshgithubbbcrsa I then tried to clone one of the BBCs private repos This is where I discovered I didnt have authorisation to clone the private repo It turns out that although I had both SSH Key Pairs loaded within my SSHAgent sshadd l Returns something like 4096 SHA256xxxx UsersMsshgithubrsa RSA 4096 SHA256xxxx UsersMsshgithubbbcrsa RSA it was using the first key it came across for the host githubcom so it used my personal account to try and access the private BBC repositories which obviously its no longer authorised to do This is a problem because I have two separate keys for the same host and I work on both BBC and personal code from my work laptop so I needed to figure out how to get around this issue The Solution The solution turned out to be pretty straight forward if not immediately obvious I would need to modify my sshconfig file youll need to create that file if you dont already have one Then when cloning a private BBC repo I simply modify the clone command slightly So where I would normally use git clone gitgithubcombbcmozartgit I would now use git clone gitBBCMarkMcDonnellbbcmozartgit So lets take a look at hows this is done Creating a new Key First things first create a new SSH Key Pair and name it something relevant eg I used githubbbcrsa sshkeygen t rsa b 4096 C youexamplecom Next paste the public key part into your GitHub account and add the private key to your SSHAgent eval sshagent s sshadd K sshgithubbbcrsa Note sshadd l will show you what keys have been added to the agent SSH Config Now create the file sshconfig or modify the existing one you have Host BBCMarkMcDonnell User git HostName githubcom IdentityFile sshgithubbbcrsa Host Integralist User git HostName githubcom IdentityFile sshgithubrsa As you can see Ive created two hosts 1 BBCMarkMcDonnell 2 Integralist Note you can call them whatever you like I opted for the username for each account The keys within these two hosts are exactly the same They state for the user git and the hostname githubcom make sure you use the specified IdentityFile So if I want to access the BBC private repos then Ill use the following modified git clone command git clone gitBBCMarkMcDonnellbbcmozartgit Where you can see the host section githubcom has been replaced with BBCMarkMcDonnell which maps to the host block defined inside my sshconfig file So itll use the relevant identity file needed to authorise successfully with Now the second Host Integralist is actually redundant for my use case because if I want to clone one of my own repos which are all public then Id execute something like gitgithubcomIntegralistGoRequestergit this being the same command Ive always run no modification to it What happens is SSHAgent will do what it did before which is look through the list of signatures within the SSH Agent and pick the first one that matches which happens to be my personal Integralist account any way But it works either way I can either leave it up to the SSH Agent to locate my personal account or I could explicitly specify it like so gitIntegralistIntegralistGoRequestergit Optional Shell Function The last thing I did was to create a quick shell function that allowed me to update my global git settings By default theyre set to the following git config global username Integralist git config global useremail markintegralistcouk But this means if Im pushing code for a work project then itll show those details for the author information Id rather it show more BBC specific details So whenever Im working on a BBC project Ill execute switchgithub BBCMarkMcDonnell markmcdonnellbbccouk This will change the above details to the ones provided I can then execute it again whenever I switch backed to a personal project like so switchgithub Integralist markintegralistcouk The function is added to my zshrc file function switchgithub git config global username 1 git config global useremail 2 print updated values just to be sure echo git config global username git config global username echo git config global useremail git config global useremail Alternatives So Simon Thulbournhttpstwittercomsthulb informed me that he personally wouldve used GITSSH as a simpler alternative to the above modification I made to my sshconfig file Now the following might not actually be the way he was thinking to do this but it seems to be the most common route people take using GITSSH so thats the one Im covering The way GITSSHhttpswwwkernelorgpubsoftwarescmgitdocsother works is like so When set git fetch and git push will use the specified command instead of ssh when they need to connect to a remote system So one way we could use this environment variable is like so Create the following script file sshgitsh binsh if z PKEY then ssh if PKEY is not specified run ssh using default keyfile else ssh i PKEY fi Note script originally written by Alvin Abadhttpsalvinabadwordpresscom20130323howtospecifyansshkeyfilewiththegitcommand Next well need to make this custom script executable chmod x sshgitsh Now well need to set GITSSH to point to this script export GITSSHsshgitsh Finally we can execute our git clone command and specify the key would like it to use PKEYsshgithubbbcrsa git clone gitgithubcombbcmozartgit Personally I prefer the sshconfig solution as it feels a little cleaner to me as apposed to using a custom user script and then still having to specify your key manually every time you git clone The config route seems simpler Although that being said there are quite a few different ways GITSSH can be used see Alvin Abads blog post for more ideas But now you know about GITSSH maybe youll find a variation that suits you or youll decide to just create your own Enjoy Another Alternative Ive found all sorts of issues recently with my original solution with things like Rubys bundler or cli scripts that are hardcoded to use gitgithubcom where I cant change it to be a different host The solution is a manual step but as Ive managed to automate the process see below its faster than what I was using before and doesnt require me to export any variables or retype the host name AND it actually works across everything so far The downside is that it only works with one other key If you had to switch between three keys work home other then youd need to find a different solution or use the initial solution I defined at the start of this post Simply add the following inside your ssh config I comment it out by default switch Host githubcom User git HostName githubcom IdentityFile sshgithubbbcrsa Note obviously change the IdentifyFile to point to your own private key Then if you have a project that requires you to use your work ssh keys then uncomment it so it becomes active Ive also automated the process using the following alias the switch comment is important as well as the line number that it starts on in your config file alias sshconfignvim c norm 12ggVjjjgc c wq sshconfig cat sshconfig awk switch fori0 i3 i getline print So this uses NeoVim although it works the same with standard Vim to open the file and to use Tim Popes Commentary plugin to toggle the comments around the Host block My switch line starts on line 12 of my config file so you might need to change the alias to fit your use case I then use Awk to display only those lines so I can see whether its toggled onoff Thats it Seems to work fine for me "},{"title":"MVCP: Model, View, Controller, Presenter","tags":["mvc","patterns","ruby"],"href":"/posts/mvcp","content":" Introduction1 Model View Controller Mixed definitions2 God Controller Problems Skinny Controller Presenters3 What problem are Presenters trying to solve How do they work Code Example4 Controller View Presenter Conclusion5 Introduction Model View Controller MVC This is a pretty standard architectural pattern and has been in use when developing software since the early 1970s The basic principle of the pattern is to separate the different areas of logic from your application into distinct compartments Model The model holds your business data Typically this will be data that is pulled in from a database or external data service of some kind View The view is your user interface This is what the client will interact with when using your application Controller The controller is the boss He sits at the top and delegates responsibilities to either the view or the model Mixed definitions There seems to be a dispute in the dev community regarding how the responsibilities should be divided Some feel a fat controller principle is best where by the controller tells the model not only when but where and how it should get its data My understanding of the pattern is that it was designed so that the Controller stays skinny It may be the boss but like most good bosses it doesnt try and stay in control It knows the best team member for the job at hand and delegates accordingly This is also good code design because the Controller doesnt have too much context ie it doesnt know everything which means itll be easier to maintain and scale God Controller There are a few ways we can implement an MVC pattern one is known as the God Controller This is where a single Controller exists and it oversees everything no matter what was requested by the client For example the single Controller would be passed the request from the client usually handled by a custom routing application and most frameworks will provide their own The Controller would determine what type of request was made if the request was for a contact page then itll make a request for the Contact model or if the request was for the about page then itll make a request for the About model Once it knows the type of request itll proceed to get the relevant model data and assign it to some View variables and render the required View Problems Now there are two problems with this implementation 1 maintainability 2 scalability As mentioned before this comes down to bad code design The God Controller knows too much and tries to do too much Once you start getting above a few different types of requests youll start to understand what a mess the code can become by having multiple branches for different routing scenarios I work as an engineer for the BBC News team in London and we had suffered from this exact setup hence the lessons the team has learnt and improved upon are the reason why Im able to write this post for you now Skinny Controller There is another approach we can take which is known as the skinny controller approach The way it works is that a request will come into the application and will get passed to a page specific Controller The page specific Controller will call the relevant Model and will assign the returned data to a few View variables The Controller will then render a View and pass through the variables into the View for it to use As you can see this isnt that different from the God Controller with the exception that the Routing part of the application now will have extra logic which determines which specific Controller should be loaded This is a better situation to be in because youre making your code base both more maintainable and scalable Note as I mentioned in the previous section BBC News had a sort of God Controller issue and our first step to resolving the problem was to take a similar approach as described above ie to start creating page specific Controllers That was a good first step The next step from here was to separate out our logic even further by implementing Presenters and it was our tech lead at BBC News John Cleveleyhttptwittercomjcleveley who made that decision which resulted in a much more efficient maintainable and scalable code base Presenters What problem are Presenters trying to solve Lets imagine weve gone for the Skinny Controller approach There are still some inherent issues First of all our Controller can still have too much context and be handling more information than it should But also and more importantly you may find there is still a lot of duplication of code across your Controllers The reasoning for this is that if you consider the structure of a web pageapplication youll notice that it is typically made up of unique features For example if youre displaying your tweets on a page then thats a unique feature Each feature must be able to stand on its own We normally describe these features as being components Each component can be loaded whenever and wherever needed Having a component based architecture allows your code base to become more modular and reusable For example the navigation menu on a page could be considered a component Also the navigation menu component is likely going to need to appear on every single page of the application So if youre splitting up your logic into page specific Controllers then its possible that youre still repeating code across the Controllers to handle the loading of reoccurring components such as the navigation eg pulling its data from a navigation Model and setting View variables etc Now there are ways that this code repetition can be avoided and one such way is to use the concept of Presenters How do they work Presenters like everything in software engineering can be implemented in many different ways For example at BBC News we initially were manually creating new Presenter instances within our page Controllers But the team here are quite clever chaps especially Robert Kennyhttptwittercomkenturamon and Simon Thulbournhttptwittercomsthulb and they realised that this process could be greatly improved by using configuration files instead specifically YAMLhttpyamlorg As we have multiple teams working on the BBC News code base and in multiple languages using configuration files is a much easier and maintainable solution Im not going to go into the configuration setup we use at BBC News Instead Ill focus on the basic principles of how Presenters work which is quite simply a case of moving the logic getting component specific Model data and assigning it to to component specific variables into separate files called Presenters which you can instantiate within your controller Code Example Controller Here is a basic example in Ruby require apppresentersa require apppresentersb class AboutController ApplicationController get do a PresentersAnew b PresentersBnew title About erb about end end in this example we have an About page which is made up of two components a and b As you can see we require the presenters which handle those two components and within our Controller we instantiate the Presenters Notice thats all we do Each Presenter encapsulates the logic needed to prepare the data to be passed to the about view template View Before I show you the Presenter code itself Ill show you the View template file atitle summary asummary data adata bname age bage as you can see we have very minimal logic in place If anything I have too much logic in the View as I initially was reusing the same View template over and over and so I wanted to protect again errors appearing when loading a template which referenced a component I wasnt loading but Ive since changed how my application was working but left the conditional checks in as an example of how code can evolve over time We literally just check to see if the component has been initialised in this case we created a run property we set to true when the components Presenter is first initialised We then render the View for the component and pass through the variables that were setup from within the Presenter Now I can also open up my home View file and add in the a component there as well just as easily It would be even easier if I didnt have to manually add the a component to the home View file but thats where running from configuration files like we do at BBC News would come in handy but that would have been too complicated an implementation for the sake of such a basic example as required for this post Presenter Now lets take a look at one of our Presenters in this case the Presenter for our b component require apppresentersbase require appmodelsb class PresentersB modelname age modelage end end as you can see we load a specific Model for this component and then generate our View data by passing the Model information through to a prepareviewdata method see below for the implementation details The Base Presenter which our component Presenters inherit from is very straight forward as you can see from the following example module Presenters class Base attraccessor model def prepareviewdata hash hasheach do name value instancevariablesetname value end end end end its just a module namespace with a base class that has a single method prepareviewdata which dynamically generates instance variables based on the data we passed through from the inheriting Presenter class and which then are usable within the View Conclusion Thats all there is to it as far as understanding the Presenter pattern Its a nice clean solution for componentising your different page features and keeping your code more easily maintainable Ive created a repo on GitHub called MVCPhttpsgithubcomIntegralistMVCP which is written in Rubyhttpswwwrubylangorg and uses the Sinatrahttpwwwsinatrarbcom web framework Note I had some help from my colleague Simonhttptwittercomsthulb in cleaning up and refactoring some of the code it may only have been minor changes but as with all good refactorings it made a massive difference to the quality of the code so thanks to him for helping out If you have any questions then feel free to contact me either here on twitterhttptwittercomintegralist and let me know your thoughts "},{"title":"New Laptop Configuration","tags":["configuration","laptop","new"],"href":"/posts/new-laptop-configuration","content":" Introduction Im an engineer with a new laptop which requires setting up with various development tools and configuration This post is my attempt to capture and document my process for getting a new dev environment setup I used to try and automate a lot of this with bash scripts but realised over time that things go out of date quite quickly eg OS configurations can change substantially as well as my preferred ways of working I also find that if an error occurs with an automated script unless youve coded things defensively enough you can end up with your machine in a weirdly broken state Given a straight forward set of instructions doing things manually doesnt take long at all and you can modify things at that point in time without just blindly installing various things you no longer need Heres a list of what were going to be discussing Defaultsdefaults Package Managerpackagemanager Essential Packagesessentialpackages Essential Appsessentialapps Curlcurl Gitgit Shellshell Terminalterminal GitHubgithub Password Storepasswordstore Gogo Pythonpython Vimvim Tmuxtmux Miscellaneousmiscellaneous macOSmacos Homebrew Outputhomebrewoutput Defaults Its good to begin by surveying your current system and understanding what you have already installed For me this looked something like OS macOS Mojave 10144 Curl usrbincurl 7540 Bash binbash 3257 Python usrbinpython 2710 Ruby usrbinruby 237p456 Git usrbingit 2201 PATH usrlocalbinusrbinbinusrsbinsbin Whats worth me noting additionally here is that I primarily use two programming languages Go and Python The reason I mention this is because Python has an interesting history with the name of its binaries The binary name python generally refers to Python version 2x Where as Python 3x has traditionally been named python3 to help distinguish the two So looking above we can see which python reveals the location as usrbinpython and without checking the version eg python version I was fairly certain it would be a 2x version based on the naming history This has been the generally accepted rule for a while except when dealing with tools that handle virtual environments For example pipenvhttpspipenvreadthedocsio is a tool that helps you to manage not only different Python versions but also the dependencies installed for different projects referred to as virtual environments A tool like pipenv will proxy a command such as python through a shim script eg Usersintegralistpyenvshimspython and that shim script will then determine which Python interpreterbinary to execute A shim script typically identifies the virtual environment youre working under and will then figure out the most appropriate Python interpreter to invoke So within that virtual env if you call python then you may not necessarily get the Python2 interpreter as your virtual env might be configured such that the expectation is to proxy your invocation to a Python3 interpreter This is why when setting up a new laptop getting a good development environment setup is essential because it can get quite confusing untangling a mess of default Pythons vs brew install versions of Python 3 and then subsequently using multiple environment tools like pipenv which confuse things further by hiding the actual versions behind the generically named python command The situation reminds me a lot of XKCDs classic comic striphttpsxkcdcom1987 Package Manager Lets begin our journey by first installing a package manager This software will enable us to search and install various pieces of software The macOS provides its own GUI implementation referred to as the App Store but its heavily moderated by Apple and an app can only be found there if it abides by Apples own set of rules and criteria for what they consider to be safe software Note there are many apps that arent available in the App Store because Apple can be a bit anticompetition see Spotifys time to play fair campaignhttpstimetoplayfaircom So we have to download our own package manager and the defacto standard for macOS is a program called Homebrewhttpsbrewsh which is a terminal based tool so no GUI In fact Im not actually sure what alternatives to Homebrew exist other than MacPortshttpswwwmacportsorg which if you want to understand the differences between it and Homebrew then read thishttpssaagarjhacomblog20190426thoughtsonmacospackagemanagers On Linux you have tools such as yum or apt but for macOS you either use the builtin App Store or find your own alternative so in this case well use Homebrew To install Homebrew execute the following command in your terminal usrbinruby e curl fsSL httpsrawgithubusercontentcomHomebrewinstallmasterinstall Note notice that installation command uses the default installation of Ruby which Homebrew presumes is available and for the most part is a safe presumption to make as Ruby as been provided by macOS for the longest time If you need to update Homebrew you can execute a brew update command but the installation will install the latest version any way so that wont be necessary Essential Packages OK so I start with what I refer to as a essential packages and specifically these are packages that do not require any configuration on my part Meaning I can install them and consider the job done where as with other packages I install Ill have to make some additional tweaks to which well see as we move on past the essential segments of this post To install a package via Homebrew execute the following command in your terminal brew install Here is a list of the packages Ill install ag a grep like tool aka thesilversearcher gnused its the gnu version of sed gsed used for filteringtransforming text jq tool for parsinginspecting json output docker useful for running containerized programs hugo static site generator used to make this website node serverside js programming language used to compile a static search feature for my website pwgen generates random passwords reattachtousernamespace used by tmux for clipboard storage shellcheck bash linter sift another command line search tool tree displays directory heirarchy structures as a tree watch executes given command every N seconds Heres a handy oneliner brew install ag gnused jq docker hugo node pwgen reattachtousernamespace shellcheck sift tree watch Essential Apps Homebrew now allows you to also install GUI applications not just command line tools but to do this youll need to configure Homebrew to use Cask brew tap homebrewcask Once thats done you can install an app via Homebrew using the command brew cask install Here is a list of the apps Ill install alfred like Apples Spotlight search but better caffeine stops the Mac from going to sleep dash offline documentation docker this is the counterpart to the package installed earlier googlebackupandsync syncs files between computer and Google Drive googlechrome web browser keybase encryption tool lepton GitHub Gist UI slack communication tool spotify music streaming service if you installed the docker package then you need the docker app as well for it to work You cant have one without the other this is because the app sets up the interface for macOS to interact with the underlying docker clientserver implementation Heres a handy oneliner bash brew cask install alfred caffeine dash docker googlebackupandsync googlechrome keybase lepton slack spotify The Dashhttpskapelicomdash app will ask you what documentation you would like to download so its available offline I use the following docsets I used to have lots more but realised I never really used them so this is my essential docs list boto3 Go HTTP Header Fields HTTP Status Codes NGINX Python2 Python3 Regular Expressions tmux Tornado vim Curl I like to use a more modern version of curl eg supports HTTP2 and other features bash brew install curl But in order to use this version of curl youll need to modify your PATH bash export PATHusrlocaloptcurlbinPATH Git Similarly to curl I like to have the most recent version of git installed bash brew install git Once thats installed I configure it like so bash curl LSso gitpromptsh httpsrawgithubusercontentcomgitgitmastercontribcompletiongitpromptsh curl LSso gitignoreglobal httpsrawgithubusercontentcomIntegralistdotfilesmastergitignoreglobal curl LSso gitconfig httpsrawgithubusercontentcomIntegralistdotfilesmastergitconfig Note its always worth checking gitignoreglobal is up to date ie not referencing file types I no longer work with Shell To install and configure latest version of the Bash shell execute the following commands bash brew install bash echo usrlocalbinbash sudo tee a etcshells chsh s usrlocalbinbash Also make sure to install autocomplete for bash bash brew install bashcompletion Finally well configure Bash like so bash curl LSso bashpreexecsh httpsrawgithubusercontentcomrcalorasbashpreexecmasterbashpreexecsh curl LSso bashrc httpsrawgithubusercontentcomIntegralistdotfilesmasterbashrc curl LSso bashprofile httpsrawgithubusercontentcomIntegralistdotfilesmasterbashprofile Note bashrc references fzfbash which is needed and comes from installing the FZF vim plugin which well sort out shortly Terminal To install my custom terminal theme bash curl LSso tmpIntegralistterminal httpsrawgithubusercontentcomIntegralistmacosterminalthemeintegralistmasterIntegralistterminal open tmpIntegralistterminal rm tmpIntegralistterminal Note dont forget to change the terminal font to menlo if not already set and also set Integralist theme as your default I used to do this via defaults write comappleTerminal Default Window Settings Integralist and defaults write comappleTerminal Startup Window Settings Integralist but those have changed now in the latest macOS see defaults read GitHub Lets now setup a new SSH key for GitHub access bash mkdir ssh cd ssh sshkeygen t rsa b 4096 C foobarexamplecom eval sshagent s sshadd K sshgithubrsa Dont forget to pbcopy Note there is a slight catch22 here which is if your password for GitHub is in your Password Store see next section then that makes things trickier For me I also have a copy of the encrypted store on my phone and so I can utilise that to access the password But failing that you can just reset your password in GitHub UIs and follow the email instructions to gain access and thus login and add your new SSH key Password Store I use the opensource password storehttpswwwpasswordstoreorg for handling secrets and passwords This tool provides the pass command and that requires the use of gpg so lets start by installing GPG bash brew install gpg Now you have gpg make sure you pull your private key from wherever you have it stored eg external USB stick then execute bash gpg import gpg export public key by default Note dont forget you can sign your git commits git config global usersigningkey But you might prefer to use a Keybase key for that instead Next install pass and pass otp commands brew install pass passotp zbar You can now pass a QR code into pass otp and use the terminal for generating onetime pass codes for 2FAMFA authentication bash zbarimg q raw tmpqrpng pass otp insert WorkAcmetotpfoo pass otp c WorkAcmetotpfoo Note installing zbar provides the zbarimg command Lastly we need to setup a new Password Store and to do that we need to provide our GPG key id bash needs to be your email or part of the keys description keyidgpg listkeys head n 2 tail n 1 cut d f 7 pass init keyid Now we can pull our Password Store from a private repository bash pass git init pass git remote add origin gitgithubcomFooBargit pass git pull git branch setupstreamtooriginmaster master Note I also like to ensure my encrypted datastore is synced up to other online providers and symlinked to passwordstore as well so changes are backed up automatically in multiple places Go Well install the latest version of go as far as Homebrew is concerned bash brew install go This is required because in order to handle different versions of go well want to manually compile gohttpsgistgithubcomaf300f602fa4da8cc14863f36a24bd1e and that ironically requires a version of the go compiler Finally make sure the default Go directory is set in your PATH so that any installed binaries will be available bash export PATHHOMEgobinPATH Python The macOS comes only with Python 2x and although the specific version should according to the Python docs have the pip command available thats not the case So we have to install pip for Python2 manually using the very old but builtin easyinstall command bash sudo easyinstall pip Now when running pip version we should see pip 1903 from LibraryPython27sitepackagespip1903py27eggpip python 27 At this point in order to have a sane Python setup we should look towards virtual environments There are three aproaches well look at each of them rely on pyenvhttpsgithubcompyenvpyenv 1 pipenvhttpspipenvreadthedocsioenlatestinstall 2 Poetryhttpspoetryeustaceio 3 pyenvvirtualenvhttpsgithubcompyenvpyenvvirtualenv Well start by showing you how to install pipenvhttpspipenvreadthedocsioenlatestinstall which is a highlevel abstraction across multiple tools inc pyenvhttpsgithubcompyenvpyenv and virtualenvhttpsvirtualenvreadthedocsio then well move onto installing Poetryhttpspoetryeustaceio which I prefer for reasons Ill explain later Pipenv There is a brew install bash brew install pipenv This will install Python 373 for you and so itll be made available via python3 and pip3 Pipenv cant install Python versions for you Youll need a tool such as pyenv which can be installed like so bash brew install pyenv Note pyenv will also install pythonbuild no need to install that separately but its useful to know because the version of Python you want to install will be based on whats available from pythonbuild definitions So lets setup a new Python environment remember installing pipenv resulted in python3 command being installed and thats specifically version 373 so well install a different Python3 version to that bash mkdir p CodePython371 cd CodePython371 pyenv install 371 pipenv python 371 pipenv install boto3 pytest structlog tornado pipenv install dev flake8 flake8importorder mypy tox ipython notice the following command will fail as we havent installed anything into the python3 version 373 that was installed when we installed pipenv ipython instead you can use pipenvs run subcommand to use Python 371 pipenv run python version pipenv run ipython pipenvs shell subcommand is an alternative to pipenv run itll drop you into a new shell which uses the relevant Python version pipenv shell now these commands will work as theyll be using 371 python version ipython If you dont have your bashrc setup with eval pyenv init then you wont have Usersintegralistpyenvshims prefixed to your PATH and so pipenv wont be able to locate your installed Python versions If you dont want to modify your PATH then you can manually specify the location of the Python interpreter version you want to use bash pipenv python Usersintegralistpyenvversions371binpython Note virtual environments and their project files can be found here Usersintegralistlocalsharevirtualenvs If you have problemshttpsgithubcompyenvpyenvwikiCommonbuildproblemsbuildfailederrorthepythonzlibextensionwasnotcompiledmissingthezlib installing a Python version then you might need to reinstall XCode The following is one solution that works for macOS Mojave bash xcodeselect install sudo installer pkg LibraryDeveloperCommandLineToolsPackagesmacOSSDKheadersformacOS1014pkg target Note its also useful to setup autocompletion for pipenv in your bashrc configuration file eval pipenv completion Poetry Poetry is better in that its a cleaner and more isolated installation process unlike Pipenv which requires us to brew install python3 bash install curl sSL httpsrawgithubusercontentcomsdispaterpoetrymastergetpoetrypy python reload bashprofile and check poetry version poetry version update poetry to latest version poetry selfupdate generate autocomplete for Homebrew installed version of bash poetry completions bash brew prefixetcbashcompletiondpoetrybashcompletion install python version pyenv install 2715 check help for poetry init which generates a pyprojecttoml poetry doesnt allow installing packages via cli they need to be specified in toml poetry init h create pyprojecttoml interactively see below for generated pyprojecttoml notice toolpoetrydependencies specifies the Python version used this is required poetry init install dependencies poetry install add additional dependencies use dev for dev dependency poetry add requests poetry add dev requests execute commands within the virtual environment eg dev dependency ipython was installed poetry run ipython load virtual environment permanently for the current shell eg now python version will be the expected environment not the OS version poetry shell python version here is a shortened Python3 example as the above uses the OS default of Python2 for installing 2715 where as if you tried to set the Python version in the pyprojecttoml to 37 it would fail as that Python version wouldnt be available it means whenever you want to setup a new Python3 environment youll need a compatible Python interpreter running first eg if you want to install 371 youll need 373 running first to execute Poetry this isnt necessary with Python2 as we already had 27 available by the OS pyenv install 373 pyenv local 373 poetry add boto3 pytest structlog tornado poetry add dev flake8 flake8importorder mypy tox ipython Here is an example configuration file I use toml toolpoetry name 373 version 010 description authors Integralist toolpoetrydependencies python 37 boto3 19 pytest 44 structlog 191 tornado 60 toolpoetrydevdependencies black python36 version193b0 allowprereleasestrue flake8 37 flake8importorder 0181 mypy 07010 tox 39 ipython 75 buildsystem requires poetry012 buildbackend poetrymasonryapi pyenvvirtualenv This tool is a plugin for pyenv and is designed to manage virtual environments only where as pipenv and poetry are toolkits designed for solving many different problems one of which is virtual environments You install the plugin via Homebrew brew install pyenvvirtualenv Next you add the following lines to your bashrc bash eval pyenv init eval pyenv virtualenvinit Now within some directory you can define a new virtual environment using bash pyenv virtualenv 371 testingpluginwith371 To see the available virtual environments bash pyenv virtualenvs 371envstestingpluginwith371 created from Usersintegralistpyenvversions371 testingpluginwith371 created from Usersintegralistpyenvversions371 To activate and deactivate the virtual environment bash pyenv activate testingpluginwith371 pyenv deactivate Simple Python Packages Here are some packages I like to install as a general rule blackhttpsgithubcompythonblack formatter like golangs gofmt flake8httpflake8pycqaorg linter flake8importorderhttpsgithubcomPyCQAflake8importorder validates imports mypyhttpwwwmypylangorg static analysis ipythonhttpsipythonorg REPL pytesthttpspytestorg testing framework structloghttpwwwstructlogorgenstable structured logging tornadohttpswwwtornadoweborg async web framework boto3httpsboto3amazonawscomv1documentationapilatestindexhtml AWS CLI tool toxhttpstoxreadthedocsioenlatest generic virtualenv management and testing tool Vim You can either install more recent version of vim via Homebrew bash brew install vim Or you can manually compile vim yourself Note I manually compile vim as I need Python3 support baked in which Homebrews version no longer does it used to but not any more Python3 support means my Python linting tools will work as expected bash git clone httpsgithubcomvimvimgit cd vim configure withfeatureshuge enablemultibyte enablerubyinterpyes enablepython3interpyes relies on brew install python3 enableperlinterpyes enableluainterpyes enableguigtk2 enablecscope prefixusrlocal make make install The above code will copy the compiled vim binary into usrlocalbin so which vim will show usrlocalbinvim Configure vim with vimplughttpsgithubcomjunegunnvimplug plugin manager bash curl fLo vimautoloadplugvim createdirs httpsrawgithubusercontentcomjunegunnvimplugmasterplugvim curl LSso vimrc httpsrawgithubusercontentcomIntegralistdotfilesmastervimrc Ensure Vim is configured with spell checking options bash vim E s Note fzfhttpsgithubcomjunegunnfzf doesnt need a brew install when installed via vim See my vimrc configuration file for more details but in essence it contains Plug junegunnfzf dir fzf do install all Also ensure the Golang environment has what it needs by executing GoInstallBinaries Tmux Install tmux bash brew install tmux Configure tmux and expose tmuxy command defined in my bashrc for quickly spinning up a new working environment bash curl LSso tmuxconf httpsrawgithubusercontentcomIntegralistdotfilesmastertmuxconf curl LSso tmuxsh httpsrawgithubusercontentcomIntegralistdotfilesmastertmuxsh Note check PATH to make sure tmux isnt double setting values in your PATH as it starts up If it does you can check an older version of my bashprofilehttpsgithubcomIntegralistdotfilesblobcc906bd14636543e71d9c034d6507f5986a80bbdbashprofileL18L21 for a workaround Miscellaneous Not every app can be installed via Homebrew Monosnaphttpsmonosnapcomwelcome is one such example Also if youre into torrents then npm install g tget might be of interest to you macOS It can be cool to configure the macOS via the terminal things like mouse cursor speed or keyboard repeat key performance But unfortunately that all changed with macOS Mojave and I couldnt be bothered to figure out the correct way to do it via the terminal when doing the setup via the GUI is just as quick and I know the few things I like to tweak offbyheart For example you used to be able to do things like bash defaults write NSGlobalDomain ApplePressAndHoldEnabled bool false But since macOS Mojave those settings and namespaces seem to have changed If youre interested in figuring it out then Id recommend starting with bash defaults read The above command will display all the current macOS settings for you From there you can drill down into individual namespaces like so bash defaults read Apple Global Domain comapplemousetapBehavior Note herehttpsgithubcomIntegralistdotfilesblobcc906bd14636543e71d9c034d6507f5986a80bbdbootstrapshL7L53 are the settings I used to configure via the terminal One thing I like to do is to make sure macOS Spaces feature doesnt rearrange spaces based on their recent usage and to do that you need to open up the Mission Control settings panel and disable the option Automatically rearrange Spaces based on most recent use Homebrew Output Finally for those interested below is the output of installing Homebrew for the first time I like to see what Homebrew creates so in future if I ever want to know where something should exist I can refer back to this as a reference point IntegralistMBP integralist usrbinruby e curl fsSL httpsrawgithubusercontentcomHomebrewinstallmasterinstall This script will install usrlocalbinbrew usrlocalsharedochomebrew usrlocalsharemanman1brew1 usrlocalsharezshsitefunctionsbrew usrlocaletcbashcompletiondbrew usrlocalHomebrew The following existing directories will be made group writable usrlocalbin The following existing directories will have their owner set to integralist usrlocalbin The following existing directories will have their group set to admin usrlocalbin The following new directories will be created usrlocaletc usrlocalinclude usrlocallib usrlocalsbin usrlocalshare usrlocalvar usrlocalopt usrlocalsharezsh usrlocalsharezshsitefunctions usrlocalvarhomebrew usrlocalvarhomebrewlinked usrlocalCellar usrlocalCaskroom usrlocalHomebrew usrlocalFrameworks The Xcode Command Line Tools will be installed Press RETURN to continue or any other key to abort usrbinsudo binchmod urwx usrlocalbin Password usrbinsudo binchmod grwx usrlocalbin usrbinsudo usrsbinchown integralist usrlocalbin usrbinsudo usrbinchgrp admin usrlocalbin usrbinsudo binmkdir p usrlocaletc usrlocalinclude usrlocallib usrlocalsbin usrlocalshare usrlocalvar usrlocalopt usrlocalsharezsh usrlocalsharezshsitefunctions usrlocalvarhomebrew usrlocalvarhomebrewlinked usrlocalCellar usrlocalCaskroom usrlocalHomebrew usrlocalFrameworks usrbinsudo binchmod grwx usrlocaletc usrlocalinclude usrlocallib usrlocalsbin usrlocalshare usrlocalvar usrlocalopt usrlocalsharezsh usrlocalsharezshsitefunctions usrlocalvarhomebrew usrlocalvarhomebrewlinked usrlocalCellar usrlocalCaskroom usrlocalHomebrew usrlocalFrameworks usrbinsudo binchmod 755 usrlocalsharezsh usrlocalsharezshsitefunctions usrbinsudo usrsbinchown integralist usrlocaletc usrlocalinclude usrlocallib usrlocalsbin usrlocalshare usrlocalvar usrlocalopt usrlocalsharezsh usrlocalsharezshsitefunctions usrlocalvarhomebrew usrlocalvarhomebrewlinked usrlocalCellar usrlocalCaskroom usrlocalHomebrew usrlocalFrameworks usrbinsudo usrbinchgrp admin usrlocaletc usrlocalinclude usrlocallib usrlocalsbin usrlocalshare usrlocalvar usrlocalopt usrlocalsharezsh usrlocalsharezshsitefunctions usrlocalvarhomebrew usrlocalvarhomebrewlinked usrlocalCellar usrlocalCaskroom usrlocalHomebrew usrlocalFrameworks usrbinsudo binmkdir p UsersintegralistLibraryCachesHomebrew usrbinsudo binchmod grwx UsersintegralistLibraryCachesHomebrew usrbinsudo usrsbinchown integralist UsersintegralistLibraryCachesHomebrew Searching online for the Command Line Tools usrbinsudo usrbintouch tmpcomappledtCommandLineToolsinstallondemandinprogress Installing Command Line Tools macOS Mojave version 1014 for Xcode102 usrbinsudo usrsbinsoftwareupdate i Command Line Tools macOS Mojave version 1014 for Xcode102 Software Update Tool Downloading Command Line Tools macOS Mojave version 1014 for Xcode Downloaded Command Line Tools macOS Mojave version 1014 for Xcode Installing Command Line Tools macOS Mojave version 1014 for Xcode Done with Command Line Tools macOS Mojave version 1014 for Xcode Done usrbinsudo binrm f tmpcomappledtCommandLineToolsinstallondemandinprogress usrbinsudo usrbinxcodeselect switch LibraryDeveloperCommandLineTools Downloading and installing Homebrew remote Enumerating objects 63 done remote Counting objects 100 6363 done remote Compressing objects 100 4646 done remote Total 121248 delta 31 reused 41 delta 15 packreused 121185 Receiving objects 100 121248121248 2867 MiB 1810 MiBs done Resolving deltas 100 8869688696 done From httpsgithubcomHomebrewbrew new branch master originmaster new tag 01 01 new tag 02 02 new tag 03 03 new tag 04 04 new tag 05 05 new tag 06 06 new tag 07 07 new tag 071 071 new tag 08 08 new tag 081 081 new tag 09 09 new tag 091 091 new tag 092 092 new tag 093 093 new tag 094 094 new tag 095 095 new tag 098 098 new tag 099 099 new tag 100 100 new tag 101 101 new tag 102 102 new tag 103 103 new tag 104 104 new tag 105 105 new tag 106 106 new tag 107 107 new tag 108 108 new tag 109 109 new tag 110 110 new tag 111 111 new tag 1110 1110 new tag 1111 1111 new tag 1112 1112 new tag 1113 1113 new tag 112 112 new tag 113 113 new tag 114 114 new tag 115 115 new tag 116 116 new tag 117 117 new tag 118 118 new tag 119 119 new tag 120 120 new tag 121 121 new tag 122 122 new tag 123 123 new tag 124 124 new tag 125 125 new tag 126 126 new tag 130 130 new tag 131 131 new tag 132 132 new tag 133 133 new tag 134 134 new tag 135 135 new tag 136 136 new tag 137 137 new tag 138 138 new tag 139 139 new tag 140 140 new tag 141 141 new tag 142 142 new tag 143 143 new tag 150 150 new tag 151 151 new tag 1510 1510 new tag 1511 1511 new tag 1512 1512 new tag 1513 1513 new tag 1514 1514 new tag 152 152 new tag 153 153 new tag 154 154 new tag 155 155 new tag 156 156 new tag 157 157 new tag 158 158 new tag 159 159 new tag 160 160 new tag 161 161 new tag 1610 1610 new tag 1611 1611 new tag 1612 1612 new tag 1613 1613 new tag 1614 1614 new tag 1615 1615 new tag 1616 1616 new tag 1617 1617 new tag 162 162 new tag 163 163 new tag 164 164 new tag 165 165 new tag 166 166 new tag 167 167 new tag 168 168 new tag 169 169 new tag 170 170 new tag 171 171 new tag 172 172 new tag 173 173 new tag 174 174 new tag 175 175 new tag 176 176 new tag 177 177 new tag 180 180 new tag 181 181 new tag 182 182 new tag 183 183 new tag 184 184 new tag 185 185 new tag 186 186 new tag 190 190 new tag 191 191 new tag 192 192 new tag 193 193 new tag 200 200 new tag 201 201 new tag 202 202 new tag 203 203 new tag 204 204 new tag 205 205 new tag 206 206 new tag 210 210 HEAD is now at 1c655916f Merge pull request 5993 from amysparkdropunzipinmacos Homebrew is run entirely by unpaid volunteers Please consider donating httpsgithubcomHomebrewbrewdonations Tapping homebrewcore Cloning into usrlocalHomebrewLibraryTapshomebrewhomebrewcore remote Enumerating objects 4958 done remote Counting objects 100 49584958 done remote Compressing objects 100 47294729 done remote Total 4958 delta 51 reused 1272 delta 38 packreused 0 Receiving objects 100 49584958 398 MiB 798 MiBs done Resolving deltas 100 5151 done Tapped 2 commands and 4743 formulae 5000 files 124MB Already uptodate Installation successful Homebrew has enabled anonymous aggregate formulae and cask analytics Read the analytics documentation and how to optout here httpsdocsbrewshAnalytics Homebrew is run entirely by unpaid volunteers Please consider donating httpsgithubcomHomebrewbrewdonations Next steps Run brew help to get started Further documentation httpsdocsbrewsh "},{"title":"Object Oriented Design","tags":["design","oop","patterns","ruby"],"href":"/posts/object-oriented-design","content":" Quick Summary1 Introduction2 Objects3 Class Analysis4 Dependencies5 Flexible Interfaces6 Duck Typing7 Inheritance8 Inheritance vs Composition9 Further good rules of development from Sandi Metz10 Summary11 Quick Summary Here is a short summary for those of you who prefer to see a quick bulletpoint list of items covered Decouple your code we discuss this in more detail below Describe your class to see if it does too much eg for each class write down a single line description and try to avoid the words and or from occuring Review each method thoroughly you may find some methods dont belong in your class and deserve their own interface Manage your dependencies Check the method arguments youre passing around Use dependency injection dont hard code class names Avoid direct references to complex data structures transform your data into a more appropriate form Abide by the Single Responsibility Principle SRP Review comments to ensure their purpose and usefulness Your commented code could be better handled by moving into a separate method with a descriptive name Write more flexible interfaces ObjectOriented code is more about the messages sent between objects than the objects themselves Think about the messages you want to send and create objectsinterfaces to handle them Ask for what you want and dont include how to do what you want Ensure messages you send eg method calls you make dont rely on knowledge of the object that implements the method Reduce your objects context ie how much it knows about other objects Dependency Injection can help here Trust your objects eg Duck Typing design principles If using the inheritance pattern Abstract your shared functionality into the base class Make sure sub classes inherit only what they need Avoid calling super as its a code smell Introduction All of the following information has been distilled from Sandi Metz Practical ObjectOriented Design in Rubyhttpwwwpoodrinfo and although the code in this post is based on the Ruby language dont worry the concepts are applicable for any objectoriented language I would highly recommend you read Practical ObjectOriented Design in Rubyhttpwwwpoodrinfo as the author goes into far more code detail and background information as well as covering other subjects such as testdriven development and the process of writing efficient unit tests which will help you understand the concepts better than I could in this single post But hopefully the following distilled version should be a sufficient starting point for your journey into writing more flexible and maintainable code Objects The best description I have ever read regarding good ObjectOriented design goes like this ObjectOriented Design is about the messages that get sent between objects and not the objects themselves This single line quote perfectly captures the intention behind good ObjectOriented design It seems our focus on objects has been wrong We should be thinking primarily about the messages we want to send This way we build up classes based on good clean interfaces and so our subsequent objects are clearer and more direct in their message handling Class Analysis We want our classes to be as decoupled as possible The benefit of this is to allow changes to occur over time with little to no sideeffects If your classes have too many dependencies which are likely too tightly coupled to the class then any designcode changes in the future could potentially have a negative knockon effect on the rest of your code To ensure a class only contains the behaviour it needs try describing your class in one sentence If you find you have to use the word and within your description then the class appears to have more than one responsibility this is a bad thing your classes should be small and focused on a single responsibility If you find you have used the word or to describe your class then you not only have more than one responsibility but the responsibilities arent even related That again would be an indication of a code smell Another way to analyse your classes is to ask each method within the class a question and to see if any of the answers sound out of place eg Please Mr ClassName what is your methodname This sounds strange and maybe a bit childish but its surprising what methods suddenly appear to no longer fit within the responsibilities of the class being interrogated For example look at the following class which is based on a part of a bicycle specifically gears class Gear attrreader chainring cog rim tire def initialize chainring cog rim tire chainring chainring cog cog rim rim tire tire end def ratio chainring cogtof end def gearinches tire goes around rim twice for diameter ratio rim tire 2 end end now start to ask each of its methods a question remember that attrreader generates a getter method and so those need to be queried as well Please Mr Gear what is your ratio seems fine Please Mr Gear what is your gearinches seems fine also Please Mr Gear what is your tire hmm notice this doesnt sound like it quite fits the purpose of a Gears class You can tell that the tire method doesnt fit in with a class which handles bicycle gears information and would be better suited to be placed in its own class A simple querying of the methods has pointed us in the direction of a potential code smell One other potential code smell worth avoiding is the direct referencing of class attributesproperties You should only access them via a getter method to ensure good separation of data access For example class Gear attrreader chainring cog def initialize chainring cog chainring chainring cog cog end def ratio chainring cogtof bad chainring cogtof good end end Dependencies Dependencies can be many things for example external class references or arguments passed to methods Below are some rules to help you spot a dependency and how to better manage them Direct References Avoid direct references These are things like drilling down into a complex array structure to grab some data to work with You may know the data structure now but thats not to say it wont change in the future But also linking to a complicated data structure is confusing to other users because it obscures what the data really is and what it is meant to represent So in the following example we are directly accessing item0 and item1 from a multidimensional array BAD class MyClass attrreader data def initializedata data data end def dosomething dataeach do item puts item0 puts item1 puts "},{"title":"Observability, Monitoring, Instrumentation","href":"/posts/observability-monitoring-instrumentation","content":" "},{"title":"OpsBot: Operations Slackbot","tags":["dependencies","go","google","hackweek","slack"],"href":"/posts/opsbot","content":" So this post is a long time coming It has been pushed to the forefront by the fact that BuzzFeed recently held its annual Hack Week and riding on the wind of a massive success Ive had with my hack project this past week Mark has casually saved the company 60k a year by letting us break away from a commercial dependency without losing any features BuzzFeed UK Newsletter Ive decided to revisit last years hack week project OpsBot which is a Slackbot for operational tasks Note for those interested the presentation slides for my 2018 hack can be found herepdfshackweek2018nginxpdf What does OpsBot do Creates standardized incident channels Autoinvite users to incident channel based on emoji reactions Looks up service runbooks How do you use OpsBot incident visibility reporter runbook Lets break down the arguments provided to each command name required hyphenated name of the incident eg ServiceFoo5xx visibility optional whether the channel should be public or private reporter optional has a default channel name of channel where incident was first reported service required searches our company Google Drive for specified runbook Incident Example Imagine Ive noticed something bad has happened Ill go to the appropriate Slack channel and report it At that point people who are interested in the incident will use an emoji we support various types to indicate their interest in being autoinvited to a new incident channel if one is to be created The following image demonstrates what that might look like Note we have automated notifications sent to specific monitoring slack channels and so people can also use the emoji reaction on those messages Now imagine this incident has been triaged and yes it is indeed a problem Well need to spin up an incident channel so we can focus discussions and get a resolution in place Its at that point someone runs the command incident bf4life or whatever you want to name the incident and well see the following Notice that we automatically prefix the given incident name with the current date so its easier to go back and reviewidentify past incidents If were dealing with a service that were unfamiliar with then we might also want to look at the runbook for that service a runbook is a compilation of routine procedures and operations that the system administrator or operator carries out Wikipedia This is when someone runs the command runbook site router or whatever the affected service is and well see the following How does OpsBot work When creating an incident channel OpsBot will link back to the channel that reported the incident as well as linking to the specific notification in that channel This works because OpsBots logic is trying to match one of two possible patterns within the messages body 1 The phrase ISSUE begins the message see example from earlier 2 The message is identified as a automated NAGIOS CRITICAL notification If the message is identified as an incident trigger then thats what we link to within the incident channel What did we learn Have a prehack document with feature specifications in place Slackbots are fun to create The Google API was kinda tricky a PITA What could we improve Two issues cropped up fairly early on in the design 1 Identify who not to autoinvite to a private incident 2 Identify better nagios incident message grouping logic The first issue is an awkward one because we cant necessarily stop people from accessing a private incident channel if theyve gone into a public slack channel and used the emoji reaction on a particular incident That being said we very rarely have to create private incident channels Thats only when an incident relates to some serious security vulnerability and nearly all the time those types of issues are raised via HackerOne and dealt with outside of typical monitoring notifications like 5xxs The second incident occurs when people add an emoji to a NAGIOS automated notification but then a repeat notification message is sent later In this scenario a message pings to say there is an issue people add the emoji reaction but later on before an incident channel has been created an updated NAGIOS error notification is sent Now when we create the incident channel itll look for the first NAGIOS warning that matches and itll find no emoji reactions so it wont autoinvite people to the incident channel Although this isnt the end of the world as people can still see the incident channel link generated if its a public channel and click on that to access the incident channel What else could OpsBot do OpsBot has the potential to do lots of things it just depends on your needs and use cases For us a few things we planned to do but never quite got round to was A postmortem command for automatically creating our incident Post Mortem documents A service command for looking up the oncall team leads for that service Add runbook info to our deployment platform so OpsBot can pull it in automagically A damn good refactor and some tests this was a hack after all this might materialise into another project we have for improving team relations called WhoWhatWhy coming soon So thats OpsBot If youre interested the presentation slides for the OpsBot hack can be found herepdfshackweek2017opsbotpdf "},{"title":"Post Mortems","tags":["leads","management","post-mortems"],"href":"/posts/post-mortem-template","content":" Note for those short on time heres the PostMortem Templatehttpsdocsgooglecomdocumentd1rYERE1LaobML3puIa94jJSh2cQ0kHflCk8zmVo3pWq0edituspsharing What is a postmortemwhatisapostmortem How do we make postmortems effectivehowdowemakepostmortemseffective Schedule the meeting at the right timeschedulethemeetingattherighttime Invite the right peopleinvitetherightpeople Have the right attitudehavetherightattitude Talk about the right thingstalkabouttherightthings Know the right followup actions to takeknowtherightfollowupactionstotake Notify the right peoplenotifytherightpeople A template for successatemplateforsuccess Observationsobservations Symptomssymptoms Hypothesishypothesis Remediationremediation Impactimpact Key Pointskeypoints Participantsparticipants Timelinetimeline Additional Detailsadditionaldetails Communicationcommunication Imagesimages Taskstasks Questionsquestions What we did wellwhatwedidwell What we didnt do so wellwhatwedidntdosowell What can we learn from in futurewhatcanwelearnfrominfuture Template Documenttemplatedocument Conclusionconclusion What is a postmortem When you have a service outage or any form of unexpected service disruption you first resolve the issue and then proceed to discuss what happened why and how The process of discussion is referred to as a postmortem How do we make postmortems effective You shouldnt just get a bunch of people together in a room to discuss an incident You need to Schedule the meeting at the right time Invite the right people Have the right attitude Talk about the right things Know the right followup actions to take Notify the right people Lets go over each of these items briefly they might sound obvious but Ive seen each of them carried out in vastly different ways hint not all of them good Schedule the meeting at the right time You should schedule the postmortem to occur as soon as possible after the event DO NOT have this discussion two weeks after the incident because people will forget important details Have this discussion while everything is still fresh in peoples minds The trouble with getting a postmortem scheduled at the right time is that youll need a prepared document a postmortem template that is filled in with as much detail as can be reasonably recalled from memory almost immediately after the incident This document doesnt just magically appear its someones responsibility to create it this is harder than you imagine even with modern day communication tools like Slackhttpsslackcom there are so many things going on at once multiple people chatting and trying to isolate the problem then finding a quick and safe solution that its easy to forget things or not notice them happening hence a postmortem helps bring all this information together You should have the postmortem document filled in as much as you can before having the postmortem as a way to help drive the conversation and ideally whoever was oncall at the time of the incident would be marked as the pointperson for prepping this postmortem document One last useful point is to make sure all those who need to be involved in the postmortem are reminded of the meeting a day before This allows them to read through the postmortem document and prep accordingly and generally makes for a smoother discussion Invite the right people This is a tricky one to get right and will vary depending on your organisational structure and even how the incident itself unfolded You shouldnt invite only those people involved during the incident because there are insights that can be gained from Product Managers just as an example who might not have been online at the time of the incident and who might be able to elucidate certain aspects that the engineerssupport team were not aware of You also dont necessarily want to invite too many people outside of those involved in the incident As the old saying goes too many cooks spoil the broth You could find the noise to signal ratio goes up Have the right attitude Postmortems should be blameless Do not come into the meeting with an axe to grind People make mistakes were dealing with software and oftentimes complex distributed systems so we should have an attitude of support understanding patience and a willingness to want to genuinely improve things Talk about the right things This is where the postmortem template comes in Ive linked to it at the top and bottom of this post as it includes different topics that can help steer the conversation in the right direction Things like observations symptoms hypothesis remediation impact etc You should also identify who the meeting moderator is the person responsible for keeping the meeting on track and not falling into a war of words and also who the notetaker is they cant be the same person Its also probably worth mentioning that you shouldnt be having a 3 hour meeting of any kind let alone a postmortem so make sure the short time you have together is yielding the right feedback and information Know the right followup actions to take Have real actionable tasks as takeways from the postmortem You dont want to just discuss how things could be improved you want to actually improve them Its very important you take this opportunity to identity things you can do to prevent this issue from occuring again If service downtimedisruption is important to your business and lets face it when is it not then you need to take incidents seriously and put the time and effort into ensuring stability for the future Once you have the postmortem document filled in fully reviewed and all takeway tasks have been actioned or at least scheduled to be actioned in the near future then you can finish up this whole process by sharing what you learnt with your colleagues who were not involved which leads us onto the next point Notify the right people We generally take the finished postmortem document and email it around to our development mailing list so that all engineers in the organsation get to see what happened why and how it was resolved This is super important because there are so many benefits that can come from this sharing of knowledge Probably top of that list would be that it supports the notion that your organisation respects accountability and that its honest about mistakes that are made It highlights to others not just tech and engineering that these mistakes arent punished but treated as opportunities for growth and learning As well as exposing engineers of varying skill levels to different architectures systems and services that are in place and helps them not only understand them a little better but encourages them to investigate more Because of all that and more who you share the postmortem document with really does depend on you and your companys valuesstandards A template for success Below I link to a Google document we use as a template for our post mortem meetings and Ive included the relevant sections below just for an at a glance view Note you dont have to include all of these if you dont want Take what is useful to you and leave whatever isnt Observations What empircal things did people see eg System X was Y which indicated Z Symptoms What was the physical outcome the user experience of these observations eg Users were seeing X output Hypothesis What do we believe to be the contributing factors Note theres typically never a single root cause Remediation What did we do to resolve the incident Note this isnt the same as fixing the problem which suggests something more long term was put in place eg We did X to resolve the problem We also did Y to resolve the problem Impact What was the cost of this incident Note this isnt the same thing as the symptoms eg Thing A broke for N hours Thing B broke and all entered data for User C during the incident was lost Key Points Theres lots of interactions during the incident what were the most important things that happened eg Person A identified incident at N time Person B notified stakeholders at N time Person C ramped up resources at N time Participants Who was involved and what are their roles in the company Note you dont know everyone and what they do so make it easier for people reading the postmortem to understand the breadth of skills involved eg Jane Doe Engineer Joe Bloggs Support Staff Timeline Describe what happened and when This should be much more detail than those extracted for the key points section Note if you work for a distributed company then identifying the timezone of the incident or at least the timezone youre reporting it from can help clarify when these things happened eg Timezone BST 20181222 Saturday 0300 Joan said a thing happened 0310 Joe notified the stakeholders Additional Details Not everything said is going to have happened within the incident channel you were looking at during the incident maybe it happened in an email thread between support staff product and other stakeholders This is something that is likely to be fleshed out during the postmortem itself as more people who were having those conversations give their perspective eg Bilbo Baggins Product Manager said something useful that not everyone would have seen Communication Where was the war room ie the incident channel where everyone was gathered to help problem solve eg someslackchannel Link to Logs Link to Dashboards Images If you have any screen shots of the broken system then thats useful for people not involved to understand the impact more visually But also linking to a specific point in time of a graph that shows a spike and hopefully dip later in errors can also be useful to reference back to Tasks Put together a list of tasks for people to complete Doesnt have to be done at the time of sending out the postmortem document to the wider organisation but ideally youd have those things done before you sent it out Jane responsible for doing this task Another task that has no specific person assigned to it Confirm to Bilbo that weve done everything we can Questions During the postmortem questions will be raised and they should be placed here Youre not necessarily going to have all the answers during that meeting Ideally youd have answers tied to this questions before you sent out the postmortem document to the wider organisation though What we did well A list of things the team did well during the incident eg The incident was identified quickly The solution was quickly rolled out No one panicked Cross team communication was exceptional What we didnt do so well A list of things the team didnt do so well during the incident eg It took a long time to understand what the alarm meant and who was affected The oncall person didnt acknowledge the initial alarm and so it kept firing We didnt setup an incident slack channel so conversations were happening everywhere What can we learn from in future A list of the things we should proactively improve upon Generally this will be resolutions to the things that didnt go well Template Document View the PostMortem Templatehttpsdocsgooglecomdocumentd1rYERE1LaobML3puIa94jJSh2cQ0kHflCk8zmVo3pWq0edituspsharing Conclusion Let me know what you think on twitter If you have any improvements or you think I have things totally wrong then Id love to hear about it "},{"title":"Practical Monitoring","href":"/posts/practical-monitoring","content":" "},{"title":"Profiling Go","tags":["bash","go","profiling"],"href":"/posts/profiling-go","content":" Memory Management1 Types of Profiling2 Tools Matrix21 Analysis Steps3 Base Example4 ReadMemStats5 Pprof6 Trace7 Conclusion8 Memory Management Before we dive into the techniques and tools available for profiling Go applications we should first understand a little bit about its memory model as this can help us to understand what it is were seeing in relation to memory consumption Gos implementation is a parallel markandsweep garbage collectorhttpwikic2comMarkAndSweep In the traditional markandsweep model the garbage collector would stop the program from running ie stop the world while it detects unreachable objects and again while it clears them ie deallocates the memory This is to prevent complications where the running program could end up moving references around during the identificationcleanup phase This would also cause latency and other issues for users of the program while the GC ran With Go the GC is executed concurrentlyhttpsbloggolangorggo15gc so users dont notice pauses or delays even though the GC is running Types of Profiling There are a couple of approaches available to us for monitoring performance Timers useful for benchmarking as well as comparing before and after fixes Profilers useful for highlevel verification Tools Matrix ProsCons "},{"title":"Profiling Python","tags":["bash","profiling","python"],"href":"/posts/profiling-python","content":" Memory Management1 Types of Profiling2 Tools Matrix21 Analysis Steps3 Base Example4 Timer5 Builtin module timeit6 Builtin module profiler7 Line Profiler8 Basic Memory Profiler9 Tracemalloc10 PyFlame Flame Graphs11 Conclusion12 Memory Management Before we dive into the techniques and tools available for profiling Python applications we should first understand a little bit about its memory model as this can help us to understand what it is were seeing in relation to memory consumption Python manages memory using reference counting semantics What this means is every time an object is referenced either by a variable assignment or similar the counter for that object is incremented Once an object is not referenced anymore its memory is deallocated its counter is decremented every time a reference is removed until it reaches zero But as long as there is a reference somewhere in the program then the object will not be deallocated as the internal counter will be greater than zero Now this can cause problemshttpengineeringhearsaysocialcom20130616circularreferencesinpython when dealing with cyclical referenceshttpsstackoverflowcomquestions9910774whatisareferencecycleinpython so thats something to be aware of when investigating memory leaks and other memory related concerns Note for lots of details of how Python allocates memory I highly recommend this presentationhttpsdmalcolmfedorapeopleorgpresentationsPyConUS2011MemoryUsagepdf Types of Profiling There are a couple of approaches available to us for monitoring performance Timers useful for benchmarking as well as comparing before and after fixes Profilers useful for highlevel verification Tools Matrix ProsCons "},{"title":"Project Management in Five Minutes","tags":["leads","management"],"href":"/posts/project-management-in-five-minutes","content":" Introduction If youre a technical lead aka engineering lead lead developer etc then there comes a time where youll be put in charge of a project You may also already be quite familiar with the process read red tape of certain types of project management agile scrum lean etc So what do you do follow one of those methodologies to the letter and have to suffer all the process that comes along with it You could do For me I find the following things are good enough to get you by without having to worry about whether youre doing scrum or whatever correctly I suggest you take what you want and leave the rest But first lets take a moment to recognise and understand a few distinct roles that otherwise can appear to have a lot of crossover responsibilities this has been summarised from the great work originally published by Lara Hoganhttpsmediumcomlarahogan I highly recommend you check out her work What Who How Why Product Manager owns the story of what Engineering Manager owns the story of who Engineering Lead Tech Lead owns the story of how All Three each person shares ownership of the story of why team leader venn diagramimagesteamleadervenndiagrampng More detailshttpsmediumcommakingmeetupemelpmvenndiagram764e79b42baf Lara Hogans original post Project Management Checklist This is a short checklist of things you probably should be doing Understand the requirements and specifically why they are important If you dont understand the project values then youll have a hard time building something that benefits your users Does the project even align with your teams mission statementresponsibilities You should be focused on building things that bring value to your stakeholders Create simplehighlevel Gherkin user storiesuserstories This is one way of helping you better understand the product values and standards there are other ways but this is what I like using Break down the requirements Define milestones and manageable sub tasks inc investigation time QA Quality Assurance and security testing Prioritise tasks You can do this either by importanceimpact High Medium Low or using MSC Must Should Could Once categorized group related items eg group together all high medium low items A table matrix can help visualise the various tasks and their importance Regardless of the approach you choose you wont be able to prioritize using just a problemsolutionimpact approach Youll need to document and consider both cost and risk as part of your assessment Here is an example documenthttpsdocsgooglecomdocumentd1Qd5wrcTLuwQFIUvHgpvYUC889tk1XLnFsRRbkjvTYpUedituspsharing you can use Work through the unknowns Do this over and over until there is no more value to be gained in spending time on them Figure out who and which teams need to be consulted Then communicate as often as practical andor relevant Run the project and adjust the plan as you go How far has the project come How far is it from completion Dont work faster and or harder when you discover the team is missing deadlines eg its ok I know we missed this deadlinemilestone but were sooooo close lets crank it up and get it done STOP take a breath and understand why the deadline was missed Were there any trending patterns that are likely to repeat themselves Track changes to requirements Be clear about the cost of those changes How do these changes affect the completion Should we cut some existing features in order to accommodate the new work Ensure observability and monitoring is in place Never release a product you cant track out in the wild Dont forget your oncall process Ensure monitors are in place Define a rollout plan and what the roll back steps look like What systems need to be integrated with Notify appropriate oncall and support teams Rollback plan can be a spreadsheet word doc doesnt matter just have one Retro What went well what didnt what could we do differently in future Communicate with radical candor care personally challenge directly CELEBRATE Doesnt matter how big or small the project or whether it was difficult at times Always always celebrate the finishing line and thank your team for their hard work Shielding Dont shield the team from issues that are arising around them eg dont think oh theyre stressed and missing deadlines so I need to pretend like everything is ok its not ok Treat the team with respect as adults and let them know things arent working and that we need to adjust the process to resolve that Yes shield the team from distractions but that is something altogether different than shielding them from bombs exploding around them They can help you negotiate those mindfields if you let them Were working with adults and they dont need to have scary things hidden from them Positive Mindset Ive found that swapping the word problem for challenge a good thing to do in general whether it be talking about an actual technical challenge or discussing a challenging interaction with another employee The subtle switch in language helps me refocus on a more positive and motivated projectory rather than setting myself up to be in a negative mindset for the conversation Recognising Trends As time goes on you might start having issues with your team for various reasons were humans were notoriously difficult creatures When reporting to your line manager eg 11s where you discuss things that are on your mind it can be difficult sometimes to voice concerns without explicit examples Depending on the situation explicit examples arent always possible to recall In those cases where you have a niggling feeling something isnt quite right but you couldnt point to an exact moment in time where an incident occurred then being able to see a trend of something negative happening can help you to raise it up to leadership Be aware of trends in people otherwise you might find yourself in a bad situation and not sure how or why you got there in the first place If you catch problems early enough you can help work towards a solution that gets your team back on track User Stories In case youre not familiar with user stories Gherkin is plaintext with a little extra structure and is designed to be easy to learn by nonprogrammers yet structured enough to allow concise description of examples to illustrate business rules in most realworld domains httpscucumberiodocsreferencehttpscucumberiodocsreference I find its good for documentation but it can also be helpful to some teams to use these user stories as a foundation for their own integration testing systems although I personally wouldnt I prefer just using them as a simple reference for what it is we want to achieve at a highlevel Below is an example feature broken down into various scenarios Feature User Authentication optional description about this feature here Scenario authenticated user requesting a page Given I am a BuzzFeed user internal or external And Im already signed in When I visit wwwbuzzfeedcom or wwwbuzzfeedcompost Then I am directed to my destination page Scenario unauthenticated user requesting a page Given I am a BuzzFeed user internal or external And Im not signed in When I visit wwwbuzzfeedcom or wwwbuzzfeedcompost Then I am able to login with Examples method Facebook Twitter Google UsernamePassword Scenario unauthenticated user successful login Given I am a BuzzFeed user internal or external And I provide valid credentials When I attempt to login Then I am directed to my destination page Scenario unauthenticated user failed login Given I am a BuzzFeed user internal or external And I provide invalid credentials When I attempt to login Then I am presented with a login error Scenario unauthenticated user signup Given I am a BuzzFeed user internal or external And I am not already registered in the system When I visit wwwbuzzfeedcomcms Then I am directed to a legacy signup flow "},{"title":"Python Code Design and Dependency Management","tags":["asyncio","python"],"href":"/posts/python-code-design","content":" Introduction This post is going to cover a few tools and features I plan on using when writing Python code in 2019 Ive grouped these into the following sections Dependency Managementdependencymanagement Type Hints and Static Analysistypehintsandstaticanalysis Interfaces Protocols and Abstract Methodsinterfacesprotocolsandabstractmethods Note if you want to learn the basics of Python then I recommend reading Python for Programmershttpsleanpubcompythonforprogrammers Dependency Management Python has historically utilised a requirementstxt file for defining the dependencies required of your program but there are varioushttpsmediumcomknerdtheninecirclesofpythondependencyhell481d53e3e025 annoying complicationshttpsrealpythoncompipenvguidedependencymanagementwithrequirementstxt that go along with the traditional model of handling dependencies which has meant we have a few new players in the field to help us One such concern is the setting up of multiple virtual environments for the various projects we need to work on XKCD right as always So here are the various alternatives we have to play with in 2019 Hatchhttpsgithubcomofekhatch Poetryhttpsgithubcomsdispaterpoetry Pipenvhttpsgithubcompypapipenv Ill be showing you the last tool in the list Pipenv Although another alternative approach to the specific problem of virtual environments is to utilise docker containers for doing your development but youll need to be comfortable using a terminal editor like Vim unless you want to jump through some X11 hoops Using containers also doesnt eliminate the other issues with determining the right dependencies so keep reading anyway Note if using Docker with a terminal editor like Vim to solve this problem sounds like a good approach for you then review an older post of mine that explains how to do thatpostsdevenvironmentswithindockercontainers Here are the commands necessary to install Pipenv on macOS brew install pyenv pip install pipenv Note youll need Homebrewhttpsbrewsh to install the pyenvhttpsgithubcompyenvpyenv command a sub dependency using brew and macOS should have Python 27x installed by default so you should have the pip command available already Here are my quick steps for setting up a new project with Pipenv mkdir foobar cd foobar pipenv python 37 Note use pyenv install list to find out what Python versions are available to install Now when working on a Pipenv project cd foobar pipenv shell or pipenv run python apppy Note use the shell subcommand to have your current terminal permanently use the chosen Python version eg python apppy will work as if the current Python version is what youve defined otherwise use the run subcommand to execute the given command eg python apppy within the chosen Python version temporarily You can now install dependencies specifically for the projects specific Python environment pipenv install tornado502 pipenv install dev mypy tox flake8 Note if you have an existing requirementstxt file then you can generate a Pipfile from that using pipenv install r requirementstxt alternatively if you need to do the reverse generate a requirements from a Pipfile pipenv lock requirements Now none of these new tools are perfect and if you want a good run down of one engineers perspective on them read herehttpschriswarrickcomblog20180717pipenvpromisesalotdeliversverylittle Type Hints and Static Analysis Some languages are weakly typed JavaScript some are strongly typed Python and some are statically typed Go Rust Being strongly typed means you cant perform operations inappropriate to the type For example in Python you cant add a number typed variable with a string typed variable In Python 35 we get type hints which are a way of annotating Python code with type information in a bid to allow for external tools to provide safety similar to what you might see with a statically typed language Python actually ignores these annotations so type hints wont break your code if you specify one expected type but provide a different one at runtime Consider the following code snippet which uses type hint annotations to indicate the types for both a functions parameters as well as the functions return value def foon int str printfinteger n return n foonot an integer prints integer not an integer So we can see that the code is stating we expect our foo function to receive a parameter of type integer but when the function is called its actually passed a string Youll find this incorrect argument type isnt going to break your program Our example code also expects a string to be returned by the foo function but we return an integer value So this also doesnt break our program If you did nothing else at this point your code would at the very least be very descriptive of the expectations for its use but what these type hints afford us is the ability to use external tools for handling static analysis The most popular tool currently is called mypyhttpmypylangorg You can run mypy via the command line and it also provides integrated support for most code editors This means you can catch yourself breaking the expectations of your program when writingediting code Type hints by themselves are quite basic and dont offer much additional contextual information so Python added a new typing modulehttpsdocspythonorg3librarytypinghtml to allow for more contextual annotations One nice feature provided by this typing module is the ability to alias types The following example is copied verbatim from the Python documentation from typing import Dict Tuple List ConnectionOptions Dictstr str Address Tuplestr int Server TupleAddress ConnectionOptions def broadcastmessagemessage str servers ListServer None The static type checker will treat the previous type signature as being exactly equivalent to this one def broadcastmessage message str servers ListTupleTuplestr int Dictstr str None I intend to use type hints a lot more in 2019 to help me both identify potential issues in my code during development as well as being a means of code clarity as to what inputoutput types are expected For more examples of using the typing module please refer to either the official documentation or this useful tutspluscom articlehttpscodetutspluscomtutorialspython3typehintsandstaticanalysiscms25731 Interfaces Protocols and Abstract Methods In the field of programming there are two concepts that can be a bit confusing to understand 1 interfaces 2 abstract classesmethods Well cover both of these in the following sections along with Pythons own protocols and abstract base classes In summary an interface is a contract that defines behaviour but has no implementation An abstract class is an actual class that can define common behaviour including its implementation along with abstract methods that have no implementation The implementation of the abstract methods will be defined by the subclass Interfaces An interface is useful for when you dont necessarily care for a specific concrete implementation of some functionality and will happily accept any object as long as it abides by the behavioural contract your receiver requires For the specific use case of interfaces Python has traditionally relied on duck typing which is where a caller provides an object and the receiver will attempt to call the appropriate method on the object thus trusting the object has a corresponding method available Note this is where the term duck typing comes from If it walks like a duck and it quacks like a duck then it must be a duck If the provided object has the expected method exposed then we presume its of a suitable type In some cases such as a reusable shared library it might be preferable to write code defensibly Meaning you verify the provided object has the interface the receiver is expecting This is demonstrated in the following example which expects a client object to be provided and that object needs to be a HTTP client and so must have both a get and a post method We could trust the caller has read the documentation and provided an appropriate object but we dont want to rely on that so we manually defend against that failure scenario in our code async def executefetchclient endpoint Make asynchronous requests via given http client if invalidclientinterfaceclient raise tornadowebHTTPError500 reasonInvalid HTTP Client def invalidclientinterfaceclient Ensure http client has supported interface if hasattrclient post and hasattrclient get return False return True Python not being a statically typed language means it has no support for traditional interfaces but the above example defensive code is a way to manually mimic it at runtime as we have no means to validate this at any other time as there is no compilation step with Python being its a dynamic language Protocols Python provides protocolshttpsdocspythonorg37librarycollectionsabchtmlmodulecollectionsabc as part of their collections module and are homed alongside their abstract base classes which well also look at shortly Protocols are similar in spirit to interfaces in other languages but in practice act more like guidelines If one of your custom defined objects implements specific magic methodshttpsrszalskigithubiomagicmethods eg len del etc then youll find a selection of builtin Python functions become available to use on those objects that otherwise those builtin functions wouldnt necessarily support For example if we implement the len magic method then our object will be able to utilise the builtin len function Also once we have protocols in place we can use mypy along with type hinting to implement a development time interface check Consider the following code snippet class Team def initself members selfmembers members t Teamfoo bar baz tmembers foo bar baz lent TypeError object of type Team has no len This code doesnt work because the len function provided by the Python standard library doesnt work on custom classes unless the class defines a len magic method In doing so see the following example the Team class is now supporting the collectionsabcSized protocolhttpsdocspythonorg37librarycollectionsabchtmlcollectionsabcSized and so the len function will be able to work when given an instance of Team class Team def initself members selfmembers members def lenself return lenselfmembers t Teamfoo bar baz tmembers foo bar baz lent 3 If we want to utilise mypy to help verifying our code at development time lets say we want a function to accept anything that we can use the len function on ie anything that supports the collectionsabcSized protocol then we can do so using the typingSized type import typing class Team def initself members selfmembers members def lenself return lenselfmembers t Teamfoo bar baz def printsizes typingSized printlens printsizet Notice that in the above example we state that the first argument to printsize should be a type of the typingSized which is actually a mapping to the collectionsabcSized protocol The Python typing module also lets you define your own protocols using typingNewType In the following example we create a new custom protocol called CustomProtocol import typing class Team def initself members selfmembers members def lenself return lenselfmembers t Teamfoo bar baz def printsizes typingSized printlens printsizet prints 3 CustomProtocol typingNewTypeCustomProtocol Team cp CustomProtocolTeambeep boop printsizecp prints 2 Note the first argument to typingNewType needs to match the name of the variable it is assigned to Also when we create an instance of CustomProtocol the underlying type is Team The mypy static analysis tool can subsequently be used to verify code for both native protocols and custom protocols like so see the type hint annotation added to the printsize function which mypy is happy with class Team def initself members selfmembers members def lenself return lenselfmembers CustomProtocol typingNewTypeCustomProtocol Team def printsizes CustomProtocol use could also set type to Team printlens prints 2 cp CustomProtocolTeambeep boop printsizecp Note the argument type passed to printsize is CustomProtocol which doesnt make mypy complain because the underlying type for CustomProtocol is actually the Team class and the underlying Team class is supporting the typingSized interface which maps to the collectionsabcSized protocol If you want more information on mypys support of protocols I suggest reading their specific documentation herehttpsmypyreadthedocsioenlatestprotocolshtml Abstract ClassesMethods An abstract class allows you to define common behaviour as well as abstract methods that have no implementation in which the subclass will be required to provide the implementation We can mimic that concept in Python using standard classes along with the classic template method patternhttpsenwikipediaorgwikiTemplatemethodpattern as shown in the following example class MyAbstractClass def commonself printcommon behaviour def MyAbstractMethodself raise NotImplementedError class FooMyAbstractClass def MyAbstractMethodself printdo something class BarMyAbstractClass pass f Foo fcommon prints common behaviour fMyAbstractMethod prints do something b Bar bcommon prints common behaviour bMyAbstractMethod raises NotImplementedError o MyAbstractClass not possible in other languages see note below ocommon prints common behaviour oMyAbstractMethod raises NotImplementedError Note in other languages that support proper abstract classes you would not be able to instantiate the abstract class directly like we have done in our example Luckily Python does also provide us with what it refers to as Abstract Base Classes here in referred to as ABCs which are a form of traditional abstract class so theres no need to necessarily mimic the behaviour like in our earlier example See the following example that demonstrates this feature import abc class FooabcABC abcabstractmethod def barself pass class ThingFoo pass t Thing TypeError Cant instantiate abstract class Thing with abstract methods bar To make the above example code work correctly we need our class Thing to actually implement the exepected behaviour ie a bar method If Thing doesnt provide the expected behaviour then we cant instantiate a subclass of Foo This also means that if were using a static analysis tool such as mypy we could have a receiver state it expects a type of Thing and know more confidently that Thing will definitely provide the behaviour we need Its important to understand that the use of an abstract class is subtly different to the use of traditional interfaces in that an interface doesnt rely on a concrete implemention For example our Thing class is a concrete implementation and so we cant provide the receiver with a different class even if the other class also happened to inherit from Foo as it wont be equivalent to a Thing type Note the mypy docs have a good detailed breakdownhttpsmypyreadthedocsioenlatestkindsoftypeshtmlthetypeofclassobjects of how to indicate a dependency of a specific class type Conclusion Thats it Weve looked at how to handle dependencies with Pipenv and how to utilise static analysis tool mypy along with type hinting to give us more confidence in our code as well as having the code become clearer intent Lastly we looked at how to utilise interfaces and abstract classes to help improve the structure and safety of our code Along with new additions to the asyncio module a simpler api for a start and cleaner abstractions such as the new data classeshttpsrealpythoncompythondataclasses features the future of Python hasnt looked brighter "},{"title":"Security with Python","tags":["encryption","profiling","python","security"],"href":"/posts/python-security","content":" Introduction1 KDF or PBKDF22 generatedigest3 decryptdigest and validatedigest4 Dependencies5 Usage6 Tests7 Implementation8 Conclusion9 Introduction I recently implemented a Python library which acts as an abstraction layer on top of an existing security algorithm in this case scrypthttpswwwtarsnapcomscrypthtml The motivation was for allowing teams to have a consistent experience utilising encryption and hashing in their applications and services without necessarily having to know the insandouts of whats important with regards to salts key lengths etc Note I always encourage people to understand what it is theyre doing but in some cases thats not always a practical mindset The library provides three functions 1 generatedigest 2 decryptdigest 3 validatedigest KDF or PBKDF2 Before we start looking at the three functions provided by this libraryinterface lets very briefly talk about KDF and PBKDF2 A KDFhttpsenwikipediaorgwikiKeyderivationfunction Key Derivation Function accepts a message a key and produces a digest for its output They are designed to be more computationally intensive than standard hashing functions and so they make it harder to use dictionary or rainbow table style attacks as they would require a lot of extra memory resources and become more unfeasible as an attack vector By default the KDF will generate a random salt thus output is nondeterministic and have a maximum computational time of 05 although this can be overridden using a maxtime argument as well see later A PBKDF2httpsenwikipediaorgwikiPBKDF2 on the other hand is able to provide deterministic output as well as the ability to specify an explicit salt value The internal implementation will repeat its process multiple times thus reducing the feasibility of automated password cracking attempts similar to a KDF I mention both of these KDF and PBKDF2 because the generatedigest function Ive written is a multiarity function that will switch implementation based upon the provided arguments in the method signature Originally I had two separate functions to distinguish them a bit more clearly but realised if this library is to make life easier for developers who dont understand encryption or hashing concepts then I need to provide a single function that intelligently handles things internally Because KDF accepts a key and is able to return the original message given the same key its acting as a form of symmetrical encryption whereas a PBKDF2 is more like a oneway hash function Hence I named the function in this library generatedigest rather than something like encryptmessage which wouldnt have made sense when dealing with PBKDF2 generatedigest This is a multiarity function that will generate a digest using either a passwordbased key derivation function KDFhttpsenwikipediaorgwikiKeyderivationfunction or a PBKDF2httpsenwikipediaorgwikiPBKDF2 depending on the input given If a password argument is provided then KDF will be used along with a random salt to generate a nondeterministic digest If a salt is provided then a PBKDF2 will be used to generate a deterministic digest Note salts should be a minimum of 128bits 16 characters in length Also when specifying a maxtime with generatedigest ensure you include that same value when decrypting with decryptdigest or validating via validatedigest decryptdigest and validatedigest The decryptdigest and validatedigest functions only apply to digests that have been generated using a password ie KDF Given the right password decryptdigest will return the original message and thus is considered more a form of symmetrical encryption than a straight oneway hash function The validatedigest function will return a boolean true or false if the given password was able to decrypt the message Dependencies This abstraction library requires scrypt which itself requires the following dependencies to be installed within the context of your service buildessential libssldev and pythondev If your service has a Dockerfile adding these dependencies should be as simple as adding a line like the following RUN aptget update aptget install y buildessential libssldev pythondev Usage I suggest looking at the test suite see below to get an idea of how you would use the functions in this library Note for a glossary of security terms refer to this documenthttpsdocsgooglecomdocumentd1qs3jEIQvocdVhSxCSPLF1BoLnp91aLnuUIasvlmaYoedituspsharing Tests Before we look at the implementation of the library lets take a moment to sift through its test suite Note I named the library secure and have it running on a private PyPy instance This code is made available via GitHubhttpsgithubcomIntegralistPythonEncryption import pytest from secureinterface import ArgumentError generatedigest validatedigest decryptdigest message mymessage password mypassword salt mysaltislongenough def testgeneratedigestwithbothapasswordandasalt Providing both a password and a salt should raise an exception with pytestraisesArgumentError generatedigestmessage saltsalt passwordpassword def testgeneratedigestwithapassword Generating a digest with a password should be nondeterministic digest1 generatedigestmessage passwordpassword digest2 generatedigestmessage passwordpassword digest3 generatedigestmessage passwordpassword maxtime15 digest4 generatedigestmessage passwordpassword maxtime15 digest5 generatedigestmessage passwordpassword maxtimeint1 digest6 generatedigestmessage passwordpassword maxtimeint1 assert digest1 digest2 assert digest3 digest4 assert digest5 digest6 def testgeneratedigestwithoutapassword Generating a digest without a password should be deterministic digest1 generatedigestmessage digest2 generatedigestmessage digest3 generatedigestmessage saltsalt digest4 generatedigestmessage saltsalt digest5 generatedigestmessage length128 digest6 generatedigestmessage length128 assert digest1 digest2 assert digest3 digest4 assert lendigest5 lendigest6 def testgeneratedigestwithdifferentsaltlengths Salts should be at least 128bits 16 characters in length generatedigestmessage saltsalt with pytestraisesArgumentError generatedigestmessage salttooshort def testvalidatedigest Validation only applies to digests generated with a password digest1 generatedigestmessage passwordpassword digest2 generatedigestmessage passwordpassword digest3 generatedigestmessage passwordpassword maxtime15 digest4 generatedigestmessage passwordpassword maxtime15 digest5 generatedigestmessage passwordpassword maxtimeint1 digest6 generatedigestmessage passwordpassword maxtimeint1 assert not validatedigestdigest1 incorrectpassword assert validatedigestdigest1 password assert validatedigestdigest3 password maxtime15 assert validatedigestdigest5 password maxtimeint1 def testdecryptdigest Decryption is possible given the right password digest generatedigestmessage passwordpassword assert decryptdigestdigest password message Implementation OK time to see the library code itself Note I like to use MyPyhttpmypylangorg for type hinting import scrypt from typing import Union class ArgumentErrorException pass def generatedigestmessage str password str None maxtime Unionfloat int 05 salt str length int 64 bytes Multiarity function for generating a digest Use KDF symmetric encryption given a password Use deterministic hash function given a salt or lack of password if password and salt raise ArgumentErroronly provide a password or a salt not both if salt and lensalt bytes Decrypts digest using given password return scryptdecryptdigest password maxtime def validatedigestdigest bytes password str maxtime Unionfloat int 05 bool Validate digest using given password try scryptdecryptdigest password maxtime return True except scrypterror return False Conclusion Let me know what you think on twitter Have fun "},{"title":"Interview Topics","tags":["goals","interviews","jobs"],"href":"/posts/questions-when-interviewing","content":" Introduction I was approached recently by an organisation about whether Id be interested in working for them The focus of this post isnt that company hence Ive not mentioned their name but more about a set of experiences Ive had when changing jobs and how I think theyre important to keep in mind along with specific topics of interest you should look to include when considering future opportunities What Happened A recruiter lets say they were calling on behalf of the Foo company contacted me to say they and their CTO had stumbled across and enjoyed a selection of my blog posts and found my profile on LinkedIn to fit the type of candidate they were looking for My current work situation I work at a company called BuzzFeedhttpswwwbuzzfeedcom Ive been here for almost two years now and I came to them from the BBChttpwwwbbccouknews Am I happy at BuzzFeed Yes very BuzzFeed is the first company Ive worked for where they Treat me like a human being Treat me fairly Support my need to work almost exclusively remotely Are truly diverse and culturally supportive As far as interviewing BuzzFeed also got brownie points for Paying me for my time interviewing with them Paying for my accommodation and travel to NY I live in the UK Not taking advantage of me one example being they already knew my salary and still increased it by a significant percentage Did you talk to the Foo company Yes Because I was interested to see how they compared How did they compare Well I didnt get past the initial conversation because they didnt have a remote role for me so I couldnt say with 100 certainty how they would have compared But that said there were a few key things I took away from the conversation that I feel are worth mentioning as these things were specifically firing my internal alarm bell They tried to sell me on were all engineers here which for me isnt something we should be aspiring to conversation for another day but to me its an unhealthy attitude There was a seeming lack of postmortems andor blameless culture I was given a response if you can imagine that was similar to a confused raised eye brow The concept of teams functioning differently to each other which is fine seemed to be confused with inconsistency and lack of direction Money focused The biggest takeaway by far was that I was asked constantly What would it take for you to leave BuzzFeed No interest in techniques that could help improve their ability to work more efficiently I realised they cared less about me as a person someone the company could build a long term relationship with and more about me as a number I was just a resource an entity they could acquire What was the problem This company is clearly as they themselves stated entirely made up of engineers with the lightest touch of management Through my discussion it became apparent that building new products with new tech was the focus and drive for those working there But in that type of environment being able to work effectively efficiently and with good communication along with making sure youre working on the right thing and at the right time is rarely the case in my experience For me Im at a point in my career where discussions about programming languages or tech stack X just isnt something I worry about or get giddy about because most of the time its how you solve a problem thats important In most situations if you do your due diligence and exhaust all means available to you to make the given tech work you should be able to empirically prove its not working and thus are in a better position to justify a more appropriate language or tech stack This all becomes fairly trivial Note in my experience people tend to fall back on subjective opinions and hand wavey facts rather than just cracking on with solving the problem at hand Remember were engineers we can solve a lot of these issues in many different ways But I personally find the human problem is something that is typically much harder to solve and ultimately will cause a lot more detrimental effect on our ability to learn progress in and emotionally stay connected to our jobs After my conversation with the Foo company I already had an unsettling feeling about them but decided that I would do some research on Glassdoor to see if my gut feelings stemming from the feedback I received were correlating to other peoples past experiences working at the company its important to realise that with sites like Glassdoor youre only getting half the story so take it with a pinch of salt For me I already had a lengthy conversation with the company and so I felt like I had sufficient information on which to corroborate my experience against Turns out my gut instincts were reciprocated by others and by that I mean engineers either working still or who used to work at Foo had voiced concerns that if you want to get paid lots of money to work on software with lots of tech debt and with management that doesnt care about improving the situation with how teams collaborate andor communicate with each other then company Foo might be the place for you Now what Im about to say I can say because of privilege Im totally aware of that and I try to keep that in check wherever possible For me getting paid is important because ultimately it allows me to look after my family but in truth theres a whole world outside of just getting paid which is equally if not more important for being happy Ive been fortunate enough with my privilege to be able to have that opinion and I appreciate that for others this isnt necessarily going to be the case and so for those of you who dont feel that way because of circumstances thats of course absolutely ok and valid So with that in mind I will cover what I feel are those other important topics next Whats important to you First I just want to take a moment to reiterate that you need to find whats important to you If youre just starting out in your career or your midway through or youre 20 years in like me your priorities your standards and your values are going to be different depending on the stage of life youre at For me in a general sense Im looking for A well paid job to enable me to support my family A stable work environment again family A diverse cultural environment because diversity is important to me To be able to have impact I like helping people To be happy ie not stressed Now lets be honest who of us isnt looking for those qualities Everyone wants a good paying job everyone wants to be happy and in some cases most people prefer stability over chaos Maybe diversity or having an impact isnt high up on your list of things because youre just starting out and to be honest at that stage in life just getting paid is probably going to be your highest priority At any rate figure out your priorities your standards and your values when considering working opportunities But also remember that these things will change over time and thats ok too sometimes we just need stability What should you talk about Well first things first Its a good idea at the start of any conversation to ask Whats the focus of todays conversation ie Whats important to the company or individual youre talking with and what insights do they want to take from this Does it align with your agenda If not clarify what you hope to get out of the conversation so its clear on both sides what intentions there are Otherwise here are the list of questionstopics I like to cover What is the companys story Give me an insight into the companys historybackground and how weve reached this point in time Presence How many offices do you have and what are their locality Culture and Diversity The companys values will help indicate whats important to them Hierarchy and Organisation Structure Are you fairly flat and lean or tall and fragmented Visibility Openness How does leadership share and handle criticalinternal business topics Collaboration Building new features across multiple crossdiscipline teams avoiding duplication Distributed Timezones Community How do internal external staff interact How do teams across officeslocality bond Process Whats your onboarding process How are new features discussed designed evolved released How do you prioritize eg impactvalue for endusers What is the leadership like How is documentation handled here Do they care about documentation Are they sharing information or is there silos of knowledge Do you practice post mortems blameless retrospection handling of failures How do you handle tech debt ie sustainability of your software Communication What do you consider to be good communication Do you work with any of the following practices SBI Situation Behaviour ImpacthttpsgistgithubcomIntegralist24c8a9ce570d78d37ed0cf9967594e0e Methodology The Center for Creative Leadership Radical Candorhttpswwwradicalcandorcom Book Kim Scott Conscious BusinesshttpswwwyoutubecomwatchvIdMvWLARF1w Video Fred Kofman Authentic Communicationhttpswwwsoundstruecomstoreauthenticcommunicationhtml Book Fred Kofman Responsibility and Ownership What are your expectations of me when I start What is your indicator that I was a successful hire What does the responsibilities look like for each of the following types of teams Engineering FeaturesProducts Site Reliability Operations How do teams work with regards to an oncall rota Job Progression Opportunities Learning budgets Conferences Remote Working How are remotes kept feeling included Work life balance and support How do staff balance their worklife How do things work in a multiregiondistributed organisation eg crossover hours between UK and US Tech Finally we get to something engineering related What does the infrastructure look like What build systems do you use What is your deployment platform How often docan you deploy How do you handle rollback processes Vision and Future Where is this company going What are the end goals Conclusion I think that pretty much covers everything of interest To me at least these broad topics will give you a good indicator of what an organisation is like or the potential they have depending on how they answer these questions What are the sorts of things you ask in an interview What matters to you Let me know on twitter "},{"title":"NSQ Queue Reader Best Practices","tags":["nsq","performance","python","queues"],"href":"/posts/queue-reader-best-practices","content":" Introduction1 Ephemeral Channels2 Fail quickly3 Verify your message handling logic4 Be wary of global variables5 Instrument timers around your primary message handler6 Pynsq doesnt support coroutines7 Prevent messages backing up in the queue8 Avoid API libraries autoretrying expensive operations9 Place blocking IO operations into a thread pool10 Rate limit yourself11 Disable yourself12 Drop or Requeue13 Introduction This post should serve as a guide for best practices when dealing with services that consume messages from queues and process those messages we refer to them as QRs or Queue Readers The best practices detailed below are from the perspective of both general programming idioms as well as useful performance patterns We also are focusing primarily on QRs that use the NSQ data pipelinehttpnsqio and specifically for services written in Python although I imagine most of the items discussed could translate well enough to your queue mechanism of choice As with all best practice guidelines they are just that guidelines Not everything listed here will be applicable for your needs So remember to start by verifying your own applications requirements and specific use cases Ephemeral Channels Imagine your server instance needs to be restarted or its nsqd daemonhttpnsqiocomponentsnsqdhtml which receives queues and delivers messages to clients is unexpectedly terminated or maybe the nsqd exceeds the allocated memqueuesize which determines the number of messages that should be kept in memory Normally this would mean messages in the queue would be lost If youre OK with that scenario and its outcome then you should append ephemeral to your channels nsqchannel qrnamegoeshereephemeral Otherwise the default behaviour for NSQ queueshttpnsqiooverviewdesignhtml is to persist messages on disk Which you choose will depend on your application and how critical you feel the messages are Fail quickly When processing a high throughput of messages its beneficial to identify invalid messages quickly then mark them as processed so you can exit your handler as quickly as possible and so not cause undue processing stress on your application andor upstream services You should wrap potentially problematic code in a tryexcept eg a function that makes HTTP requests can have multiple types of exceptions raised Doing this means you can isolate that specific call and handle the failing scenarios appropriately Verify your message handling logic You should understand the complete request flow of your message handling functions and be sure you are correctly dropping andor requeuing messages at the appropriate places within your application code Its very easy to not requeue or drop messages by mistake When processing messages synchronously you typically just return True message was processed or False requeue this message from your handler But in order to process messages asynchronously you need to call nsqmsgenableasync and then youll need to make sure you explicitly return either nsqmsgfinish or nsqmsgrequeue Be wary of global variables Most of the time global variables can be more performant as youre reusing a pointer to some data but there are some cases where a longliving and large global object such as a boto S3 connectionhttpboto3readthedocsioenlatest might end up leaking memory This is something that should be measured and verified using the appropriate Python profiling tools first though Instrument timers around your primary message handler Its important to be able to identify anomalies in the performance of your message handlers By using a decorator to time the function you can set up appropriate dashboards and alarms from yourmetricsabstraction import metrics metricstimedmessagehandlertime async def messagehandlernsqmsg Pynsq doesnt support coroutines The pynsq library only supports a callback form of asynchronous message processinghttpsgithubcomnsqiopynsqissues186 Meaning if you were to define a message handler using a decorator like gencoroutine or a native async syntax either one will convert the function into a coroutine it will end up breaking the QR application by exiting the handler immediately See the next section8 for an example code snippet that works around this issue by utilising Tornados ioloop directly to schedule the handlers asynchronous execution Prevent messages backing up in the queue Messages can build up and cause alarms to fire if they are not pulled from the queue and successfully processed by your application in a timely fashion You can help resolve this by either configuring the nsqReadermaxinflighthttppynsqreadthedocsioenlatestreaderhtml attribute andor processing your messages asynchronously from tornado import ioloop async def corohandlermsg do stuff return msgfinish def handlermsg msgenableasync ioloopIOLoopcurrentaddcallbackcorohandler msg You can also look to tweak the nsqReadermaxtrieshttppynsqreadthedocsioenlatestreaderhtml attribute which defines the number of times a message can be requeued before it is permanently dropped this prevents cyclic errors There is also the nsqMessagetouchhttppynsqreadthedocsioenlatestmessagehtmlnsqMessagetouch method which lets you indicate to the NSQ daemon that you need more time to process the message and thus postpone for a little while at least the message processing from timing out and being automatically requeued depending on the setting of the maxtries attribute Avoid API libraries autoretrying expensive operations Some API libraries such as botohttpboto3readthedocsioenlatest allow you to configure it so that operations are retried N number of times before finally failing This can be helpful to ensure a temporary network blip or error doesnt cause a message to be unnecessarily dropped or requeued But this can also bring a performance overhead if the operation in question is very slow Review the API calls you are making and evaluate how expensive they are In some cases you might prefer to configure retries off and have NSQ handle these temporary errors ie by requeuing messages Below is an example of how to configure boto to not retry operations s3resource sessionresources3 configConfig connecttimeout2 readtimeout2 retriesmaxattempts 0 Note as per the example above its worth tweaking the connectionread timeouts as well For example we noticed that calls for xml files from S3 were really slow and so in that service we had to increase the readconnection by a significant amount but not too much you dont want the client to sit hanging for a long period of time so it requires some fine tuning to get it right Place blocking IO operations into a thread pool Some libraries do not provide asynchronous support such as Pythons redis libraryhttpsredispyreadthedocsioenlatest So if your message handler is asynchronous and youre also executing a potentially long running blocking operation such as an S3 object GET then this will end up causing your application to block the ioloop and prevent concurrently handling multiple messages from appthreadpool import runonexecutor async def messagehandler result await runonexecutorfn arg1 arg2 Then the appthreadpool referenced in the above snippet would look something like from tornado import gen from concurrentfutures import ThreadPoolExecutor from bfrig import settings THREADPOOL ThreadPoolExecutorsettingsgetpoolmaxworkers 10 gencoroutine def runonexecutorargs kwargs result yield THREADPOOLsubmitargs kwargs raise genReturnresult The above example needs to use a Tornado decorator as ThreadPoolExecutor doesnt work with native coroutines It would require the use of asynciowrapfuture which isnt much better than just using Tornados own decorator Note the ThreadPoolExecutor will only help you deal with IO bound tasks that need to be handled asynchronously and whose library doesnt support natively If the task to be executed is actually CPU bound then youll want to utilise a ProcessPoolExecutorhttpsdocspythonorg3libraryconcurrentfutureshtmlprocesspoolexecutor instead Rate limit yourself In a service where theres a potential for lots of duplicate messages it can be useful to implement some simple rate limiting logic In one of our QR services we use Redis to track duplicate requests and then execute some basic rate limiting logic in order to prevent overwhelming any upstream services that would otherwise be called Note be aware that the rate limit you set can cause unwanted sideeffects For example if you start to requeue messages during a rate limit period you may start to see that messages arent being processed quickly enough and so the queue depth will begin to increase ie the queue will start to backup and fill up and this might cause monitors eg systems like DatadogNagios to trigger Disable yourself Consider your upstream services and identify if theres ever a point where your service needs to stop making requests to it Most services will be sat behind an API Gateway so theyll likely enforce rate limiting on you But that might not always be the case One example of this is a QR service which makes requests to a separate rendering service for HTML content to be backed up into AWS S3 There are periods where this rendering service will dynamically purge its cache both its internal application cache and also the outer CDN cache layer In order to prevent the QR service from overloading the rendering service during this period where its vulnerable we automatically disable the QR service we use a shared redis cluster to identify the switch in a key value so we change it from disabled to enabled due to it having no cache none of these services we have are vulnerable in the security sense as theyre internal access only within a VPC The below example demonstrates an implementation used in one of our QR services which was to use a Python decorator from appfoo import toggle togglestatus def messagehandlernsqmsg args kwargs nsqmsgenableasync ioloopIOLoopcurrentaddcallbackmessagehandler nsqmsg The appfoo code then looked something like the following def toggleqrstatuskey str Callable When statuskey is set to stopped in redis this decorator will finish the nsq message and return immediately otherwise it will proceed with event handling Arguments status name of the status key in redis Returns Wrapped message handler def decofunction wrapsfunction def wrappernsqmsg Message args kwargs assert redis in kwargs redis parameter is required redis kwargsredis try status redisgetstatuskey except Exception as e status None if status bstopped nsqmsgfinish return return functionnsqmsg args kwargs return wrapper return deco Drop or Requeue Consider the previous section about disabling a QR service in times where it might be necessary to protect an upstream eg where rate limiting yourself maybe doesnt make sense or being rate limited by the upstream isnt possible you might then need to make a decision about what you do with the messages that are building up in the message queue Those messages will eventually reach a threshold and in some cases it might make more sense to not requeue messages while the QR service is in disable mode but instead just drop them completely The answer and your approach will depend on the message source itself are they messages that you can afford to drop are they generated fairly regularly "},{"title":"Engineering and Tech Recommended Reading List","tags":["books","reading"],"href":"/posts/reading-list","content":" Ive long kept a list of books Ive enjoyed reading and decided I would share them as a blog post so others could benefit from their learnings Note I generally keep the original gisthttpsgistgithubcomIntegralist77877126a0b13766f0de the most uptodate Algorithms Grokking Algorithms An illustrated guidehttpswwwmanningcombooksgrokkingalgorithms Best Practices Clean Code A Handbook of Agile Software CraftsmanshiphttpwwwamazoncoukCleanCodeHandbookSoftwareCraftsmanshipdp0132350882refwlitdpopCnSnCieUTF8colid1KPZ4BNV1OMC4coliidI1GJ3Q25TNFKN1 The Clean Coder A Code of Conduct for Professional ProgrammershttpwwwamazoncoukTheCleanCoderProfessionalProgrammersdp0137081073refwlitdpopCSnCieUTF8colid1KPZ4BNV1OMC4coliidIJUCNB9OWB7H8 ClientSide CSS The Definitive GuidehttpwwwamazoncoukCSSDefinitiveGuideEricMeyerdp0596527330refsr11ieUTF8qid1427038313sr81keywordscssthedefinitiveguide Dont Make Me Think A Common Sense Approach to Web UsabilityhttpwwwamazoncoukDontMakeMeThinkUsabilitydp0321965515refsr11ieUTF8qid1427038352sr81keywordsdontmakemethink Even Faster Web Sites Performance Best Practices for Web DevelopershttpwwwamazoncoukEvenFasterWebSitesPerformancedp0596522304refsr11ieUTF8qid1427038718sr81keywordsevenfasterwebsites High Performance Web Sites Essential Knowledge for FrontEnd EngineershttpwwwamazoncoukHighPerformanceWebSitesEssentialdp0596529309refsr11ieUTF8qid1427038697sr81keywordsHighPerformanceWebsites Mobile Firsthttpabookapartcomproductsmobilefirst Responsible Responsive Designhttpabookapartcomproductsresponsibleresponsivedesign Responsive Web Designhttpabookapartcomproductsresponsivewebdesign Clojure Clojure for the Brave and TruehttpwwwamazoncoukClojureBraveTrueUltimateProgrammerdp1593275919refsr11ieUTF8qid1427038088sr81keywordsclojureforthebraveandtrue Quick Clojurehttpswwwapresscomgbbook9781484229514 The Joy of ClojurehttpwwwamazoncoukJoyClojureMichaelFogusdp1617291412refsr11ieUTF8qid1427038071sr81keywordsjoyofclojure Communication Radical Candor care personally and challenge directlyhttpswwwradicalcandorcom Nonviolent Communicationhttpswwwnonviolentcommunicationcom Authentic Communicationhttpswwwsoundstruecomstoreauthenticcommunicationhtml Concurrency Programming Concurrency on the JVM Mastering Synchronization STM and ActorshttpwwwamazoncoukProgrammingConcurrencyJVMMasteringSynchronizationdp193435676X Seven Concurrency Models in Seven Weeks When Threads UnravelhttpwwwamazoncoukSevenConcurrencyModelsWeeksProgrammersdp1937785653refsr11ieUTF8qid1427038041sr81keywords7concurrencymodels Culture Conscious Business How to build value through valueshttpswwwsoundstruecomstoreconsciousbusiness4036html Powerful building a culture of freedom and responsibilityhttppattymccordcombook Functional Programming Functional Programming Patterns in Scala and ClojurehttpwwwamazoncoukFunctionalProgrammingPatternsScalaClojuredp1937785475refsr11ieUTF8qid1427039601sr81keywordsfunctionalprogrammingpatternsinscalaandclojure Functional Programming in ScalahttpwwwamazoncoukFunctionalProgrammingScalaPaulChiusanodp1617290653refsr11ieUTF8qid1427039625sr81keywordsfunctionalprogramminginscala Go The Go Programming LanguagehttpwwwamazoncoukProgrammingLanguageAddisonWesleyProfessionalComputingdp0134190440refsr11sbooksieUTF8qid1453287896sr11keywordsgoprogramminglanguage JavaScript Eloquent JavaScript A Modern Introduction to ProgramminghttpwwwamazoncoukEloquentJavaScriptModernIntroductionProgrammingdp1593275846refsr11ieUTF8qid1427038759sr81keywordseloquentjavascript High Performance JavaScript Build Faster Web Application InterfaceshttpwwwamazoncoukPerformanceJavaScriptFasterApplicationInterfacesdp059680279Xrefsr11ieUTF8qid1427038840sr81keywordshighperformancejavascript JavaScript The Good PartshttpwwwamazoncoukJavaScriptGoodPartsDouglasCrockforddp0596517742refsr11ieUTF8qid1427038785sr81keywordsjavascriptthegoodparts Test Driven JavaScript DevelopmenthttpwwwamazoncoukDrivenJavaScriptDevelopmentDevelopersLibrarydp0321683919refsr11ieUTF8qid1427038811sr81keywordstestdrivenjavascriptdevelopment Management The Managers Pathhttpshoporeillycomproduct0636920056843do Managing Humans Biting and Humorous Tales of a Software Engineering Managerhttpmanaginghumanscompitchhtml Patterns Mastering Regular ExpressionshttpwwwamazoncoukMasteringRegularExpressionsJeffreyPaperbackdpB006DVFVTUrefsr15ieUTF8qid1427038873sr85keywordsmasteringregularexpressions PHP Practical Design Patterns in PHPhttppracticaldesignpatternsinphpcom Python Effective Pythonhttpwwweffectivepythoncom Ruby Beginning Ruby From Novice to ProfessionalhttpwwwamazoncoukBeginningRubyNoviceProfessionalExpertsdp1430223634refsr11ieUTF8qid1427038449sr81keywordsbeginningruby Design Patterns in RubyhttpwwwamazoncoukDesignPatternsRubyAddisonWesleyProfessionaldp0321490452refwlitdpopdSnCieUTF8colid1KPZ4BNV1OMC4coliidI3ETXUMF5SXAB1 Metaprogramming RubyhttpwwwamazoncoukMetaprogrammingRubyProgramLikeFacetsdp1941222129refsr11ieUTF8qid1427038474sr81keywordsmetaprogrammingruby Practical Object Oriented Design in RubyhttpwwwamazoncoukPracticalObjectOrientedDesignRubydp0321721330refsr11ieUTF8qid1427038609sr81keywordspracticalobjectorienteddesigninruby Refactoring Ruby EditionhttpwwwamazoncoukRefactoringRubyAddisonWesleyProfessionalebookdpB002TIOYWGrefsr11ieUTF8qid1385646915sr81keywordsrubyrefactoring Sinatra Up and RunninghttpwwwamazoncoukSinatraUpRunningAlanHarrisdp1449304230refsr11ieUTF8qid1385656124sr81keywordsSinatraUpandRunning Shell Classic Shell Scriptinghttpshoporeillycomproduct9780596005955do System Design Networking and Security Amazon Web Services in ActionhttpwwwamazoncoukAmazonWebServicesActionAndreasWittigdp1617292885refsr11sbooksieUTF8qid1453287921sr11keywordsawsinaction Building MicroserviceshttpwwwamazoncoukBuildingMicroservicesSamNewmandp1491950358refsr11ieUTF8qid1427039585sr81keywordsBuildingMicroservices Bulletproof SSL and TLShttpwwwamazoncoukBulletproofSSLTLSIvanRisticdp1907117040refsr11ieUTF8qid1427038544sr81keywordsBulletproofSSLandTLS High Performance Browser NetworkinghttpwwwamazoncoukHighPerformanceBrowserNetworkingperformancedp1449344763refsr11ieUTF8qid1427039463sr81keywordshighperformancebrowsernetworking The Practice of Cloud System Administration Volume 2 Designing and Operating Large Distributed SystemshttpwwwamazoncoukPracticeCloudSystemAdministrationDistributeddp032194318Xrefsr11ieUTF8qid1427039519sr81keywordsthepracticeofcloudsystemadministration Testing Growing Object Oriented Software Guided by TestshttpwwwamazoncoukGrowingObjectOrientedSoftwareGuidedSignaturedp0321503627refsr11ieUTF8qid1385655342sr81keywordsGrowingObjectOrientedSoftwareGuidedbyTests Tools Pro VimhttpwwwamazoncoukProVimMarkMcDonnelldp1484202511refsr11ieUTF8qid1427038180sr81keywordsprovim Vagrant Up and Runninghttpshoporeillycomproduct0636920026358do Version Control with Git Powerful tools and techniques for collaborative software developmenthttpwwwamazoncoukVersionControlGitcollaborativedevelopmentdp1449316387refsr11ieUTF8qid1385647089sr81keywordsversioncontrolwithgit "},{"title":"Refactoring Techniques","tags":["design","oop","patterns","refactoring","ruby"],"href":"/posts/refactoring-techniques","content":" Introduction1 Languages2 Why refactor3 When should refactor4 Tests5 Refactoring Techniques6 Rename Method7 Introduce Explaining Variable8 Inline Temp9 Split Temp Variable10 Replace Temp With Query11 Replace Temp With Chain12 Extract Method13 Inline Method14 Move Method15 Replace Method With Method Object16 Replace Loop With Collection Closure Method17 Pull Up Method18 Form Template Method19 Extract Surrounding Method20 Self Encapsulate Field21 Introduce Named Parameter22 Remove Redundancy23 Dynamic Method Definition24 Extract Class25 Hide Delegate26 Replace Array with Object27 Replace Conditional with Polymorphism28 Decompose Conditional29 Introduce Null Object30 Conclusion31 Introduction Lets begin by considering What is Refactoring The definition of refactoring is a disciplined technique for restructuring an existing body of code altering its internal structure without changing its external behaviour Refactoring is a term originated from the SmalltalkhttpenwikipediaorgwikiSmalltalk community of developers back in the midlate nineties Two of the most prolific programmers of recent times Martin Fowlerhttpmartinfowlercom and Kent BeckhttpenwikipediaorgwikiKentBeck literally wrote the book on the subject of refactoring called Refactoring Improving the Design of Existing Codehttpwwwamazoncomgpproduct0201485672 well written by Martin with contributions from Kent In 2009 both Martin and Kent helped with a rewrite of the book that focused more on the Ruby language than the original books target language of Java This followup book was called Refactoring The Ruby EditionhttpwwwamazoncomRefactoringRubyEditionJayFieldsdp0321603508 and its that book which is the primary driving force of this post Since reading the Ruby edition I wanted to have a short summarised version of some of the more commonly used refactoring techniques mainly for my own reference By that I mean the techniques described in the book that I find interesting and use a lot in my day to day programming life Languages These refactoring techniques arent specific to the Ruby language You can use them when working with JavaScript or PHP or any other language for that matter Programming languages dont all offer identical APIs and so sometimes you might need to tweak the examples slightly to fit your environment Regardless the idioms and syntax differences between languages become redundant when you just focus on the patterns behind the proposed solution Why refactor The purpose of refactoring is to improve the quality clarity and maintainability of your code Simple really But also refactoring can be a great lesson in understanding an unfamiliar code base Think about it if you inherit a poorly designed code base that youve not seen before and you now need to either fix a bug or add a new feature then implementing the code necessary would be a lot easier once you had refactored it to be in a more stable maintainable and ultimately understandable state Otherwise you would be forced to retro fit your new code on top of a poorly designed foundation and that would be the start of a very unhappy relationship When should you refactor Youll usually find the time you start refactoring the most is when you are fixing bugs or adding new features For example you typically first need to understand the code that has already been written regardless of whether it was you who wrote it originally or someone else The process of refactoring helps you better understand the code in preparation for modifying it But dont fall into the trap of thinking that refactoring is something you set aside time for or only consider at the startend of a project Its not Refactoring should be done in small chunks throughout the entire life cycle of the project As the great Uncle Bobhttpwwwcleancodercom once said leave a module in a better state than you found it what this suggests is that refactoring is essential to your daily coding process Tests Before we get started its important to mention that you should have tests in place when youre refactoring You can refactor without tests but realise that without tests to back you up then you can have no confidence in the refactoring you are implementing Refactoring can result in substantial changes to the code and architecture but still leave the top layer API the same So while youre refactoring remember the old adage program to an interface not an implementation We want to avoid changing a public API where ever possible as thats one of the tenets of refactoring If you dont have tests then I recommend you write some now dont worry Ill wait Remember the process of writing tests even for an application you dont know will help solidify your understanding and expectations of the code youre about to work on Code should be tested regularly while refactoring to ensure you dont break anything Keep the red green refactor feedback loop tight Tests help confirm if your refactoring has worked or not Without them youre effectively flying blind So although I wont explicitly mention it below when discussing the different refactoring techniques it is implied that on every change to your code you should really be running the relevant tests to ensure no broken code appears Refactoring Techniques There are many documented refactoring techniques and I do not attempt to cover them all as this post would end up becoming a book in itself So Ive picked what I feel are the most common and useful refactoring techniques and I try my best to explain them in a short and concise way Ive put these techniques in order of how you might approach refactoring a piece of code in a linear top to bottom order This is a personal preference and doesnt necessarily represent the best way to refactor Final note with some of the techniques I have provided a basic code example but to be honest some techniques are so simple they do not need any example The Extract Methodextractmethod is one such technique that although really useful and important providing a code example would be a waste of time and space So without further ado lets begin Rename Method The single most effective and simple refactoring you can implement is to rename a propertyattribute method or object Renaming identifiers can reduce the need for code comments and nearly always helps to promote greater clarity Youll find that renaming things is a fundamental part of other refactoring techniques to aid understanding of the code This technique relies on giving items a descriptive name to ensure the developer knows at a glance exactly what it does The following technique Introduce Explaining Variableintroduceexplainingvariable is effectively the same Introduce Explaining Variable So here is a technique specifically based around the premise of renaming If you have a complicated expression for example youll typically have a long winded set of conditions within an if statement then place that complex expression into a temp variable and give it a descriptive identifier For example unless This is a String with some CAPSscanAZempty puts capitalised text was found end Should be capsnotfound This is a String with some CAPSscanAZempty unless capsnotfound puts capitalised text was found end Note this is the only technique that finds temps ie local variables acceptable This is because temps are deemed to be less reusable than methods due to their very nature being local and so introducing temps is something that shouldnt be considered lightly Maybe consider using the Extract Methodextractmethod technique instead before using this particular technique Also dont worry about performance until you know you have a performance issue to worry about Developers will always suggest that calling methods is slower than running code inline but good programming is about readability and maintainability and extracted methods are not only easier to understand but are much more reusable by other methods So if you are considering using the Introduce Explaining Variableintroduceexplainingvariable technique first decide whether the temp would be more useful if it was available to other methods that way you could use Extract Methodextractmethod instead and avoid defining a temp altogether Inline Temp Temp variables are a bit of a code smell as they make methods longer and can make the Extract Methodextractmethod more awkward as youd have to pass through more data to the extracted method Inline Temp effectively removes the temp variable altogether by just using the value assigned to it Id only suggest doing this if the temp is only used once or if the resulting value has come from a method invocation For example def addstuff 1 1 end def dosomething tempvariablewithdescriptivename addstuff puts Number is tempvariablewithdescriptivename end Should be def addstuff 1 1 end def dosomething puts Number is addstuff end Note a temp by itself doesnt do any harm and in some instances can actually make the code clearer especially if using a result from a method invocation and the method identifier doesnt indicate the intent as well as it should But most likely youll end up using this technique to aid the Extract Methodextractmethod technique as less temp vars means less requirement to pass through additional parameters to the extracted method Split Temp Variable This technique aims to resolve the concern of violating the SRP Single Responsibility Principle although slightly tamer in the sense that SRP is aimed more at ClassesObjects and methods not typically variable assignments But regardless if a temporary variable is assigned to more than once and it is not a loop variable or a collectingaccumulator variable then it is a temp considered to have too many responsibilities For example this is a daft example but what the heck temp 2 height width temp height width Becomes perimeter 2 height width area height width As you can see the temp variable was handling more responsibility than it should be and so by creating two appropriately distinct temps we ensure greater code clarity Replace Temp With Query This technique has a very similar intent to Inline Tempinlinetemp in that one of its primary focuses is to aid the Extract Methodextractmethod The subtle but important difference between this technique and Inline Tempinlinetemp is that the complex expression assigned to the temp needs to be first moved to a method whereas the Inline Tempinlinetemp technique is different in that the temp may already be using a method invocation For example class Box attrreader length width height def initialize length width height length length width width height height end def volume area is the temp area length width area height end end Becomes class Box attrreader length width height def initialize length width height length length width width height height end def volume notice area is now a direct method call area height end def area length width end end This technique can help to shorten a long method by not having to define lots of temp variables just to hold values If the extracted query method is given an identifier that aptly describes its purpose then the code still can be considered clear and descriptive Also it is considered bad form to define a variable which changes once it has been set hence moving to a method better indicates an unstable value Note this technique can sometimes be made easier to implement once youve used Split Temp Variablesplittempvariable Remember this technique as with other techniques is an incremental step towards removing nonessential temps so consider using Inline Tempinlinetemp afterwards thus removing the need for the temp altogether Replace Temp With Chain This is yet another technique designed to rid your code of temp variables If you have a temp variable holding the result of calling an objects method and follow the assignment by using that temp to carry out more method calls then you should consider chaining method calls instead The implementation is quite simple you just have to ensure the methods called return self or this if using a language like JavaScript By allowing methods to chain we again have the opportunity to remove an unnecessary temps For example class College def createcourse puts create course end def addstudent puts add student end end temp Collegenew tempcreatecourse tempaddstudent tempaddstudent tempaddstudent Becomes class College static method so can be accessed without creating an instance def selfcreatecourse college Collegenew puts create course college return new object instance end def addstudent puts add student self refers to the new object instance end end college Collegecreatecourse addstudent addstudent addstudent Extract Method Here it is In my opinion The most used and important refactoring technique The implementation behind this technique is very simple It consists of breaking up long methods by shifting overly complex chunks of code into new methods which have very descriptive identifiers For example class Foo attrreader bar def initialize bar bar bar end def dosomething puts my baz notice this is duplication puts bar end def dosomethingelse puts my baz notice this is duplication puts Something else puts bar end end Becomes class Foo attrreader bar def initialize bar bar bar end def dosomething baz puts bar end def dosomethingelse baz puts Something else puts bar end def baz puts my baz end end But be careful with handling local variables as youll need to pass them through to the extracted method and that can be difficult if there are lots of temps in use Sometimes to facility the Extract Method youll need to first incorporate other techniques such as Replace Temp With Queryreplacetempwithquery and Inline Tempinlinetemp Inline Method Sometimes you want the opposite of the Extract Methodextractmethod technique Imagine a method exists whose content is already simple and clear and whose identifier adds no extra benefit In this instance were just making an extra call for no real benefit So to fix this problem well convert the method invocation into an inlined piece of code unless of course the method is used in multiple places in that case leave it where it is as having it in a separate method keeps our code DRY Move Method In a previous post about ObjectOriented Designhttpwwwintegralistcoukpostsobjectorienteddesignoodclassanalysis I explained that you should query your classesobjects to ensure the methods they define are actually where they should be another reason is feature envy if a method is asking another class a lot of questions then it may be an indication the method is on the wrong object The Move Method technique ensures this decoupling by simply moving the identified misplaced method onto the correct one Once the method has been moved you should clean up the previously passed parameters by seeing what can be moved over to the other object or whether additional data needs to be passed over now via the method invocation For example class Gear attrreader chainring cog rim tire def initialize chainring cog rim tire chainring chainring cog cog rim rim tire tire lets asked the question Please Mr Gear what is your tire size hmm notice this doesnt sound like it quite fits the purpose of a Gears class end def ratio chainring cogtof end def gearinches tire goes around rim twice for diameter ratio rim tire 2 end end Becomes class Gear attrreader chainring cog rim tire def initialize chainring cog rim tire chainring chainring cog cog rim rim tire tiresize end def ratio chainring cogtof end def gearinches tire goes around rim twice for diameter ratio rim tire 2 end end class Tire def selfsize 5 end end From the original classobject keep the original method in place while you test and change it so it now delegates to the method on the new object Then slowly refactor by replacing delegating calls throughout your code base with direct calls to the method via its new host Finally remove the old method altogether and the tests should tell you if you missed a replacement somewhere Replace Method With Method Object You may run into a problem where you have a long method you want to use Extract Methodextractmethod on but the number of temporary local variables are too great to allow you to utilise the Extract Methodextractmethod technique because passing around that many variables would be just as messy as the long method itself To resolve this issue you could look at different types of smaller refactors such as Inline Tempinlinetemp but in some cases it would actually be better to first move the contents of the long method into an entirely new object So the first thing to do is create a new class named after the long method and add the temp local vars as propertiesattributes of the classobject Now when you try to implement Extract Methodextractmethod you dont have to pass around the temp vars because they are now available throughout the classobject Then from within the original classobject you can delegate any calls to the original method on to the object youll still pass on the original arguments to the method within the new object but from there on the method extraction becomes easier For example class Foo def bar puts Were doing some bar stuff end def baza b c if a something do something end if b else do else end if c none do none end end end Becomes class Foo def bar puts Were doing some bar stuff end end class Baz attraccessor a b c def initializea b c a a b b c c if a something do something end if b else do else end if c none do none end end end From here were now in a better state to use both the Extract Methodextractmethod and Replace Conditional with Polymorphismpostsevenmorerefactoringtechniquesreplaceconditionalwithpolymorphism techniques to refactor the Baz class Replace Loop With Collection Closure Method If you write a loop that parses a collection and interacts with the individual elements within the collection then move that interaction out into a separate closure based method meaning you replace the loop with an Enumerable method This refactoring may not be as clear or impressive as other refactoring techniques but the motivation behind it is that you hide the ugly details of the loop behind a nicer iteration method allowing the developer looking at the code to focus on the business logic instead For example managers employeeseach do e managers e if emanager end Becomes managers employeesselect e emanager Ruby has a few of these types of enumerable methods but other languages such as PHP and JavaScript arent so lucky JavaScript has a couple of accumulators Arrayreduce and ArrayreduceRight but they arent very useful as closure based collection methods compared to Ruby which has methods such as Enumerableinject Enumerableselect seen in above example or Enumerablecollect Note in JavaScript you can implement a similar effect with clever use of closures Pull Up Method When you have duplicated code across two separate classes then the best refactoring technique to implement is to pull that duplicate code up into a super class so we DRY Dont Repeat Yourself out the code and allow it to be used in multiple places without duplication meaning changes in future only have to happen in one place For example class Person attrreader firstname lastname def initialize firstname lastname firstname firstname lastname lastname end end class MalePerson Person This is duplicated in the FemalePerson class def fullname firstname lastname end def gender M end end class FemalePerson Person This is duplicated in the MalePerson class def fullname firstname lastname end def gender F end end Becomes class Person attrreader firstname lastname def initialize firstname lastname firstname firstname lastname lastname end def fullname firstname lastname end end class MalePerson Person def gender M end end class FemalePerson Person def gender F end end Form Template Method The technique is reliant on inheritance a parent class and two sub classes of that parent The two sub classes have methods which have similar steps in the same order and yet the steps themselves are different The technique involves moving the sequence of steps into the parent class and then using polymorphism to allow the sub classes to handle the differences in the steps Here is a silly example Im no good at giving real examples you may have noticed here is an example of our problematic code class Foo end class Bar Foo def initialize hey 1 hai 2 end def qux a hey hai b a 10 a b end end class Baz Foo def initialize hey 5 hai 7 end def qux a hey hai b a 10 20 a b end end bar Barnew baz Baznew puts barqux puts bazqux we could try to inject the values each sub class requires but then we still have a lot of duplication in this code We can see the sequence of steps is determine what a should be determine what b should be return a specific calculation so we can clean up our code a little by abstracting the commonality class Foo def initializehey1 hai1 hey hey hai hai end def qux determinea determineb result end def determinea a hey hai end def result a b end end class Bar Foo protected def determineb b a 10 end end class Baz Foo protected def determineb b a 10 20 end end bar Barnew1 2 baz Baznew5 7 puts barqux puts bazqux Extract Surrounding Method If you find you have different methods which contain almost identical code but with a slight variant in the middle then pull up the duplicated code into a single method and pass a code block to the newly created method which it yields to in order to execute the unique behaviour def dosomething puts 1 yield puts 3 end dosomething puts 2 This is actually a common pattern in Ruby known as the wrap around method This technique is similar to the Form Template Methodformtemplatemethod but is different in that you can use it without forcing an inheritance model on your code Note JavaScript doesnt have the ability to pass a code block but it can be replicated by passing a function that acts like a callback function doSomething callback consolelog1 callback consolelog3 doSomethingfunction consolelog2 although in the latest versions of Node as of November 2013 Generators are implemented and would allow JavaScript code to yield similar to how Ruby works Self Encapsulate Field When inheriting properties from a parent classobject then it can be more flexible if the parent class only allows access to the properties from within a gettersetter The motivation for this technique is that a sub class can override and modify the behaviour of the gettersetter without affecting the parent class implementation Which is similar to how the Decorator design pattern works eg modifying the behaviour without affecting the original This technique should only be used once you find the coupling between objects is becoming a problem Otherwise direct access to properties and instance variables should be acceptable initially For example def total baseprice 1 taxrate end Becomes attrreader baseprice taxrate def total baseprice 1 taxrate end Introduce Named Parameter When method arguments are unclear then convert them into named parameters so they become clearer and easier to remember Although Ruby supports named parameters def turnOnTheTV channel 1 volume 1 end turnOnTheTVchannel 101 volume 10 neither PHP or JavaScript do so for PHP you can pass an associated Array and with JavaScript you can pass an ObjectHash For example JavaScript function turnOnTheTVc v turnOnTheTV101 10 Becomes function turnOnTheTV config configchannel 101 configvolume 10 turnOnTheTV channel 101 volume 10 Note ECMAScript 60 the latest JavaScript specification which is still being worked on as of Nov 2013 implements named parameters Remove Redundancy This isnt an explicit technique more a grouping of techniques The principle idea being that code evolves and as it evolves you may find techniques you previously implemented as part of an earlier refactoring have since become redundant Imagine you implemented the Introduce Named Parameterrefactoringtechniqueshtml22 technique passing a hash with named properties as a single argument instead of multiple unidentified arguments Now after some other refactorings have taken place you discover the method originally refactored is no longer as complex and so your argument hash refactor has been reduced down to just a single named property In this particular scenario you should remove the named parameter and simply pass a single argument instead This principle applies with other refactoring techniques Imagine an earlier refactoring included implementing a default parameter value for a method call As your code evolves if you discover you now only ever call the method with an argument then the default value becomes redundant and makes the code more complex than it needs to be by providing a default value So just remove the redundant code Dynamic Method Definition Sometimes defining multiple methods can be wasteful when functionally they carry out similar steps For example imagine we had the following code def failure do selfresult failure end def success do selfresult success end def error do selfresult error end Notice how the functions are structurally identical They simply set a result property to have a value This can be refactored using Rubys definemethod method which lets you create methods dynamically at run time failure success erroreach do method definemethod method do selfresult methodtos end end Note you could also abstract this code into a more reusable and easier to maintain function like so def dynamicmethodsmethodnames block methodnameseach do methodname definemethod methodname do instanceexecmethodname block end end end You can also use this technique to help ease creating properties on an object For example I used this technique in my MVCPhttpwwwintegralistcoukpostsmvcp blog post to dynamically create instance variables require apppresentersbase require appmodelsperson class PresentersPerson modelname age modelage end end module Presenters class Base attraccessor model def prepareviewdata hash hasheach do name value instancevariablesetname value end end end end Extract Class This is a pretty standard technique which helps ensure your objects abide by the SRP Single Responsibility Principle If you find your classes are doing too much then simply create a new class and move the relevant fields and methods over one by one while running the tests as you go to ensure all code continues working as expected Doing so youll end up with two small focused and clean classes which are easier to manage Hide Delegate This technique focuses on the principle of object encapsulation Specifically decoupling two or more objects by reducing the context the objects have of each other The following code demonstrates the idea module Bar def display puts Bar Stuff end end module Baz def display puts Baz Stuff end end class Foo include Bar def dosomething display end end foo Foonew foodosomething as you can see the user only needs to rely on the interface having a dosomething method The implementation details of dosomthing in this case the delegation off to another method are hidden If we changed include Bar for include Baz or maybe we dont mixin a module at all and just write some code inside of dosomething it doesnt matter because the public interface is set as far as the user is concerned Replace Array with Object The motivation for this technique is to convert a simple data container which holds multiple data types into an object with clear and descriptive identifiers This principle helps to present your complex data into a more sensible format I demonstrated this in a previous post on objectoriented designpostsobjectorienteddesignooddirectreferences This technique also makes the data interaction more maintainable by providing an easier and understandable interface to the data Here is an example where were violating the principle of a clean data interaction class Foo attrreader data def initializedata data data end def dosomething dataeach do item puts item0 puts item1 puts "},{"title":"Remote Working","tags":["home","remote","working"],"href":"/posts/remote-working","content":" Introduction I was asked on twitter recently about how I am able to be successful and impactful at an organization as a remote worker What follows is a run down of how I work remotely and how it has changed over the last seven years This is not a rules for how you should do remote working but more so a single perspective on the problem of remote working and my own simplified set of guidelines that I feel have worked for me Why Remote Lets start off by talking about why I am a remote worker I typically work for organizations who have an office in Central London while my home is out on the east coast of England My commute into work in the morning is just over two hours and my commute home is either the same or worse depending on the state of the trains and the nightmare that is the home time commuting hours of 4pm7pm Thats almost five hours a day sitting or more likely standing on a train Also because of my locality I have to be up before 6am in order to get to work by a reasonable hour It also means to avoid the commuter rush in the evening including trying to acquire a seat for that long trip home I typically have to leave the office by approximately 430pm latest I dont know about you but I consider that 1 stressful 2 tiring 3 inefficient Before I had a family that would have been time back to myself to do things such as having a more relaxed breakfast having more time to get ready for the day ahead and maybe even going to the gym before work Now that I have a family thats critical time I get to spend with them before I start work as well as to do some house chores and help my wife with getting our son ready in the morning If you dont have children then its probably best that I explicitly state what otherwise might be considered stating the obvious and apologies if so but just in case getting children ready in the morning is a lot harder time consuming and stressful than youll probably realise So its important I help my wife with this otherwise its a massive burden on her shoulders Note this can often be something as simple but meaningful as looking after our son for a mere ten minutes just so she can get washed and dressed Not to mention things like getting deliveries to your home or getting a plumber round to fix your sink or even time to take your kids to the doctors All of these things that people have to deal with in their daytoday lives can be made easier if we were able to work from home Problems with Remote Working OK so not everything is roses Here is a short list of things that can be a concern when working from home and Ill dig into each afterwards When do I startfinish How do I get noticedfeel included How can I be impactful How should I communicate How do I prove Im getting stuff done How do I avoid procrastinating How can I stay healthy Note this is a small list from a much larger set of questions people often have about remote working but hopefully these are enough items for me to discuss to help get across my thoughts When do I startfinish This is important You need a routine Doesnt matter what it is but you need one If you think remote working is waking up whenever you like every day and wandering around the house in your pyjamas and whimsically selecting the code tasks youre going to hack on then youre in for a rude awakening Remote working is WORK So you get up at a consistent time in the morning you get washed and dressed you have breakfast and a coffeetea and then you think about the day ahead Start WORK This is likely going to sound no different to maybe how youre working at the moment as a nonremote worker right Exactly Im up at 7am and I start work by 9am sometimes 10am if theres something else I need to do in the morning I do a full days work so if I start at 9am Ill be finished by 5pm If Im starting at 10am Ill work till 6pm Sometimes youll find a natural wrap up of your day is earlier than when you should officially finish For example youre planning to finish at 5pm but its 430 and you dont want to pick up another large task today In those instances I would normally just jump over to read through any late email Ive received Id then jump onto Slack and checkin with my team most likely Ill be dropping an end of day summary which can be very helpful for my team mates who are based in different timezones to me How do I get noticedfeel included This is actually a subtle question with many different layers to it In most companies I work for I start as a partial remote worker so maybe two days a week in the office That can be really helpful as far as building up relationships with my colleagues You need good relationships But if youre starting off as a completely 100 remote worker dont worry building relationships is easy enough to do It just takes a bit of effort on your part which Ill explain next I can say its easy to do because I have built up many great working relationships with my colleagues based in the United States both in the eastern and western timezones Note although reaching the western timezone is tricker than eastern as its a solid eight hours behind Its possible to do still but again will take just a bit more effort if thats what you want Ultimately it comes down to communication and good communication at that For communication to be considered good it needs to happen at the right time and in the right medium see How should I communicatehowshouldicommunicate for more details on this subject Youll find that I will every morning hopefully around the same time drop a message into Slack for the UK engineering team saying something along the lines of Morning all some explanation of the gif I typically put an explanation below the gif if it is based on the current weather outside or maybe theres a big event happening or maybe its just a funny caption to go with the gif whatever The reason I do this is because thats me Thats the type of person I am and I want other people to recognise that this is my personality coming through Im giving a bit of myself to these people I also then do the same thing for my team who currently are all based outside of the UK in our own team Slack channel Of course I dont just do that Im also very open about my personal life and what Im doing For example Ill often post pictures of my daily lunch time walks with my son Again the reason I do this isnt to gloat although sometimes it is when the weather is sunny but a nice coastal scenery shot with me and my son in it reminds people that Im a human being and not some anonymous entity behind a keyboard and screen You should try to learn more about your colleagues and appreciate what sorts of things theyre into Maybe they love photography or are into games or books Maybe they like riding their bike on the weekend or going rock climbing Having freeform nonwork related conversations is great for this so find a space to have those conversations So for example I know a colleague of mine who works in Argentina loves cats and has two of her own I also have cats and we chat regularly in DMs about not only work but we share pictures of our cats Simple things like that help build stronger relationships and makes working together more meaningful We also follow each other on Instagram and I have many work friends current and old who follow me and viceversa on twitter and facebook We spend a large majority of our life at work so making friends and really caring about who you work with will make a massive positive impact on your enjoyment while at work Dont limit yourself to reaching out to just your team or to people in your locality Make an effort to take part in various channels of communication including those that might be happening across different timezones This might be as simple as joining various Slack channels where people talk about certain languages or tech but it can include joining various working groups Ive been part of and led many working groups architecture code design documentation communication You can spin up a working group about anything youre interested in and by sharing it with the organization youll be surprised to discovered that there are likely lots of likeminded individuals interested in the same things and willing to discuss and to want to help improve the quality of the thing happening within the organization Once you start doing the groundwork of understanding your colleagues and proactively sharing a piece of your true self at work youll also find that people will start reaching out to you for both your opinion and to include you in conversations this could be because they recognize you have specific skills that will benefit their project or maybe its something more informal someone cracked a joke and ccs you as part of it You have for lack of a better word exposed yourself to the organization How should I communicate Communication takes many forms and is one of the biggest contributing factors to misunderstanding requirements and to personal unhappiness Its one of the most critical skills to learn and to utilize regardless of whether youre a remote worker or not For example dont just Slack people all the time You should notice when conversations are getting out of hand ie too much back and forth overlapping textual messages with no one really reading the other persons message youre both firing off in the hope that your point is heard first and offer instead to jump over to a facetoface video call to clarify With facetoface people generally are more easily able to listen because the convention in our society isnt to shout interweavingly across each other Instead someone talks and if youre doing what youre supposed to be youll be listening and really taking in what the other person is saying Youll then adapt your response based on what the other person has just said this is much different to what a lot of people mistakenly do which is stay quiet until the other person has stopped talking and then reply with their precanned response ie again theyre only interested in getting across their own points One thing that I like to do after taking a long Slack conversation into a facetoface video call is to go back to Slack and to summarize what was just disussed There are a few reasons for doing this its a documented record of what was discussed it confirms that everyone has understood the outcome correctly it shows youre conscientious of people in other timezones who might have been following the conversation Another important characteristic of good communication you should also aim to be clear and if the situation calls for it concise this is something Im guilty of not being good at but Im aware of that problem and try hard to be better at In my blog posts Im definitley not concise but then Im usually telling a story and thats a different situation to writing an email or a Slack message but even still Ive a long way to go to improve no ones perfect For those of you who are interested I would highly recommend reading the following material Radical Candor care personally and challenge directlyhttpswwwradicalcandorcom Nonviolent Communicationhttpswwwnonviolentcommunicationcom Authentic Communicationhttpswwwsoundstruecomstoreauthenticcommunicationhtml A couple of other important notes to make Make sure you block out time in your calendar for important events such as a lunch hour or family time etc This becomes especially useful when working within a distributed company for example in my calendar I block out 5pm to 10pm for family time so people in New York and LA cant accidentally try to book a meeting with me Schedule monthly 11 meetings with people of differing levels within the company such as interns seniors managers designers etc This is useful because it gives you a regular checkin with whats happening elsewhere in the organisation and it also helps others to get to know you How can I be impactful To have impact can take many different forms Being impactful isnt just about getting a job done It could be mentoring colleagues or it could be noticing a problem space either in the tools your organization uses or the services it provides and producing an RFC documentation that tackles some possible solutions I think ultimately to be impactful requires you to care You need to care about the organization and its success You need to care about your colleagues and their success If youre just turning up at work to go through the motions and to just get paid then youll likely always fall short of having true life affirming impact How do I prove Im getting stuff done This is a classic concern for both people wanting to be a remote worker as well as organizations who are unsure remote working is something they want to offer to their workers Any half decent organization is going to have process and that process should be effective regardless of whether youre remote or not You should have some form of projecttask tracker eg Jira you should have some form of methodology of working eg Kanban Scrum Agile etc With this process you should have enough to demonstrate youre getting things done For example Daily team standups this is your chance to say what you worked on yesterday what you worked on today and whether anything is blocking you This is a great time to express things concerning you and for your managerteam to get an insight into that Task tracking if you use Jira then youll have a set of tasks on a backlog that are prioritized and are assigned to specific individuals Management can track and review what youre doing and see if things arent getting done One to one meetings you have an opportunity at 11 meetings with your line manager to express your concerns and it can be a good time to highlight things youre doing not projecttask specific as thats not the point of 11s but maybe youve created a working group or are working on an important RFC How do I avoid procrastinating I personally feel that people are capable of procrastinating regardless of whether theyre remote or not To avoid procrastinating means being focused getting rid of distractions settings yourself deadlines and generally being organized None of these are traits that should be limited to remote workers as this is just what it means to be an attentive and considerate worker I would say that if youre going to be a remote worker though youll want to have yourself a space where you can work Yes its nice to be able to work from any where in your home but to have a dedicated office space is important I have an office space which I use the majority of the time because its quiet and its away from the family noise But I also find that when its sunny I like to work from the lounge in the morning because the sun hits that room first and also means I can be around my son while hes enjoying his morning playtime I might then move to working from the kitchen so I can look out at my garden and again enjoy some sunshine and scenery as the sun moves around during the day But again I do mainly work from my office space especially when I have meetings or I feel I really need to concentrate If you listen to music while working then that can actually be a cause of distraction For me if I need to focus or am working on something Im unsure about I tend to find music with lyrics generally distracts me so Ill listen to genres like jazz classical or downbeat instrumental trip hopish stuff like DJ Crushs Code 4109 But if Im working on a system I know very well and Im just churning out stuff like a batch of unit tests then I can listen to anything Ultimately you need to get used to recognizing when youre distracted and to combat that however it makes sense for you How can I stay healthy This is a very important topic as remote workers are notorious from suffering burnout from over working You need to have clear boundaries so you need to say I start at N time in the morning and Ill finish at N time in the evening Ill also have lunch at N time during the day and have a full lunch For me personally I have a desk that transitions from a sitting desk to a standing desk I find in the morning I like to stand for an hour or two and then Ill sit for an hour or two and then Ill go for a walk along the seafront sometimes with my son or sometimes by myself if I need some space during my lunch hour I make sure that my lunch is something I cook I avoid ready meals or snacky foods like crisps and chocolates I generally only ever have a nibble of dark chocolate in the evenings if I have a sugar craving As an example a general day of eating will look something like Breakfast bowl of mixed nuts raisins blackberries banana pumpkin seeds and oat milk Lunch salmon and salad Dinner meat veg and a bit of carb I also dont drink coffee or normal tea Im primarily a drinker of water peppermint tea and if I need caffeine Ill drink a pot of Guayusahttpstheguayusaco green tea which has more caffeine than coffee but is much healthier for you Note if youre able to try and do some exercise as well A sedentary lifestyle is not good for us If you cant get to a gym or arent motivated to do exercise then dont kick yourself about it But do get up and out of your chair for a walk every once in a while Fresh air really does make a world of difference The key thing is to be active and to have space to think about other things that arent work related as this will actually help your subconscious to really kickin behind the scenes and process conversations and other things going on at work Its also important to realize that youre remote so you dont have to work from home You can work anywhere theres an internet connection if your work provides you with a mobile wifi then thats even better because you can just go to the beach and work from there if you like Ive never done it but I could imagine going to my local library and working from there to be quite nice Maybe going to a bustling coffee shop with free wifi Either way get out and away from your home as often as you can so you dont start building up a form of cabin fever What doesnt work well OK there are some things that still dont work that great when being a remote Below is a small selection Whiteboard discussions architecture design meetings using a whiteboard can be difficult there is technology to help with this but its not great One thing were looking to explore is for all employees remote and inoffice to have access to a drawing tablet for their computer so they can draw with an electronic pen into some shared screen software This would be much more flexible and easier to work with than trying to draw with a mouse Remote meetings specifically Im referring to people who arent familiar with doing remote meetings and so theyll do silly things like treating the remotes as if they arent there So theyll sit with their back to the camera or theyll ask for opinions from people in the room but not take the time to say for those here who are attending remotely does anyone have anything to add because jumping into a conversation remotely can be much harder than if youre in a physical officeroom Unplanned conversations sometimes conversations are sparked in the office between colleagues They dont always happen on Slack and so its important that those people in the office are thinking about their remote colleagues and include them when necessary That said theres not much thats so problematic that remote working couldnt be a consideration for most organizations Conclusion Hopefully youve found this rundown of remote working from someone who has been remote working for many many years now with great success and much learnings under his belt If you have any additional questions then do please reach out to me on twitter "},{"title":"Security basics with GPG, OpenSSH, OpenSSL and Keybase","tags":["bash","gpg","keybase","keys","openssh","openssl"],"href":"/posts/security-basics","content":" Introduction1 What are keys and how do they work2 Understanding PKI3 OpenSSL vs OpenSSH4 What is GPG5 Creating your own keys6 OpenSSH61 OpenSSL62 GPG63 Multiple Keys631 How to encrypt data using GPG OpenSSL and Keybase7 GPG encryption71 Asymmetrical encryption711 Symmetrical encryption712 Key Signing713 Digital Signatures714 Revoking Keys715 OpenSSL encryption72 Keybase73 Which should I use74 Creating selfsigning issuing and revoking certificates8 Conclusion9 UPDATE for those short on time read the following Introduction1 What are keys and how do they work2 and then skip over the sections Understanding PKI and OpenSSL vs OpenSSH as these just go into more depth on the technical aspect of different encryption concepts Just skip until What is GPG5 in there are two sub sections about OpenSSH SSH Agent and OpenSSL just skip those until you get to the next GPG section and continue all the way from there Introduction This post isnt meant to be this is how you do security Im not a security expert Im not even a security intermediate When I titled this post security basics I wasnt kidding If youre working with applications andor servers in production then please consult someone better equipped on the subject of security Note although quite a tough read at times I would highly recommend Bulletproof SSL and TLS written by Ivan Risti Now the actual purpose of this post was twofold 1 Solidify my own understanding of the tools Ill be covering 2 Helping others to also understand the purposes of said tools Security can be confusing Its taken me longer than I care to admit to really understand the things Ill be discussing here and even then Ill likely have missed a lot of important nuances and with that said what am I planning on covering in this post Well that would be What are keys and how do they work Understanding PKI OpenSSL vs OpenSSH What is GPG Creating your own keys How to encrypt data using GPG OpenSSL and Keybase and a lot more inbetween Plaintext and Ciphers Throughout this post youll see me use words like plaintext and cipher Its important to know what these mean before moving on so lets clarify this now When a file is said to be plaintext it simply means that its unencrypted whereas a cipher is a noun that refers to a plaintext that has been encrypted Thats it What are keys and how do they work Imagine you have a plaintext file which contains a password and you want to share this file with someone else across the internet lets say this someone is our friend Bob You could open an email or chat program attach the file and send it to Bob But this isnt very safe because you could have some devious person sniffing your network traffic picking up your communication and subsequently stealing the plaintext file containing the password youd rather they not get access to To prevent this devious person from being able to see the password we would need to encrypt the plaintext document into a cipher and to transfer the cipher instead of the plaintext meaning if anyone was to interrupt your communication then they would get the cipher and it would be unreadable In order to encrypt the plaintext into a cipher we need to use a technique that relies on the concept of a key A key is a mathematical algorithm for turning plaintext into seemingly random alphanumeric characters A key can be used to both encrypt and decrypt plaintext Note the longer the key the more secure the encryption will be This is why when generating keys youll typically be asked to provide the key size eg 128bit you want to use for your encryption key But in order for your recipient Bob in this case to know how to decrypt the cipher youve sent to them they also need to know the key youve used So how do you let the intended recipient Bob know the algorithm youve used You cant just email them and say Ive used algorithm X to turn this plaintext document into a cipher because the same devious person who originally sniffed your network traffic and grabbed your cipher will also be able to sniff this additional communication and learn the algorithm ie the key that was used to encrypt the plaintext allowing them to decrypt your cipher and to retrieve your super secret password Publickey cryptography To resolve the issue of not being able to safely communicate an encryption key some clever people designed a scheme known as publickey cryptography The principle idea being that you generate two keys 1 A public key 2 A private key As you can probably already guess the public key is something that is safe to become public ie if some devious person got a hold of your public key then its not that much of an issue while the private key is something you should keep hidden and not share with anyone its very important you protect this file But how exactly do these two keys help our situation Well the keys are the mathematical inverse of each other which means you can encrypt data with either the public or private key and only the alternating key can be used to decrypt the data So if you encrypted a plaintext using your public key then the only way you can decrypt the resulting cipher is by using your private key But imagine you encrypted a file using your public key as your private key is something only you have access to it means your cipher is safe from everyone as long as your private key stays private Now that we have a basic understanding of publickey cryptography you should be able to see how this can be used to keep our ciphers safe from being decrypted by unintended devious type people So lets go back to our previous example where we have a plaintext document that we want to encrypt and only share with our friend Bob Publickey example Note this example is INSECURE and I explain why afterwards In this example there are two prerequisites 1 Weve generated our own publicprivate keys 2 Bob has generated his own publicprivate keys With a basic understanding of publickey cryptography the steps involved appear quite straightforward We email Bob and ask him for his public key Bob emails us his public key We encrypt our plaintext using Bobs public key We email Bob the cipher created using his public key Bob receives the cipher and uses his private key to decrypt it Bob now has a copy of the original plaintext In an ideal world these steps are fine but we dont live in an ideal world Now you may have already noticed the problem with this process but if not Ill clarify why this isnt secure the devious network sniffer has intercepted your email communication asking Bob for his public key and the devious person sends back his own public key instead So at this point you get a public key that you think is Bobs but which actually belongs to the devious person Now when you go to send the cipher back to Bob the devious person sees your communication going across the wire and intercepts it again and grabs the cipher and is able to decrypt it using his private key and subsequently gets access to the plaintext Authentication We arrive at yet another security problem with encrypting data and although using something like publickey cryptography helps it doesnt solve the issue of authentication By this we mean how to do we know the person were communicating with is really who they say they are You might think for everyone to securely identify themselves they could publish their public keys online This would mean instead of people having to provide you with their public key via an insecure communication channel they could point you to a secure location where their public key resides This is where a service such as httpskeybaseiohttpskeybaseio comes in this is still in preview There are also more traditional services that you can use such as keyserverubuntucomhttpkeyserverubuntucom11371 pgpmiteduhttppgpmitedu and keyserverpgpcomhttpskeyserverpgpcomvkdGetWelcomeScreenevent Note You can access my public key here keybaseiointegralisthttpskeybaseiointegralist But unless youre talking in reallife or over the phone with the actual person you want to communicate with then how do you really know who published the key was the person you think it is Authenticating people is a difficult problem to solve and this is where PKI Publickey infrastructure comes in Understanding PKI Public key infrastructure is built on top of Publickey cryptography The difference is that PKI introduces the concept of certificates and these certificates are used in the software realm much like we would use a passport In the real world the government is a trusted authority ok so maybe thats questionable nowadays but go along with it please and they issue you a passport which contains details and information that uniquely identifies you In the digital world a certificate does much the same thing Now at this point its worth pointing out that certificates are designed to identify websites rather than people and so PKI is built on the premise that you are communicating with a domainweb server They dont really help us with regards to the problem we had earlier with transferring a cipher securely Ill come back to that issue later Note technically certificates are created using the X509 standardhttpsenwikipediaorgwikiX509 What PKI can do is help verify the communication between you eg your web browser and another website is handled securely and is happening with the correctrelevant endpoint This is useful because if youre doing some online banking you want to be sure that communication between you and the bank are happening privatelysecurely without anyone being able to sniff your information over the wire But also you want to be sure youre communicating with your bank and not some devious endpoint pretending to be your bank but in fact is getting you to type in your account and password details The way that PKI manages the ability to authenticate an endpoint ie some websiteservice youre communicating with is through the use of certificates When you visit a website youll use either the http or https protocols The latter is what signifies a secure connection When using https if the website has a valid certificate then your browser knows that the communication is happening with the right website So how can you trust a certificate Surely someone can set up a website that looks like your bank then create a certificate and associate it with their website domain Yes this is possible but the idea of PKI is that it is built upon a web of trust Let me clarify what that means Certificate Authorities CAs Your web browser has a list of organisations it trusts known as a CA or Certificate Authority and these organisations can issue certificates If a website uses a certificate that has not been issued ie signed by one of these trusted CAs then your web browser will display a warning that you probably shouldnt continue on to the website as it doesnt appear to be who it says it is ie the website could be who they say they are your bank but we cant really trust them because the certificate theyve presented to us wasnt issued by a CA we know of Note certificates are created and then signed using an encrypted signature This is done using the CAs private key Because the CAs public key is well public it means our browser can use the public key to verify that the certificate it is presented by a website was indeed issued by a CA we trust and wasnt created by some devious personorganisation instead So where do these trusted organisations come from Well as you can imagine there is a very high cost and detailed process involved with becoming an authorised CA This is because we have to implicitly trust them to look after our best interests and only issue certificates to companiesorganisations who have proved their true identity through the CAs own rigorous registration process Intermediate CAs Now CAs will sometimes create intermediate CAs These are organisations who can issue certificates on behalf of the original CA also known as the Root CA If you go to a website that has a certificate you can inspect the certificate to verify whether it was issued by a root CA or by an intermediate CA If the certificate came from an intermediate CA then you can follow the thread back to the root the web browser typically handles this verification check for you One of the reasons this is done is because the root CA is very very important It has the power to issue certificates and so if the private key ever fell into the wrong hands then it could be used to generate certificates for all sorts of domainswebsites that werent who they claimed to be In the real world once a root CA is setup the private key is stored offline For example the hard drive the private key is stored on is extracted from the computer and stored in a fire safe even input ports are filled with glue preventing someone from stealing the drive and trying to extract the data Serious business this CA stuff Certificate Revocation List CRLs Certificates are issued with a validity period expiration date Every time your browser interrogates a sites certificate it is checking the certs validity period If the date for the validity period has passed then the browser will warn you that the certificate is now expired At the same time though if the certificate hasnt expired then your browser will consult its Certificate Revocation List to see if the certificate has been revoked This CRL is downloaded by your browseroperating system on a regular basis and there in lies the problem with CRLs theyre not realtime results Imagine a certificate was issued for the website wwwfoocom but later needed to be revoked for whatever nefarious reason In this scenario the CRL is updated to state the website wwwfoocom has a revoked certificate and so it cannot be trusted But because the CRL has to be downloaded in order to see the updated list the user you could end up visiting the website before you had the new CRL and so the certificate would still be seen to be valid Because of the lack of realtime validation checking the Online Certificate Status Protocol OCSP has superseded CRLs in that it is as the name would suggest an online resource which systems can query at runtime to verify the validity of a certificate SSL and TLS So far weve been talking about certificates being the solution to how we can authenticate a servers identity and PKI as the overarching process for helping us to secure that communication using publickey cryptography under the covers To help PKI achieve its goals a cryptographic protocol was designed called SSL Secure Socket Layers This protocol was subsequently superseded by a new protocol called TLS Transport Layer Security PKI uses these protocols to enable the secure communication Note you might wonder why you dont hear the phrase TLS used much and instead see SSL referenced everywhere on the internet when talking about PKI security This is just an unfortunate case of SSL having become a marketing term that most people can recognise and understand The majority of the time if someone mentions they have SSL enabled then what they probably really mean is that theyre using the TLS protocol SSL handshake Cipher Suites and Key Exchanges In order to secure the communication between the client and the server PKI uses the stages defined within its protocol to fufil whats commonly referred to as the SSL handshake This is a set of communicative steps taken between the client your web browser and the server Remember from earlier we discussed how publickey cryptography works and that with it we can secure the communication channel but at this point were still not sure how that happens without exposing the encryption key necessary to encrypt our data back and forth across the wire to any devious people sniffing our network traffic As well see in a moment one of the steps in the SSL handshake is called the key exchange this exchange between the clientserver is for the encryption key and is done using a publickey cryptography algorithm The most popular choice at the time of writing is the RSA algorithmhttpsenwikipediaorgwikiRSAcryptosystem which uses the servers public key provided in the certificate the server sends to the client to encrypt the key before sending it to the server Note if youre using the DiffieHellman key exchange algorithmhttpsenwikipediaorgwikiDiffieE28093Hellmankeyexchange youll find a great visual explanation of the process which uses the analogy of mixing colours to indicate the maths behind the equation eg easy to calculate in one direction but very difficult to reverse much like mixing two colours together is easy but unmixing would be quite arduous There are also performance penalties associated with some more advanced key exchange algorithms that you need to take into consideration In order for the SSL handshake to proceed successfully the client needs to provide the server with some preliminary options one being a cipher suite A cipher suite has a structure that looks something like the following TLSRSAWITHAES128CBCSHA This might just look like a jumble of acronyms so lets break down what this means TLS the protocol RSA the key exchange algorithm AES the encryption algorithm 128 the encryption strength CBC the encryption mode SHA the hash function for a digital signature In the above example we use RSA which is interpreted as both the key exchange algorithm AND the authentication mechanism But with other cipher suites youll see two separate values eg DHERSA where DHE is the key exchange and RSA is the authentication mechanism The client supports various different cipher suites and so itll send all of the different variations it is happy to handle while the servers job is to find the most secure match and respond to confirm the cipher suite it has selected Note cipher suites are just one of many areas of communication open to a MITM maninthemiddle attack For example a devious network sniffer intercepts your initial insecure communication with a server and removes all the cipher suites leaving only the weakest one The server has no option but to select the one and only cipher suite left meaning the attacker has an easier time brute forcing through the weaker encryption methods One other item well want to be aware of is whats called a MAC Message Authentication CodehttpsenwikipediaorgwikiMessageauthenticationcode The MAC is a way of ensuring authentication and integrity by combining an agreed key and a hashing cipher to create a signature for some content If we send some data well also send a MAC with it and because both sides have the keycipher information we can ensure the message content hasnt been tampered with Well see in just a moment that one of our handshake steps will be for the clientserver to verify each other using a MAC Lets take a quick look at the SSL handshake this isnt exhaustive and Ive left out lots of steps for brevity Client sends cipher suite a random number a protocol version and list of compression methods Server sends back a selected cipher suite most secure match a random number the protocol version it supports and a selected compression method Client requests certificate for identification Server sends its certificate Client sends a premaster secret encrypted using the key exchange algorithm defined in the selected cipher suite Server decrypts the premaster secret Both clientserver can generate a master secret using both sets of random numbers previously sent to each other ClientServer uses the master secret to derive the encryption keys used to encrypt all future communication Client sends a MAC of the communication so far Server creates a MAC and compares with Client MAC if the same then Server switches to encryption Server tells the client its ready for secure messages Client sends secure mesages Example messages In the previous sub section I briefly ran through the different steps the client and server take in order to communicate securely with each other But Id like to add onto that some examples of these messages Note these examples are copied verbatim from the excellent book Bulletproof SSL and TLS written by Ivan Risti Here is the first example this is the client opening communication with the server Handshake protocol ClientHello Version TLS 12 Random Client time May 22 2030 024346 GMT Random bytes b76b0e61829557eb4c611adfd2d36eb232dc1332fe29802e321ee871 Session ID empty Cipher Suites Suite TLSECDHERSAWITHAES128GCMSHA256 Suite TLSDHERSAWITHAES128GCMSHA256 Suite TLSRSAWITHAES128GCMSHA256 Suite TLSECDHERSAWITHAES128CBCSHA Suite TLSDHERSAWITHAES128CBCSHA Suite TLSRSAWITHAES128CBCSHA Suite TLSRSAWITH3DESEDECBCSHA Suite TLSRSAWITHRC4128SHA Compression methods Method null Extensions Extension servername Hostname wwwfeistyduckcom Extension renegotiationinfo Extension ellipticcurves Named curve secp256r1 Named curve secp384r1 Extension signaturealgorithms Algorithm sha1rsa Algorithm sha256rsa Algorithm sha1ecdsa Algorithm sha256ecdsa As you can see all the ingredients are there as we described earlier the cipher suite being the most important to take note of at this time Lets move on and see what the servers response would typically look like Handshake protocol ServerHello Version TLS 12 Random Server time Mar 10 2059 023557 GMT Random bytes 8469b09b480c1978182ce1b59290487609f41132312ca22aacaf5012 Session ID 4cae75c91cf5adf55f93c9fb5dd36d19903b1182029af3d527b7a42ef1c32c80 Cipher Suite TLSECDHERSAWITHAES128GCMSHA256 Compression method null Extensions Extension servername Extension renegotiationinfo Here we can see the server has sent back its random data used to construct the premaster secret and also we can see which cipher suite it has selected to be used Verify SSL Sometimes you might need to debug an issue with your SSL connection In order to do that youll want to utilise the OpenSSL utility command sclient This will allow you to open a connection to your host using the SSLTLS protocol of your choice and control the various different configuration settins A basic example would be as follows openssl sclient connect googlecom443 showcerts Youll see were connecting to Google which is secured using SSLTLS and we also specify the showcerts flag which allows the response to display all certificates provided within the chain The response looks something like the following CONNECTED00000003 depth2 CUSOGeoTrust IncCNGeoTrust Global CA verify errornum20unable to get local issuer certificate verify return0 "},{"title":"Setting Up Nginx With Docker","tags":["docker","nginx","ruby"],"href":"/posts/setting-up-nginx-with-docker","content":" Introduction1 Just give me the code2 Setting up nginx3 Setting up the Ruby application4 Linking your app to nginx5 Conclusion6 Introduction I wanted to learn about how to use the popular proxy server nginxhttpnginxorg for handling reverse proxy duties on a Ruby project I was working on So I decided the easiest thing to do in order to play around with nginx as I work on a MacBook Pro laptop would be to install and run it and my backend application within Dockerhttpdockercom containers Note I did some preliminary Googling and sadly didnt find anything straightforward that demonstrated this relatively simple requirement ie run nginx and have it proxy requests to a backend service So I decided it would be best to write about it myself Now if youre unsure of what a reverse proxy does it simply takes in traffic ie users requesting a website domain and proxies those requests onto another service the service could be external to the host server or it could be running on the same box which is the scenario I have and typically the service being proxied to isnt publically available to the internet The nginx server has many features such as load balancing caching and serving static files to name but a few When used as a reverse proxy it can also be useful for handling SSL termination ie SSL is an expensive operation and so the proxy server will authenticate the provided credentials allowing access but terminate the SSL requirement at that point before directing traffic onto the other protected service In this post well be primarily focusing on using nginx as a reverse proxy although I also demonstrate how to serve static files But the focus will be reverse proxying and not demonstrating other features within nginx Ill also not be explaining Docker and how it works Im assuming youve used Docker in some form or another previously Lastly Im going to take you on the same journey I took while setting this all up so rather than work through a perfect scenario youll get to see some of the errors I stumbled across along the way If youd prefer to just read the small amount of code then the next section is for you Just give me the code githubcomintegralistdockerexamplesnginxhttpsgithubcomIntegralistDockerExamplestreemasterNginx Setting up nginx So to begin with I went to Docker Hubhttpshubdockercom and found the official nginx Docker imagehttpsregistryhubdockercomnginx It suggested the easiest thing to do to get started was to download and run the image so knowing not a lot about nginx thats exactly what I did docker run name nginxcontainer P d nginx Note you can use the p option instead of P if youre not going to be running multiple nginx instances again for the moment I was just following the recommendation from Docker Hub Once the container was running I attempted to curl the endpoint something like httplocalhost but I found this didnt work and by that I mean it didnt return a recognisable nginx home page as was suggested it would This was the first tripup I made The reason this didnt work is because for me using Boot2Dockerhttpboot2dockerio on a Mac rather than being on a pure Linux machine capable of running Docker natively means that localhost is the Mac and not the Docker host which is the Boot2Docker VM So to resolve that issue I needed to curl the ip of the Boot2Docker host To get the ip simply run boot2docker ip So your curl command depending on what OS youre running on should look something like curl http For me this was curl httpboot2docker ip32781 Executing this resulted in the following output Welcome to nginx body width 35em margin 0 auto fontfamily Tahoma Verdana Arial sansserif Welcome to nginx If you see this page the nginx web server is successfully installed and working Further configuration is required For online documentation and support please refer to nginxorg Commercial support is available at nginxcom Thank you for using nginx Mounting our own static files Now at this point having a standard nginx welcome page isnt very useful We want to add our own nginx configuration and to have it serve up our own static files To achieve that with Docker well need to use Volumeshttpsdocsdockercomuserguidedockervolumes But before I show the command for that I need to show you the project directory I have right now Dockerfile Gemfile Gemfilelock apprb nginxconf html testhtml This isnt the final directory structure mind you but its what I initially started out with Youll see as we move on I had to add a little more structure to the project But for now this is what weve got Ill show the contents of these files now most of them are really small anyway Dockerfile FROM ruby21onbuild CMD ruby apprb Gemfile source httprubygemsorg gem sinatra apprb require sinatra set bind 0000 get do Hello World end get foo do Foo end nginxconf user nobody nogroup workerprocesses auto autodetect number of logical CPU cores events workerconnections 512 set the max number of simultaneous connections per worker process http server listen 80 Listen for incoming connections from any interface on port 80 servername Dont worry if Host HTTP Header is empty or not set root usrsharenginxhtml serve static files from here htmltesthtml Hey there Here is my test HTML file Mounting the files Lets see the syntax structure of the command I was looking to run docker run name nginxcontainer v pathtostaticfilesonhostusrsharenginxhtmlro v pathtoconfonhostetcnginxnginxconfro P d nginx In the above example you can see Im using the v flag to mount my static files directory to usrsharenginxhtml as well as mounting my own nginx configuration file into the container at etcnginxnginxconf Note ro sets the volumes to be read only So for me a working example looked like the following docker run name nginxcontainer v pwdhtmlusrsharenginxhtmlro v pwdnginxconfetcnginxnginxconfro P d nginx Note the host path has to be absolute so tweak it as necessary I used pwd to make the command shorter If you want to debug things then you can run the container not as a daemon d but with an interactive tty it and drop yourself inside of a bash shell docker run it name nginxcontainer v pwdhtmlusrsharenginxhtmlro v pwdnginxconfetcnginxnginxconfro P nginx bash Once the container is running and the nginxconf and static files are mounted as a Volume you can verify that nginx is serving the static files by trying to hit localhost on port 80 as mentioned earlier if youre on a Mac using Boot2Docker like me then youll need to access localhost via the Boot2Docker VM ip address instead Note dont forget the port number Ive used will be different for you get yours from docker ps curl httpboot2docker ip32781 This will display a 403 error page which is to be expected because weve not mounted a folder that has a indexhtml file Our mounted directory only contained a testhtml file so if we try to access that instead then well see itll be loaded without a problem curl httpboot2docker ip32781testhtml If you were following along at home then you would have noticed that this returns the following HTML content Hey there Here is my test HTML file If we try the exact same command again but this time add the I flag eg curl I this returns just the HTTP headers well see that this is indeed being served by nginx HTTP11 200 OK Server nginx193 Date Sat 01 Aug 2015 172701 GMT ContentType texthtml ContentLength 53 LastModified Sat 01 Aug 2015 140559 GMT Connection keepalive ETag 55bcd24735 AcceptRanges bytes Dynamically updating containers without restart Because weve mounted the html directory as a volume we can now create an indexhtml file for nginx to use as the root page to load when you request httpboot2docker ip32781 and in doing this itll take effect immediately without requiring us to restart the container Once weve created the file inside the html directory we can then make the relevant curl request to see nginx serve up the html file Just so you know the indexhtml file I created looks like this Welcome This is my home page Setting up the Ruby application The Ruby application well be creating is a super simple Sinatrahttpsinatrarbcom web application with two routes defined 1 2 foo One of the first things you need to do is to make sure you run bundle install so you have a Gemfilelock generated otherwise the base Ruby image will complain The reason it complains is because the base Docker image is using an onbuild version of the Ruby image and what this means is that it follows a convention over configuration model where by it assumes you have three files available apprb Gemfile and Gemfilelock for it to automatically copy into the built Docker image for you Once youve done that you can now build the Docker image docker build t myrubyapp Once the image is built you can run your Ruby application like so docker run name rubyapp p 45674567 d myrubyapp Youll notice were not using a dynamic port range because the Sinatra app weve created explicitly binds to port 4567 and so Im exposing that specific port to the Boot2Docker VM We can then access this application directly with the following curl command curl httpboot2docker ip4567 Results in Hello World Now if we want to see the HTTP Headers coming back curl I httpboot2docker ip4567 Results in HTTP11 200 OK ContentType texthtmlcharsetutf8 ContentLength 11 XXssProtection 1 modeblock XContentTypeOptions nosniff XFrameOptions SAMEORIGIN Server WEBrick131 Ruby21620150413 Date Sun 02 Aug 2015 092047 GMT Connection KeepAlive Linking your app to nginx OK so were nearing the finish line now Weve got a working nginx container and a working Ruby container We need to gel them together by making a request and having nginx proxy the request on to the Ruby container if the request was aimed at that or have nginx serve a static file if the request was relevant to that To get this to work with Docker we have two options 1 Host file 2 ENV variables Both options require a custom Dockerfile to be used for setting up nginx and both have about the same amount of complexity in different ways But to understand these options we need to be sure we understand what Docker does when linking containers So to clarify when linking a container Ruby with another container nginx Docker adds the ip of the linked container Ruby into the other container nginx It does this by updating the containers etchosts file with a new entry that looks something like the following note the last line of the file output hostconf hostname hosts roota4a3bde52f8e cat etchosts 17217019 a4a3bde52f8e 127001 localhost 1 localhost ip6localhost ip6loopback fe000 ip6localnet ff000 ip6mcastprefix ff021 ip6allnodes ff022 ip6allrouters 17217018 app 3435f6926e83 rubyapp You can test this yourself by using the following command which interactively jumps into a running version of the nginx container docker run rm it name nginxcontainer link rubyappapp P nginx bash Once inside the container run cat etchosts to get the above output or something similar to it at least your ip will likely be different to mine While youre still inside the temporarily running nginx container we can clarify the second option which is the ENV variable setup Docker handles When linking a container Docker will add details of the linked container as environment variables as well as adding a new entry to the etchosts file So while youre still inside the running nginx container execute the following command env grep APP sort Note were greping for the phrase APP as thats what we specified in our docker run command ie link rubyappapp We should now see the following output or something very similar APPENVBUNDLERVERSION1106 APPENVBUNDLEAPPCONFIGusrlocalbundle APPENVGEMHOMEusrlocalbundle APPENVRUBYDOWNLOADSHA2561e1362ae7427c91fa53dc9c05aee4ee200e2d7d8970a891c5bd76bee28d28be4 APPENVRUBYMAJOR21 APPENVRUBYVERSION216 APPNAMEnginxcontainerapp APPPORTtcp172170184567 APPPORT4567TCPtcp172170184567 APPPORT4567TCPADDR17217018 APPPORT4567TCPPORT4567 APPPORT4567TCPPROTOtcp Notice that Docker has capitalised the link name eg app is now APP and has used the convention of Choosing an option With this knowledge secured in our minds we can now understand what we need to do for each option ie host file vs environment variable For the host file option we can manipulate our nginxconf file manually before building the image to use a named upstream server that references the link name we provided as part of the docker run command Whereas with the environment variables option we would need to manipulate the nginxconf file dynamically at run time So within our Dockerfile we would add a shell script which would be executed by the CMD statement within the Dockerfile This shell script would run and use something like the sed command to replace a placeholder reference within our nginxconf file likely wed use the APPPORT environment variable and do some regular expression parsing for the ip address Once the placeholder had been replaced with the appropriate container ip we would manually start nginx using service nginx start Out of the two options I think Ill go with the host file one So lets run through the steps Create a new directory structure that looks like the following you can grab the exact files from githubcomintegralistdockerexamplesnginxhttpsgithubcomIntegralistDockerExamplestreemasterNginx dockerapp Dockerfile Gemfile Gemfilelock apprb dockernginx Dockerfile nginxconf html indexhtml testhtml Build the images well see what these look like in a moment for the Ruby app and nginx Run the Ruby app container Run the nginx container and link it to the running Ruby container Verify everything works Other than moving existing files related to the building of the Ruby Docker image inside of a dockerapp folder the two biggest changes are the addition of a new Dockerfile inside the dockernginx folder and the following updated nginxconf file user nobody nogroup workerprocesses auto autodetect number of logical CPU cores events workerconnections 512 set the max number of simultaneous connections per worker process http upstream app server app4567 app is automatically defined inside etchosts by Docker server listen 80 Listen for incoming connections from any interface on port 80 servername Dont worry if Host HTTP Header is empty or not set root usrsharenginxhtml serve static files from here location app catch any requests that start with app proxypass httpapp proxy requests onto our app server ie a different container The aim of the new configuration is to accept any requests to app and proxy them onto our application server So if we were to request app wed want nginx to proxy it to the Sinatra route and if we were to request appfoo then wed expect nginx to proxy it to the Sinatra foo route The new Dockerfile weve created inside the dockernginx folder will have the following content FROM ubuntu install nginx RUN aptget update aptget install y nginx RUN rm rf etcnginxsitesenableddefault forward request and error logs to docker log collector RUN ln sf devstdout varlognginxaccesslog RUN ln sf devstderr varlognginxerrorlog EXPOSE 80 443 CMD nginx g daemon off Note some bright chap by the name of Steven Jackhttpstwittercomstevenjack85 suggested setting ENTRYPOINT in the Dockerfile to nginx and have the two options g and daemon off left in the CMD Doing this means if you wanted to pass other options to nginx you wouldnt have to duplicate all of your CMD So from the root of our project directory lets complete the next step and build our new Docker images technically you dont have to rebuild the Ruby application as there has been no changes to the Dockerfile but just for completion Ill demonstrate the build command again First lets rebuild the Ruby Docker image docker build t myrubyapp dockerapp Now lets build our new custom nginx Docker image docker build t mynginx dockernginx The third step in our list was to run the Ruby container docker run name rubyapp p 45674567 d myrubyapp Lets also verify that its running the following command should return back Hello World curl httpboot2docker ip4567 Now onto our fourth step which was to run our custom nginx container whilst linking it to our already running ruby container docker run name nginxcontainer v pwdhtmlusrsharenginxhtmlro v pwddockernginxnginxconfetcnginxnginxconfro link rubyappapp P d mynginx The final step is to verify that everything worked as expected ie we should be able to make a request to our Boot2Docker VMs localhost and have it proxy the request through to our Ruby application server But lets do it in stages so the first stage is to hit the root of localhost Note in the following example youll need to get the dynamically allocated port number for the nginx container You can do this by running docker ps and extracting the port number from the output provided curl httpboot2docker ip This curl request will result in the following output to stdout Welcome This is my home page The second stage is to hit the testhtml endpoint for localhost and make sure nginx is still serving back our static files correctly curl httpboot2docker iptesthtml This curl request will result in the following output to stdout Hey there Here is my test HTML file Looking good OK the third and final stage is to now try to hit the app endpoint for localhost and make sure nginx is proxying the request through to our Ruby backend application server and also sending the result back again curl httpboot2docker ip32785app This curl request will result in the following output to stdout body textaligncenterfontfamilyhelveticaarialfontsize22px color888margin20px c margin0 autowidth500pxtextalignleft Sinatra doesnrsquot know this ditty Try this get x27x2Fappx2Fx27 do quotHello Worldquot end Ewww OK so we can see nginx is proxying through to our Ruby application server but the server doesnt seem to know how to handle the request app Maybe its something to do with the extra forward slash on the end of the request If I change it to app instead well see what that does curl httpboot2docker ip32785app This curl request will result in the following output to stdout 301 Moved Permanently 301 Moved Permanently nginx146 Ubuntu Hmm ok thats not much better So whats going on here Well were seeing the 301 redirect because Sinatra is redirecting anything without a forward slash So ok but what about the Sinatra doesnt know this ditty error The final solution The cause of the problem is because weve misconfigured nginx The solution to the issue requires us to know a very subtle detail about how nginx location blocks work which is If you dont put a forward slash at the end of the upstream name then youll find nginx passes the request exactly as it was made ie app through to the backend service as apposed to passing it as just For example we had set the proxy to pass to httpapp which meant a request to app was being passed to Sinatra as app and a request to appfoo was being passed on to Sinatra as appfoo and as you can see from the Sinatra application code we have no such routes defined What we should do instead is change the proxy pass value to httpapp notice the extra forward slash just before the closing semicolon By putting a after the upstream name means it acts more like nginxs alias directive which remaps requests for you If I kept it as httpapp then I wouldve needed to have added an app route to my Ruby application But the additional forward slash remaps app so its passed to Sinatra as we have a route for that as well as remapping appfoo to just foo we have a route for that too So with this in mind youll see in GitHub Ive updated the nginxconf to look like the following with additional comments to clarify the behaviour user nobody nogroup workerprocesses auto autodetect number of logical CPU cores events workerconnections 512 set the max number of simultaneous connections per worker process http upstream app server app4567 app is automatically defined inside etchosts by Docker server listen 80 Listen for incoming connections from any interface on port 80 servername Dont worry if Host HTTP Header is empty or not set root usrsharenginxhtml serve static files from here location app catch any requests that start with app proxypass httpapp proxy requests onto our app server ie a different container NOTE If you dont put a forward slash at the end of the upstream name then youll find nginx passes the request as app rather than just Putting after the upstream name means it acts more like the alias directive If I kept it as httpapp then I wouldve needed to add a app route to Sinatra With this change made we can now make the following requests successfully curl httpboot2docker ipapp curl httpboot2docker ipappfoo Conclusion This was a bit of a whirlwind run through to getting a simple nginx reverse proxy setup with Docker You can obviously swap out the Ruby backend with whatever technology stack is more appropriate eg Node Clojure Scala whatever From here if youre new to nginx like I am you can start to experiment with the many other features nginx provides Enjoy "},{"title":"Static Search With Lunr.js","tags":["elasticsearch","javascript","js","lunr","solr","static"],"href":"/posts/static-search-with-lunr","content":" Introduction1 Background2 Steps3 Setup4 Introduction You have a statically generated website like mine and you want to implement some kind of search facility that is 1 Free 2 Doesnt Suck eg no ads or iframe 3 Quick ie no serverside communication The solution is to use Lunrjshttpslunrjscom You might be wondering about Lunrs origins Well its based loosely on the idea made popular by Solrhttpsluceneapacheorgsolr which is an opensource search platform built on a Java library called Lucene Since then weve also seen the release and rise of ElasticSearchhttpswwwelasticcoproductselasticsearch which is an opensource distributed and RESTful search engine built on top of the Apache Lucene library Background Ill explain this from the perspective of Hugohttpsgohugoio which is the static site generator I use to produce this website Hugo stores the metadata for each post eg title date categories tags etc in something it calls Front Matterhttpsgohugoiocontentmanagementfrontmatter I use YAML but you can use JSON or TOML and thats important to note because the implementation I use is based on my metadata being in YAML format So if yours is JSON or TOML for example then youll need to modify the code shown in this post to reflect your use case Below is the front matter for this post youre reading "},{"title":"Statistics and Graphs: The Basics","tags":["graphs","monitoring","performance","statistics","stats"],"href":"/posts/statistic-basics","content":" Introduction1 Information vs Data2 Frequency3 Watch out for misleading data4 Pie Chart5 Bar Chart6 Stacked Bars61 Split Bars62 Histograms7 Differences71 Calculating dimensions72 Frequency Density73 Line Graphs8 Averages9 Which average to use10 Ranges11 Percentiles12 Variance13 Conclusion14 Introduction I started learning about statistics because I found myself doing a lot of operational monitoring ie making systems more observable instrumenting individual services and monitoring that data via custom built dashboards Although for the most part everything worked as expected and I generally understood the data I was seeing visualised I wanted to be sure I wasnt missing any important information or worse misrepresenting the data This began my journey into learning about statistics as a complete beginner I picked up a book on the subject and started taking notes Note I highly recommend reading Head First Statisticshttpshoporeillycomproduct9780596527587do which is where the majority of this information has stemmed There youll find a lot more detail and better break downs of the ideas This blog post is the result of what I have learnt so far and its motivation is to explain some of the basic concepts behind utilising statistics to represent data This post is aimed at beginners as I myself am very much a beginner in this space So lets begin by defining what statistics means Statistics is a branch of mathematics dealing with the collection analysis interpretation presentation and organization of data WikipediahttpsenwikipediaorgwikiStatistics OK that seems reasonable enough Statistics is an umbrella term that encapsulates the complete pipeline of how data is acquired analysed and visualised So what is data and what does it look like Lets move onto the next section where we can begin to clarify and understand it a bit more Information vs Data If youre like me you may well have confused the words data and information as being the same thing But there are actually important differences that should be understood Data is the raw factsfigures Information data that has extra contextmeaning applied For example raw data might look like 3 5 7 Whereas information would be the taking of that raw data and giving it extra context So in this example those three data points could represent the age of three children In a classic CSV comma separated values format which is used for storing tabular data it might be represented like so Age 3 5 7 Note data is either numerical dealing with numbers quantitative describing quantities or categorical data is split into categories that describe qualities or characteristics also referred to as qualitative Frequency When dealing with statistical data the first thing you typically learn about is data frequency Lets start with a definition In statistics the frequency or absolute frequency of an event is the number of times the event occurred in an experiment or study WikipediahttpsenwikipediaorgwikiFrequencystatistics Consider the following data 3555772 The first number 3 and the last number 2 both have a frequency of one in that they only appear once throughout the entire dataset Whereas the number 5 has a frequency of three and the number 7 has a frequency of two for similar reasons ie 5 appears three times and 7 appears twice Another way to represent this data is by defining a separate column for the frequency which allows you to more clearly see the unique numbers that are present in the dataset Age Frequency 3 1 5 3 7 2 2 1 Note strictly speaking you might want to refer to the above snippet as information rather than data simply because the raw data now has context added to it which clarifies what the numbers mean The datainformation can be visualised in many ways depending on the graph type you wish to use Some graphs are better suited for representing certain types of data than others Well take a look at some different graph types to see how they work but first lets take a moment to consider how data can trick us Watch out for misleading data The following example is very contrived and silly but it does illustrate the point about being aware of how data can be manipulated to represent what you want it to Below are two line graphs Well talk about this type of graph in more detail later but effectively we create two axis and then plot our data onto the graph and draw a line between the dots Now both graphs use the same datapoints but the first graph is misleading the viewer while the second graph is more accurate Take a look and see why that might be At a quick glance or if you just didnt know any better you would see the first graph and think the company has some incredible profit growth But the second graph doesnt look as impressive This is because the first graph is zoomed in from the starting point 20 whereas the first graph is zoomed out at the correct level so you can see the data in a more accurate and representative form See also my comment in the next section about pie charts5 Youll notice there that the data can be misrepresented if not all the information is made available to the user So the data isnt lying its just the view the user has of the data isnt accurately portrayed due to purposeful data ommission this is a trick newspapers and academic papers use to represent a point of view they wish to push Pie Chart A graph such as a pie chart will split your data up into distinct groups These groups are represented as relative percentages of the total group meaning the total area adds up to 100 Below is an example pie graph that uses the data 15020050 which could represent just for example usage of certain programming languages within an organisation The pie chart is generated based off the percentage representation of the underlying data Here is how the graph data is calculated Start by finding the total ie sum all the data Then calculate 1 of that total Finally calculate each groups individual percentage Total 15020050 400 1 Percent 400100 4 Now we know the total is 400 and 1 of that is 4 we can calculate each groups individual percentage Python group 50 ie 400100 50 200 Go group 375 ie 400100 375 150 Bash group 125 ie 400100 125 50 Well know if weve calculated things correctly if the sum of the group percentages results in 100 50 375 125 100 Pie charts are useful for understanding at a glance the relative difference between groups of data But they become less useful when the data is close together as each slice becomes effectively the same size Misleading As mentioned in the previous section all graphs can be presented in such a way as to mislead you and make you think that the reality is different to what it really is Pie charts are no exception Pie charts visually display their information in relative percentages but if they dont also show or include somewhere near the graph the frequency for each group within the pie chart then the graph could be misleading The reason it can be misleading is because without the frequency you cant identify whether the data being presented is consistent This may or may not indicate whether its fair to compare the data in this way as it might not be truly representative Whether that is the case or not depends on the type of data being presented In our case its not really an issue for two reasons 1 We include the frequency data in our pie chart hover over or click on the chart to view frequency 2 Our data is simple enough to not be misrepresented So you have to be careful with data to make sure its as inclusive as possible or that youre explicit about what youre focusing on or how the data might be lacking Earlier we mentioned a problem of data groups percentages being too close together resulting in a graph that was hard to distinguish subtle differences In those scenarios you might find a more suitable option would be the bar chart Bar Chart If you wanted to see the programming language data in a format that is more suitable for subtle differences then one option would be a bar chart Lets view the dataset we have Language Frequency Go 150 Python 200 Bash 50 When dealing with a horizontal bar chart directly below the individual data groups in this case the programming languages are placed along the y axis while with a vertical bar chart example below that theyre placed along the x axis One thing to notice about a bar chart is that the width of the bar is the same doesnt matter if its a horizontal or vertical variation their widths stay consistent Its the length of each bar thats actually important This is because the length represents the frequency of the groups data This can trip people up as they might mistake a bar chart for a histogram where the width of the bar does change in relation to the frequency dont worry well come back to histograms later Stacked Bars Now imagine you wanted to visualise data that represented how much people liked or disliked specific programming languages this is different to the previous data which was the general usage of programming languages The dataset might look something like the following Language Like Dislike Go 290 10 Python 150 100 Bash 50 250 To represent this multifaceted data you could use a specific type of bar chart known as a stacked bar chart see below This type of graph is useful because it represents both the frequency and the relative percentage of the various data types Note stacked bar charts are also known as segmented Its less likely for this chart to be misrepresented because the length of each bar is based on the data frequency You can see for the groups Go and Bash weve had a consistent number of reports 300 in total and so when the values are visualised as a percentage the length of the bars are the same Whereas the Python group has less data frequency compared to the other groups only 250 in total where the other groups were 300 and so although it correctly represents that data as a percentage 60 were like vs 40 dislike its still not as representative as a whole in comparison to the other data groups we have Ideally each group would have consistent frequencies Split Bars Another type of bar chart is called splitcategory and is useful for comparing frequencies unlike the stackedsegmented bar chart which compares frequency but represents them visually in percentages Histograms Ive yet to find a charting library that lets me create an actual histogram in the strict definition most libraries seem to call standard bar charts histograms which makes it hard for me to visually demonstrate them unless I hand draw a chart which is what Ive had to resort to below Differences Based on what Ive read there are some key differences between histograms and standard bar charts These are 1 The area width and height of each bar is proportional to its frequency 2 There should be no gaps between each bar The reason for histogram bar sizes being proportional unlike a traditional bar chart is because histograms are usually best suited to dealing with grouped numerical data remember with a traditional bar chart each bar width is the same and only the bar length is relevant as its determined by the frequency Calculating dimensions When constructing a histogram youll place the groups on the x axis and make the width of each bar the same as the range it covers while placing the frequencies for each group on the y axis and make the length of the bar match the frequency value for that group Doing this is fine as long as the groups have a consistent range ie they all have the same interval size like 055101015 and whose rangeinterval distance are all five Inconsistent Ranges But in some datasets a single group can cover a much wider range than the other groups For example consider a dataset for gaming hours played by a group of 17 users Hours Frequency 01 5 120 2 2022 10 Here hours is the grouped numerical data As explained above we would put the frequency on the y axis and the grouped data along the x axis see below for an example graph Note we can see our first bar 01 spreads only one interval and 2022 spreads only two intervals whereas the middle bar 120 spreads over nineteen intervals The problem with the above graph is that the height of the middle bar is wrong as weve set it to be as high as the frequency value itself and the third bar is incorrect too which normally would be fine if the interval range were the same across all groups but in this case it isnt the correct approach due to each group covering a different range from each other In using this data the 120 groups area could mistakenly look disproportionately large the 2022 group also has a different range so that would be a problem as well So to solve the problem of disproportionate sizes when dealing with multirange groups we have to make sure that both the width and height or area of the group is proportional How to fix the proportions To fix the bar area size problem we need a different calculation for determining the height of each bar also known as the frequency density frequency range frequency density For example the range for 120 is 19 and its frequency is 2 2 19 010 Meaning the height of the bar its frequency density for 120 should be set to 010 and not to the frequency value itself which was 2 You would then apply this calculation to the height of each groupbar see below Youll see we also needed to change the height of the 2022 group based on this new area calculation 10 2 5 Note I appreciate the difference between the original height of 2 vs the correct height of 010 for the range 120 and 10 vs 5 for the range 2022 when using this contrived example isnt exactly ground breakingly different but this is just to help you understand the general idea behind calculating histogram areas for groups that have inconsistent ranges We can now see the overall area or shape for each bar represents the actual frequency and can be calculated like so frequency range frequency density Example the group 2022 had a range of 2 and a frequency density of 5 If we were to look at the graph by itself which only shows the proportional area relative to the other groups ie it only shows the frequency density and the range we could reverse engineercalculate the actual frequency value with the abstract calculation above 2 5 10 Frequency Density As alluded to earlier the height of each bar indicates the frequency density which itself is best explained via an analogy Imagine you have a tall glass and you pour a set amount of liquid into it that fills the glass to approximately 34 If you now pour that liquid into a much wider glass the liquid amount hasnt changed but the height has changed Itll be at a lower level as the liquid has spread out more across the available glass space The frequency density is related to frequency but is focused more around its concentration If we wanted to calculate the total frequency for the entire dataset then we would use the following calculation group range x group frequency density for each group which gives us the frequency for each group and then we sum each of the frequencies together Note this is as far as I got with histograms as I felt I had learnt enough to be dangerous in a conversation There may well be nuances proscons and other aspects to histograms Ive not fully understood Id welcome anyone who knows more to educate me on this Line Graphs Weve already seen a couple of example line graphs at the start of this post To create a line graph you require two axis x and y and the data to be mapped onto different points across these axis which allow us to plot a line between the dots we mark These types of graphs are best for identifying trends in your data and are most useful when applied across numerical data such as time which again helps with overarching trending patterns Note compare this to bar charts which are generally better for comparing values or categories There are various types of line graphs one is known as an accumulative frequency graph which we can demonstrate using the earlier grouped dataset of hours played online If we were to take the hours data we could graph a specific subset view of that data For example we can visualise the number of people who played for a specific number of hours So we could say how many users were playing online for up to five hours The way we do this is by adding up all the previous groups frequencies which determines the upper limit for each group So for example using our previous data Hours Frequency 01 5 120 2 2022 10 We can calculate the cumulative frequency like so 01 upper limit 1 totalcumulative frequency 5 120 upper limit 20 totalcumulative frequency 527 2022 upper limit 22 totalcumulative frequency 521017 Resulting in the following cumulative frequencies 01 5 120 7 2022 17 We can now plot these onto a line graph by placing the cumulative frequencies onto the y axis and the groups across the x axis Then mark onto the graph the upper limits against the relevant cumulative frequency and finally join up the dots In the above example graph if we asked how many people were playing for up to 21 hours the answer would be approximately 11 people Averages When looking at data and graphs people are generally interested in averages because they help us better gauge whatwhere the majority is But there are actually three different types of average and each one has a different purpose mean average median average mode average Mean The mean average is the average that most people think about To calculate the mean you would sum every number in your dataset and then divide the result by the number of elements in the dataset For example dataset 259 sum 16 2 5 9 mean 3 16 3 Note the mean average can be expressed mathematically by xn When broken down x pronounced sigma x is a quick way of saying add together the values of all the xs without having to say what the values are This also can be expressed with the Greek symbol When dealing with datasets that have frequencies we need to ensure the frequencies are included as part of the calculation For example consider the following data Age 19 20 21 Frequency 1 3 1 What the frequencies indicate is that 19 and 21 only appear once whereas 20 appears three times So when summing the values x this doesnt mean 192021 instead it means 1920202021 Similarly when dividing by the number of items in the dataset the n in xn it means dividing by 5 131 and not taking the number of frequencies literally so not dividing by 3 131 Resulting in a calculation that gives us the mean average as 20 1920202021 131 20 or 100 5 20 Note this type of mean average ie one applied to data that has frequencies associated with it can be expressed mathematically with fxf and equates to multiply each number by its frequency then add the results together fx then divide the result by the sum of frequencies f Theres an issue with using the mean average and thats outliers Consider the following dataset Age 19 20 21 145 147 Frequency 3 6 3 1 1 If we wanted to calculate the average age using fxf that would look like the following 319 620 321 145 147 36311 38 This is telling us the average age is 38 That number doesnt actually exist in the dataset What has happened is that the outliers the large numbers at the end of the dataset 145 147 have pulled the mean higher meaning the data is skewed Data can be skewed to the left the mean is pulled lower or it can be skewed to the right the mean is pulled higher as in the case of our example above Note we say skewed left or right because when sorting the data in ascending order you would see the outliers are either mainly to the left or the right depending on the data If we were to look at this on a line histogram graph we would notice this pulling of the mean Note in the above graph you can see the longer tail that indicates the outliers How can we deal with outliers Well this is where the median can help Median The median gives us the exact middle of our data To calculate this you would take all the numbers inc their frequencies sort them and then select whatever is the middle number Lets see this using the following dataset Age 19 20 21 145 Frequency 3 6 3 1 Lets include the frequencies and sort the numbers in ascending order 19 19 19 20 20 20 20 20 20 21 21 21 145 Theres 13 numbers in total ie sum the frequencies f like so 3631 So to find the exact middle we just divide the number by 2 which gives us 65 and then round it so it becomes 7 Meaning the seventh number in the dataset is the exact middle 20 Note alternatively it can be abstracted to n12 But what happens with a dataset with an even set of numbers Well in that case you need to take the mean of the two middle numbers ie sum them and divide the result by 2 Consider the following example 19 19 20 20 20 21 22 23 145 147 There are 10 numbers so if we divide by 2 well get 5 If you count five items inwards from either the start or the end of the dataset youll find you land on two different numbers in this case 20 and 21 So in this scenario you sum 20 and 21 2021 41 and divide the result by two 412 205 Meaning the median of the above dataset is 205 Note what youll typically find is that if the data is symmetrical then the mean and the median will result in the same value But what happens if a dataset has multiple clusters of values so for example the dataset includes values that are at both low and high ends Consider a baby swimming group this would include very young ages ie the babies and older ages ie the parents and so you might find the mean and median both result in an average age for that dataset as being midteens which traditionally wouldnt necessarily be correct depending on the socialcultural environment Also the results can vary significantly if there are more babies or parents joining the class The solution to this problem is to return the most popular value the one with the highest frequency This is where the mode average can help Mode The mode is the third and final type of average and it aims to return the value with the highest frequency Which means the value that it gives as the result MUST exist within the dataset unlike the mean which could result in a value that doesnt exist Note if there are multiple frequencies with the same value then the dataset is considered bimodal Meaning there are multiple modes for the dataset The mode average is the only average that works not just on numerical data but categorical data When youre dealing with categorical data the mode is the most frequently occurring category Here are the steps for calculating the mode 1 Identify all the distinct categories or values in your dataset 2 Document the frequency for each categoy or value 3 Select the values with the highest frequency to find the mode The mode is considered most useful when dealing with data that has a small number of modes or when the data is categorical instead of numerical remember neither the mean nor the median can be used with categorical data But if there are lots of modes identified then the mode average can end up being less useful over other averages Lets see some example data and identify the mode average in each Values 1 2 3 Frequency 3 7 4 The mode for the above dataset is 2 as it is the value that has the highest frequency Categories Red Yellow Green Frequency 20 7 4 The mode for the above dataset is the Red category as it is the category has the highest frequency Values 1 2 3 Frequency 3 7 7 The above dataset actually contains multiple modes ie multiple modes with the same frequency which happens to be the highest in the set The modes being 2 and 3 whose frequency is 7 Which average to use Imagine we have staff managers and a CEO and theyre all determining how pay rises should be calculated when based on the average salary Below is a suggestion as to which average each one of them might suggest using and why Staff median staff prefer the median average as it helps reduce the effect of the CEOs salary as an outlier Managers mean the large CEO salary is an outlier thus resulting in the mean being skewed to the right so this is the preferred average of the managers as it brings a much larger pay rise CEO mode considering there are many more staff than managers or CEOs the CEO prefers the mode because it results in the staff salary being the basis of a pay rise resulting in paying out a lot less than is fair Note the median is the most fair when calculating a pay rise in this example scenario Ranges Averages help identify the center of our data and via various perspectives as weve already seen mean median and mode But this isnt useful when it comes to understanding how the data itself varies Ranges help explain how data is distributed and informs you of how far apart your highest and lowest values are In other words the range is a way of measuring how spread out a set of values are almost like were measuring the width of the dataset To calculate the range of a dataset you require the upper bound and the lower bound The upper bound is the highest value and similarly the lower bound is the lowest value The range calculation is as follows upper bound lower bound range But you can be in a situation where two separate datasets have different distribution of data and yet they have the same range value This is because the range only indicates the width of the data and not how its dispersed inbetween the higherlower bounds Note the range value is very sensitive to outliers and so it can be misleading if used to identify data distribution Consider this example dataset 1 1 1 2 2 2 2 3 3 3 3 3 4 4 4 4 5 5 5 On a bar chart this data could be mapped like so where the frequencies are set on the y axis and the numerical values are set on the x axis The range for the above data would be 4 5 1 Now append an outlier value 1 1 1 2 2 2 2 3 3 3 3 3 4 4 4 4 5 5 5 10 The range for the above data would now become 9 10 1 even though there are no new values inbetween The outlier has distorted the range Quartiles In order to avoid outliers we should focus on the central tendency of the data In order to do that we need to split our data into quarters The first and last quarters which typically contain outliers can subsequently be discarded allowing us to focus on the central quarters Given the earlier data which included an outlier on the far right 1 1 1 2 2 2 2 3 3 3 3 3 4 4 4 4 5 5 5 10 This would be split into quarters like so 1 1 1 2 2 2 2 3 3 3 3 3 4 4 4 4 5 5 5 10 The location within the data where splits have been inserted are known as quartiles quartile quartile quartile The lowest quartile is called the lower quartile or Q1 while the highest quartile is called the upper quartile or Q3 The middle quartile is called the median or Q2 effectively splitting the data in half Here are those splits but using the relevant names applied Q1 Q2 and Q3 Q1 Q2 Q3 The range of the values in the two median quarters inbetween Q1Q3 are referred to as the interquartile rangehttpsenwikipediaorgwikiInterquartilerange and provides a way to measure how values are dispersed while being less sensitive to the outliers that would have been found in the lowerupper quartiles Below is a very rough outline of where the quartiles would be found using our example dataset each quartile is highlighted in blue We can see that the interquartile range the section inside of the outer blue lines chops off the outlier that is pulling skewing the data to the right Locating the lowerhigher quartiles Consider the following sorted data 4 7 1 0 3 8 9 2 5 10 6 Note there are 11 values in the above data The calculation for finding the lower quartile is as follows N 4 roundresult So this would be 11 4 which gives us the result of 275 that we then round up to 3 Meaning the lower quartile is the third value in the dataset so that would be the value 1 The calculation for finding the higher quartile is as follows lower quartile x N 4 roundresult So this would be 3 x 275 which gives us the result of 825 that we then round up to 9 Meaning the higher quartile is the ninth value in the dataset so that would be the value 5 Meaning the interquartile range can be found inbetween the lowerupper quartile positions thus giving us the central 50 of the data to focus on Percentiles When dealing with quartiles see above youre splitting your data into quarters When dealing with percentiles youre splitting your data into percentages Each percentage is a percentile For example the 20th percentile is the value that is found 20 into your data Note quartiles are a type of percentile For example the lower quartile is the 25th percentile Whereas the upper quarter is the 75th percentile The median or Q2 is the 50th percentile Percentiles are generally used as a means for identifing values for a specific percentage of your users For example if your dataset reported performance numbers then you could look at the value at the 95th percentile of the data and say 95 of our users are seeing this measure of performance Which is more relevant than looking at the average ie meanmedianmode of that performance data Another typical example given for understanding the purpose of percentiles is to imagine you scored 50 in a test and the value in the test results dataset at the 90th percentile was 50 then this would mean you had scored the same as or beat the score of 90 of your class who took the test Calculating Percentiles In order to calculate percentiles of a dataset you first need to sort the data in ascending order Once this is done you need to identify N which is the number of values you have in your dataset From there you can do the following calculation where K is the percentile you want the value for K x N 100 If the result is not an integer then round it up and that will give you the position of your percentile Otherwise if it is an integer then the value can be found inbetween the integer position and the next position along so take the average of the two numbers at those positions to find the percentile value For example if you have 125 numbers in a dataset and you wished to see the 10th percentile for those values then the calculation would be 10 x 125 100 roundup125 13 So the 10th percentile would be the value at position 13 in your dataset Variance Measuring the spread of data isnt always as useful as being able to measure the consistency of that spread in that if our data represented player scores in a game then we might want to identify how consistent or reliable a player was Did they perform better and more reliably than another player The way to measure this is by looking at the mean and then looking at the distance of other values from the mean This will give us the variation were looking for The closer values are to the mean the more consistent they are So going back to the idea of a dataset that reports game player performance Imagine we have two players A and B Both have a mean value that is the same but player A has a closer spread of values than player B This means player A is more consistent with their scoring The approach for measuring the spread of data that we will look at is called the variance and it specifically measures not from the mean but the mean squared The reason for deciding to measure the spread of values from the mean squared is because maths read Head First Statisticshttpshoporeillycomproduct9780596527587do for the gory details To be a little more specific its because if we dont calculate things from the mean squared it can result in an incorrect value such as zero The variance calculation is xn That might look a little hairy but most of these symbols weve already seen this is the dataset mean which was shorthand for xn represents the sum of a datasets values x is the value for a particular dataset item n is the number of items in the dataset x thus means sum all values in the dataset xn thus means divide the total sum of numbers in the dataset by the number of items in the dataset x thus means we square the result of x and sum all the resulting values For example imagine we have the following data 1 2 9 The mean average for this data is 4 The distance between 1 and 4 is 3 eg 4 is three numbers away from 1 so count from 1 three numbers away and youll reach 4 like so 2 3 4 The distance between 2 and 4 is 2 and the distance between 9 and 4 is 5 Hence the resulting distances we get from the mean are 3 2 5 Theres more to do but first lets clarify the negativepostive aspect of these numbers ie if youre confused about why the distance between 9 and the mean is 5 and not 5 then read on You need to realise when measuring the distance to the mean that youre actually counting either negative or positive in direction this becomes clearer where you look at numbers visually 1 2 3 4 5 6 7 8 9 Note Ive put a square bracket around the number 4 to indicate that its the mean average If we look at the number 1 and count the distance to 4 were effectively counting forward ie positive towards 4 similarly with the number 2 But with 9 we have to count backwards ie negative towards 4 For example counting 87654 shows that 9 is five numbers away from 4 But as were treating this as a negative distance the value is 5 Whereas when calculating the distance from either 1 or 2 to the mean the mean 4 is ahead of it and so you count positive not negative OK so we now have the distances 3 2 5 from here we need to square these numbers and then add them up we have to square the numbers otherwise the sum result would be zero Finally we divide by N the number of items in the dataset This ultimately gives us the variance of 1267 Note heres a shorter calculation for the variance xn The reason the variance is useful is because it has provided the measure of distance from the mean based on every value in the dataset The downside to this approach is that youve calculated the variance from the mean squared of the dataset and not just the mean In order to have a more relative variance number ie our variance result is based on the mean squared but in practice well be dealing mainly with a normal mean value and not a squared mean value well want to use something called standard deviation Standard Deviation All standard deviation means is take the variance result and find the square root If we calculate the standard deviation of the variance value 1267 well find the result is 356 and so that value is the actual distance most values are away from the mean You would then compare this standard devication to a standard deviation of another players results to see which player ultimately performed better overall ie if the standard deviation for player A was smaller than player B then player A is a more consistent player Note standard deviation has its own Greek symbol referred to as the lowercase Sigma Remember to calculate you start by calculating the variance and then take the square root Some other realworld examples would be a company that manufactures machine parts For them they want the standard deviation for their data to be small so they can be sure all the parts they build are the same size Whereas if you were inspecting wages across a large organisation you would likely find that the standard deviation naturally becomes quite large Standard deviation is also measured in the same units as your data So if your dataset values are for example centimeters and the standard deviation is 1 this means that the values are typically 1 centimeter away from the mean Conclusion So there we have it a run through of some statistic basics Weve covered a few different graph types pie bar histograms line and explained what averages are and how theyre calculated in different scenarios Weve also looked at how data is distributed and how you might measure that to identify whether some results are more consistent than others Statistics is such a large topic area and as far as I can tell this is really only scratching the surface To be honest Im not sure how much further I myself will go with my learning I feel I now have enough understanding to help me get by in my work and know enough to be dangerous in conversation Otherwise if theres anything I discover is missing in future Ill be sure to return and update this post accordingly Thanks for reading Hopefully there was something useful in here for you var chartColors red rgb255 99 132 orange rgb255 159 64 yellow rgb255 205 86 green rgb75 192 192 blue rgb54 162 235 purple rgb153 102 255 grey rgb201 203 207 function alpharizergb Usage alpharizechartColorsred var result d23 d23 d23iexecrgb var colors if result colorsr parseIntresult1 16 colorsg parseIntresult2 16 colorsb parseIntresult3 16 return rgba colorsr colorsg colorsb 05 return rgba201 203 207 05 grey var ctxLine documentgetElementByIdmisleadingProfitsLinegetContext2d var myLineChart new ChartctxLine typeline data labelsJanuaryFebruaryMarchAprilMayJune datasets labelMisleading Company Profits data22122212324 fillfalse borderColor chartColorsred lineTension01 optionsresponsive true var ctxLine documentgetElementByIdprofitsLinegetContext2d var myLineChart new ChartctxLine typeline data labelsJanuaryFebruaryMarchAprilMayJune datasets labelAccurate Company Profits data22122212324 fillfalse borderColor chartColorsblue label data 0 optionsresponsive true var ctxLine documentgetElementByIdsimplePiegetContext2d var myLineChart new ChartctxLine typepie data labelsGo Python Bash datasets labelWorkload by Programming Language data15020050 backgroundColor chartColorsred chartColorsyellow chartColorsgreen optionsresponsive true var ctxBar documentgetElementByIdsimpleBargetContext2d var myBarChart new ChartctxBar type horizontalBar data labels Go Python Bash datasets label Programming Languages backgroundColor chartColorsred borderColor chartColorsred borderWidth 1 data 15020050 options elements rectangle borderWidth 2 responsive true title display true text var ctxBar documentgetElementByIdsimpleBarVertgetContext2d var myBarChart new ChartctxBar type bar data labels Go Python Bash datasets label Programming Languages backgroundColor chartColorsred borderColor chartColorsred borderWidth 1 data 15020050 options elements rectangle borderWidth 2 responsive true title display true text var ctxBar documentgetElementByIdstackedBargetContext2d var myBarChart new ChartctxBar type horizontalBar data labels Go Python Bash datasets label Like backgroundColor chartColorsgreen borderColor chartColorsgreen borderWidth 1 data 29015050 label Dislike backgroundColor chartColorsred borderColor chartColorsred borderWidth 1 data 10100250 options elements rectangle borderWidth 2 responsive true title display true text scales xAxes stacked true yAxes stacked true var ctxBar documentgetElementByIdsplitBargetContext2d var myBarChart new ChartctxBar type bar data labels Go Python Bash datasets label Like stack Stack 1 backgroundColor chartColorsgreen borderColor chartColorsgreen borderWidth 1 data 29015050 label Dislike stack Stack 2 backgroundColor chartColorsred borderColor chartColorsred borderWidth 1 data 10100250 options elements rectangle borderWidth 2 responsive true title display true text var ctxLine documentgetElementByIdsimpleLinegetContext2d var myLineChart new ChartctxLine typeline data labels0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 datasets labelCumulative Frequencies of Hours Played datanull5nullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnullnull7null17 fillfalse borderColor chartColorsred lineTension01 options spanGaps true responsive true scales xAxes scaleLabel display true labelString Hours yAxes scaleLabel display true labelString Cumulative Frequencies var ctxLine documentgetElementByIdmeanOutliersgetContext2d var myLineChart new ChartctxLine typeline data labels19 20 21 145 147 datasets labelAges for a group of people data3 6 3 1 1 fillfalse borderColor chartColorsred lineTension01 options responsive true scales xAxes scaleLabel display true labelString Ages yAxes scaleLabel display true labelString Frequencies var ctxBar documentgetElementByIdrangegetContext2d var myBarChart new ChartctxBar type bar data labels 0 1 2 3 4 5 6 datasets label Example of a symmetricalbalanced range backgroundColor chartColorsred borderColor chartColorsred borderWidth 1 data 0 3 4 5 4 3 0 options elements rectangle borderWidth 2 responsive true var ctxBar documentgetElementByIdrangeOutliergetContext2d var myBarChart new ChartctxBar type bar data labels 0 1 2 3 4 5 6 7 8 9 10 datasets label Example of an unbalanced range backgroundColor chartColorsred borderColor chartColorsred borderWidth 1 data 0 3 4 5 4 3 0 0 0 0 1 options elements rectangle borderWidth 2 responsive true var ctxQuartile documentgetElementByIdquartilegetContext2d var myQuartileChart new ChartctxQuartile typeline data labels1 1 1 2 2 2 2 3 3 3 3 3 4 4 4 4 5 5 5 6 7 8 9 10 11 datasets y axis data the frequencies label Example of distribution with quartile markers data3 3 3 4 4 4 4 5 5 5 5 5 4 4 4 4 3 3 3 0 0 0 0 1 fillfalse borderColor chartColorsred lineTension01 label data6 added for some breathing space options responsive true scales xAxes scaleLabel display true labelString Data Values gridLines color null null null null null blue null null null null blue null null null null blue lineWidth 5 borderDash 1 yAxes scaleLabel display true labelString Frequencies "},{"title":"Syscalls and C","tags":["syscall","C"],"href":"/posts/syscalls-and-c","content":" Syscalls and C This post is aimed at explaining the difference between a system call provided by the Linux kernal and a wrapper function that has a similar name within one of the C standard libraries I have no formal Computer Science CS background I started programming in 1999 and only from 2016 am I starting to learn the C programming language in order to help give me a deeper knowledge of how nix systems work and other core CS concepts See my previous post called Bits and Bytespostsbitsandbytes to see where these learnings are taking me Whats the issue It can be confusing sometimes knowing where to look for documentation when dealing with C that is if youre not a systems engineer and have no CS degree nor learnt C As an example you might learn about the strace command and start investigating what your Ruby application is up to In doing so youll see lots of calls to different functions and you might decide you want to look up the documentation for those functions This could be where your first problem arises You might think Ruby is written in C so Ill look at the C documentation and then come up with nothing So whats going on The key is to remember that the Linux operating system which your code is very likely running on is itself written in C But Linux provides its own set of functions written in C that arent part of the C language So you might see a function used and wonder why its not showing up within the C language documentation Thats because its not part of the C language The Linux engineers wouldve created the function within Linux so you need to look at the Linux documentation to find out what it does eg httpslinuxdienetman Some of these Linux provided functions are known as system calls If you visit the above linkhttpslinuxdienetman youll see there in section 2 a list of all system calls An alternative searchable list of syscalls can be found here httpsfilippoiolinuxsyscalltable Wrapper functions Now what makes this a little more confusing is that the system calls arent usually directly accessible So section 2 of the Linux documentation may list all the system call documentation but section 3 lists all library functions including what are referred to as thin wrapper functions for the system calls For example Linux uses a separate library that provides a fork function which is a wrapper around the actual system call fork equivalent provided by Linux itself The wrapper function is then also something other applications written in C can utilise This is noted here httpslinuxdienetman2intro in the documentation A system call is an entry point into the Linux kernel Usually system calls are not invoked directly instead most system calls have corresponding C library wrapper functions which perform the steps required in order to invoke the system call Thus making a system call look the same as invoking a normal library function So what do these thin wrapper functions do Well the docs tell us Often the glibc wrapper function is quite thin doing little work other than copying arguments to the right registers before invoking the system call and then setting errno appropriately after the system call has returned Note system calls indicate a failure by returning a negative error number to the caller when this happens the wrapper function negates the returned error number to make it positive copies it to errno and returns 1 to the caller of the wrapper Sometimes however the wrapper function does some extra work before invoking the system call For example nowadays there are two related system calls truncate and truncate64 and the glibc truncate wrapper function checks which of those system calls are provided by the kernel and determines which should be employed Using fork as an example Here is the system call docs httpslinuxdienetman2fork Here is the wrapper docs httpslinuxdienetman3fork But where do some of the wrapper equivalents come from Well one such provider is glibc which is GNUs standard C libraryhttpsenwikipediaorgwikiGNUCLibrary Which states The C language provides no builtin facilities for performing such common operations as inputoutput memory management string manipulation and the like Instead these facilities are defined in a standard library which you compile and link with your programs The GNU C Library described in this document defines all of the library functions that are specified by the ISO C standard as well as additional features specific to POSIX and other derivatives of the Unix operating system and extensions specific to GNU systems Heres a link also to the standard C libraryhttpsenwikipediaorgwikiCstandardlibrary libc if youre interested Direct system call What if one of the additional C libraries libc glibc etc dont provide a wrapper Well in these situations you can make a direct system call See httpslinuxdienetman2syscall which states syscall is a small library function that invokes the system call whose assembly language interface has the specified number with the specified arguments Employing syscall is useful for example when invoking a system call that has no wrapper function in the C library "},{"title":"Terminal Debugging Utilities","tags":["shell","terminal","bash"],"href":"/posts/terminal-debugging-utilities","content":" Introduction1 Prerequisites2 TCP and HTTP21 OSI Model22 Utilities3 top4 check running processes for cpu and memory utilisation ps5 see what processes are running strace6 monitor interactions between processes lsof7 list of open files netstat8 monitoring network traffic ifconfig9 configure or review your network interfaces iftop10 monitors network traffic and displays table of bandwidth usage iptraf11 monitoring network traffic more visual than netstat not as detailed tcpdump12 network packet sniffer wireshark13 network packet sniffer and analyser gui tshark14 network packet sniffer and analyser telnet15 utility for communicating with another host Honorable mentions16 Conclusion17 Introduction Not all programmers need to get their hands dirty and have to dig deep into what exactly their applications or services are doing at a lowernetwork level This is why programmers work with programming languages as they provide a nice highlevel abstraction layer that protects us from a lot of these concerns But regardless if youre a clientside developer or a serverside developer at some point your application will start misbehaving and it can be useful to have experience using commandlineterminal based tools to help you debug whats going on That is what I want to briefly cover today a few select programs that you may find useful to have in your debugging toolbox I wont claim to be an expert with any of these but Ive had to use these tools at some point or another so I at least know what I have available to me whenever things start going haywire Id encourage you to do followup reading once youve gone through this post and to experiment with these tools if there are any in particular that you find interesting Agony of Choice You are going to find that some of the tools I mention have a lot of crossover behaviour between them One of the main points of confusion for people when given the choice of lots of different utility tools is whenwhy should I use this over another very similar tool The answer it depends as with everything in life Sometimes it can just be a personal preference Youre happy using tool x which can sort of do that thing you need it to but maybe tool x isnt quite as good at showing you the thing as tool y which was designed specifically to solve the thing problem but hey tool x is good enough at it and it also allows me to inspect y and z problems which are things I look at the majority of the time Just be aware that that there will be crossover functionality and that on occasions certain tools might only have slight additions that could be useful to you depending on the problem youre trying to debug Deprecated Commands It was brought to my attention by Aidy Lewishttpstwittercomaidylewis that tools such as ifconfig and netstat have since been deprecated in favour of other tools See herehttpsdougvitalewordpresscom20111221deprecatedlinuxnetworkingcommandsandtheirreplacements for the details of what the replacement tools are Prerequisites TCP and HTTP The fact is in order to use tools such as tcpdump telnet or netstat you do need to understand the basics of how the TCP and HTTP protocols work in order to utilise these programs fully As far as other tools are concerned you may need to understand some networking basics I mean I know very little but I know just enough to muddle along when I need to Ill be covering how Ive used these tools but not really much more beyond that so you may need to do some additional reading in order to appreciate what these tools offer outside of my own experience with them OSI Model Finally before we get going its worth taking a moment to consider the OSI ModelhttpsenwikipediaorgwikiOSImodel What this model represents are the different layers of a system From the very real hardware level eg physical cables that make the interwebs work right up to the software level Here is a table matrix that attempts to identify these layers OSI Model Layer Protocol data unit PDU Function Examples Host layers 7nbspApplication Data Highlevel APIs including resource sharing remote file access HTTP NFS FTP Telnet SMTP SSH 6nbspPresentation Translation of data between a networking service and an application including character encoding data compression and encryptiondecryption SMIME TLS 5 Session Managing communication sessions ie continuous exchange of information in the form of multiple backandforth transmissions between two nodes RPC SCP PAP 4 Transport Segment TCP Datagram UDP Reliable transmission of data segments between points on a network including segmentation acknowledgement and multiplexing TCP UDP NBF Media layers 3 Network Packet Structuring and managing a multinode network including addressing routing and traffic control IPv4 IPv6 ICMP IPsec CLNP DDP 2 Data link Frame Reliable transmission of data frames between two nodes connected by a physical layer IEEE 8022 L2TP LLDP IEEE 802 MAC layers Ethernet IEEE 80211 etc PPP ATM MPLS 1 Physical Bit Transmission and reception of raw bit streams over a physical medium DOCSIS DSL IEEE 802 physical layers Ethernet IEEE 80211 etc ISDN RS232 Layer Protocols Description 7 Application HTTP FTP SMTP Window for user app processes 6 Presentation JPEG GIF MPEG Format the data to be presented to the Application layer network translator 5 Session RPC SQL NFS Allow session establishment between processes running on different stations 4 Transport TCP UDP SPX Flow control ensures all messages are delivered errorfree in sequence no losses or duplications 3 Network IP IPX ICMP Routers control operation of subnet deciding physical path data takes 2 Data Link PPPSLIP Provides errorfree transfer of data over physical layer 1 Physical Hub Physical structure cables hubs etc Note youll find many differing versions of the OSI Model ie the layers described are always the same but you may see more or less protocols defined depending on what version you look at this is just one such version copied vertabim from Wikipedia The reason this is useful is because you can identify which layer the relevant tools are operating at Tools like netstat operate at layer four transport tcp whereas telnet operates at layer seven application it actually has its own protocol telnet When debugging an issue if you know the problem space is a particular layer of the OSI model then youll have an easier time identifying which tool is best suited to the investigation Utilities There are many different utilities some provided as builtins to your OS others might be GNUhttpswwwgnuorghomeenhtml flavoured or home grown eg they can be built using a myriad of programming languages and which you have to download separately The ones listed below are a selection of tools I find particularly useful for different scenarios But theyre not all available on the Mac OS which is what I use and which Im making a massive assumption you are likely using too I do provide basic installation instructions for some of the tools that arent available for the Mac OS either natively or at all and for those tools that arent available for your OS I would recommend using Dockerhttpswwwdockercom for testing them out Id suggest for the purpose of this article to try one of the following if you dont wantuse Mac OS docker run it centos binbash or docker run it ubuntu binbash Also the usage between Mac OS and Linux can vary Example top n on Mac shows only n number of items Linux runs n number of ticks before stopping If you notice something different then itll likely be the OS But ultimately the examples I give are for Mac OS unless stated otherwise With all that out of the way lets begin top Summary displays running processes with cpu and memory utilisation The top command displays processor activity and also displays tasks managed by the kernel in realtime If you have a cluster of nodes that are setup to scale up based on either CPU or Memory usage then your first starting point will be to jump onto a running instance and check the different processes running and what their consumption is Or maybe an application on your laptop is running very slowly You can inspect its CPU consumption to see if its doing something odd and maxing out at 99 Here are some basic commands top user filter processes by those run by specified user eg top user who awk NR1 print 1 top n 10 stops command running after 10 intervals otherwise runs forever or until SIGINT interrupt signal received You can change which column controls the order display the default being CPU by typing o and then typing the column name For example command order by the COMMAND column ascending order command order by the COMMAND column descending order is implied so no need to type it The following key strokes can be executed whilst top is running z toggles onoff ability to have currently running processes highlighted NonMac On the nonMac version of top you may find that you need the output to show you the complete path to the running program which can be handy if you dont recognise the program and want to know where its located c toggles onoff absolute path for the COMMAND Similarly for nonMac versions you can dynamically change the refresh rate d change interval for screendata refresh Finally one other useful thing for nonMac versions is the ability to kill a process you dont like the look of k followed by PID of process you want to kill 1 toggles a breakdown of the CPU usage rather than showing aggregated value I mention the above nonMac options because when debugging on a remote server chances are youll need some of those details more Failing all that you could just install the man pages and double check the available options or run a Docker container and double check them aptget update aptget install man There are also some pretty fancy alternatives such as brew install htophttphishamhmhtop npm install g vtophttpsgithubcomMrRiovtop node ps Summary displays snapshot of running processes The ps command is useful for seeing at a glance what process ids have been provided to different programs and which user started them as well as the relationship between processes Majority of the time youll use ps just to identify the process id pid so you can then utilise another tool for inspecting the relevant process The differences between ps and top are subtle both display details about active processes albeit in different formats But top is continuous whereas ps is just a snapshop The ps command also offers some more advanced display options not just a table matrix and these can help visualise the parentchildren relationship for particular processes Here are some useful commands you can try ps aux shows all running processes even those without a tty or are not owned by you ps axjf shows parent process ppids nested children pids with tree formatting Note the f option doesnt work the same on Mac OS man ps for details strace Summary monitors interactions between processes by highlighting what syscalls are being made So strace is awesome for understanding exactly what your application is doing If you have just for example a Python or Ruby app then you should know that all the function calls it makes are actually abstractions provided by the language Most highlevel languages are written in C and so those abstracted functions will end up calling some internal C functions and those C functions will end up making OS level system calls Its these system calls that youll end up tracingtracking with strace This utility isnt available on Mac OS so youll need Linux With Ubuntu aptget update aptget install strace With CentOS yum install strace Mac OS has dtrace but it does use quite different commands from strace Although I have seen articles online that help translate You can run strace against a new process like so strace ls l But you can also attach strace to an already running process This is great for debugging an application thats misbehaving out in the wild You would do that like so strace p So lets consider a simple example where we want to see what the shell command ls l is actually doing First lets trace it to get an idea of the output youll see strace ls l execvebinls ls l 23 vars 0 brk0 0x109a000 accessetcldsonohwcap FOK 1 ENOENT No such file or directory mmapNULL 8192 PROTREADPROTWRITE MAPPRIVATEMAPANONYMOUS 1 0 0x7f22f946a000 accessetcldsopreload ROK 1 ENOENT No such file or directory openetcldsocache ORDONLYOCLOEXEC 3 fstat3 stmodeSIFREG0644 stsize32096 0 mmapNULL 32096 PROTREAD MAPPRIVATE 3 0 0x7f22f9462000 close3 0 accessetcldsonohwcap FOK 1 ENOENT No such file or directory openlibx8664linuxgnulibselinuxso1 ORDONLYOCLOEXEC 3 read3 177ELF21100000000030010000000000 832 832 fstat3 stmodeSIFREG0644 stsize134296 0 mmapNULL 2238192 PROTREADPROTEXEC MAPPRIVATEMAPDENYWRITE 3 0 0x7f22f9027000 mprotect0x7f22f9047000 2093056 PROTNONE 0 mmap0x7f22f9246000 8192 PROTREADPROTWRITE MAPPRIVATEMAPFIXEDMAPDENYWRITE 3 0x1f000 0x7f22f9246000 mmap0x7f22f9248000 5872 PROTREADPROTWRITE MAPPRIVATEMAPFIXEDMAPANONYMOUS 1 0 0x7f22f9248000 close3 0 accessetcldsonohwcap FOK 1 ENOENT No such file or directory openlibx8664linuxgnulibaclso1 ORDONLYOCLOEXEC 3 read3 177ELF211000000000300100034000000 832 832 fstat3 stmodeSIFREG0644 stsize31168 0 mmapNULL 2126336 PROTREADPROTEXEC MAPPRIVATEMAPDENYWRITE 3 0 0x7f22f8e1f000 mprotect0x7f22f8e26000 2093056 PROTNONE 0 mmap0x7f22f9025000 8192 PROTREADPROTWRITE MAPPRIVATEMAPFIXEDMAPDENYWRITE 3 0x6000 0x7f22f9025000 close3 0 accessetcldsonohwcap FOK 1 ENOENT No such file or directory openlibx8664linuxgnulibcso6 ORDONLYOCLOEXEC 3 read3 177ELF2110000000003001000P 200000 832 832 fstat3 stmodeSIFREG0755 stsize1840928 0 mmapNULL 4096 PROTREADPROTWRITE MAPPRIVATEMAPANONYMOUS 1 0 0x7f22f9461000 mmapNULL 3949248 PROTREADPROTEXEC MAPPRIVATEMAPDENYWRITE 3 0 0x7f22f8a5a000 mprotect0x7f22f8c14000 2097152 PROTNONE 0 mmap0x7f22f8e14000 24576 PROTREADPROTWRITE MAPPRIVATEMAPFIXEDMAPDENYWRITE 3 0x1ba000 0x7f22f8e14000 mmap0x7f22f8e1a000 17088 PROTREADPROTWRITE MAPPRIVATEMAPFIXEDMAPANONYMOUS 1 0 0x7f22f8e1a000 close3 0 accessetcldsonohwcap FOK 1 ENOENT No such file or directory openlibx8664linuxgnulibpcreso3 ORDONLYOCLOEXEC 3 read3 177ELF211000000000300100026027000000 832 832 fstat3 stmodeSIFREG0644 stsize252032 0 mmapNULL 2347200 PROTREADPROTEXEC MAPPRIVATEMAPDENYWRITE 3 0 0x7f22f881c000 mprotect0x7f22f8859000 2093056 PROTNONE 0 mmap0x7f22f8a58000 8192 PROTREADPROTWRITE MAPPRIVATEMAPFIXEDMAPDENYWRITE 3 0x3c000 0x7f22f8a58000 close3 0 accessetcldsonohwcap FOK 1 ENOENT No such file or directory openlibx8664linuxgnulibdlso2 ORDONLYOCLOEXEC 3 read3 177ELF211000000000300100032016000000 832 832 fstat3 stmodeSIFREG0644 stsize14664 0 mmapNULL 2109744 PROTREADPROTEXEC MAPPRIVATEMAPDENYWRITE 3 0 0x7f22f8618000 mprotect0x7f22f861b000 2093056 PROTNONE 0 mmap0x7f22f881a000 8192 PROTREADPROTWRITE MAPPRIVATEMAPFIXEDMAPDENYWRITE 3 0x2000 0x7f22f881a000 close3 0 accessetcldsonohwcap FOK 1 ENOENT No such file or directory openlibx8664linuxgnulibattrso1 ORDONLYOCLOEXEC 3 read3 177ELF211000000000300100030020000000 832 832 fstat3 stmodeSIFREG0644 stsize18624 0 mmapNULL 4096 PROTREADPROTWRITE MAPPRIVATEMAPANONYMOUS 1 0 0x7f22f9460000 mmapNULL 2113760 PROTREADPROTEXEC MAPPRIVATEMAPDENYWRITE 3 0 0x7f22f8413000 mprotect0x7f22f8417000 2093056 PROTNONE 0 mmap0x7f22f8616000 8192 PROTREADPROTWRITE MAPPRIVATEMAPFIXEDMAPDENYWRITE 3 0x3000 0x7f22f8616000 close3 0 mmapNULL 4096 PROTREADPROTWRITE MAPPRIVATEMAPANONYMOUS 1 0 0x7f22f945f000 mmapNULL 8192 PROTREADPROTWRITE MAPPRIVATEMAPANONYMOUS 1 0 0x7f22f945d000 archprctlARCHSETFS 0x7f22f945d840 0 mprotect0x7f22f8e14000 16384 PROTREAD 0 mprotect0x7f22f8616000 4096 PROTREAD 0 mprotect0x7f22f881a000 4096 PROTREAD 0 mprotect0x7f22f8a58000 4096 PROTREAD 0 mprotect0x7f22f9025000 4096 PROTREAD 0 mprotect0x7f22f9246000 4096 PROTREAD 0 mprotect0x619000 4096 PROTREAD 0 mprotect0x7f22f946c000 4096 PROTREAD 0 munmap0x7f22f9462000 32096 0 statfssysfsselinux 0x7ffca795ea30 1 ENOENT No such file or directory statfsselinux 0x7ffca795ea30 1 ENOENT No such file or directory brk0 0x109a000 brk0x10bb000 0x10bb000 openprocfilesystems ORDONLY 3 fstat3 stmodeSIFREG0444 stsize0 0 mmapNULL 4096 PROTREADPROTWRITE MAPPRIVATEMAPANONYMOUS 1 0 0x7f22f9469000 read3 nodevtsysfsnnodevtrootfsnnodevtr 1024 384 read3 1024 0 close3 0 munmap0x7f22f9469000 4096 0 openusrliblocalelocalearchive ORDONLYOCLOEXEC 3 fstat3 stmodeSIFREG0644 stsize1607664 0 mmapNULL 1607664 PROTREAD MAPPRIVATE 3 0 0x7f22f92d4000 close3 0 ioctl1 SNDCTLTMRTIMEBASE or SNDRVTIMERIOCTLNEXTDEVICE or TCGETS B38400 opost isig icanon echo 0 ioctl1 TIOCGWINSZ wsrow53 wscol172 wsxpixel1895 wsypixel1171 0 openusrsharelocalelocalealias ORDONLYOCLOEXEC 3 fstat3 stmodeSIFREG0644 stsize2570 0 mmapNULL 4096 PROTREADPROTWRITE MAPPRIVATEMAPANONYMOUS 1 0 0x7f22f9469000 read3 Locale name alias data basen 4096 2570 read3 4096 0 close3 0 munmap0x7f22f9469000 4096 0 openusrsharelocaleenUSUTF8LCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocaleenUSutf8LCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocaleenUSLCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocaleenUTF8LCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocaleenutf8LCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocaleenLCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenUSUTF8LCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenUSutf8LCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenUSLCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenUTF8LCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenutf8LCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenLCTIMEcoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrlibx8664linuxgnugconvgconvmodulescache ORDONLY 3 fstat3 stmodeSIFREG0644 stsize26258 0 mmapNULL 26258 PROTREAD MAPSHARED 3 0 0x7f22f9463000 close3 0 openatATFDCWD ORDONLYONONBLOCKODIRECTORYOCLOEXEC 3 getdents3 10 entries 32768 296 getdents3 0 entries 32768 0 close3 0 openusrsharelocaleenUSUTF8LCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocaleenUSutf8LCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocaleenUSLCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocaleenUTF8LCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocaleenutf8LCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocaleenLCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenUSUTF8LCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenUSutf8LCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenUSLCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenUTF8LCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenutf8LCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory openusrsharelocalelangpackenLCMESSAGEScoreutilsmo ORDONLY 1 ENOENT No such file or directory fstat1 stmodeSIFCHR0620 strdevmakedev136 2 0 mmapNULL 4096 PROTREADPROTWRITE MAPPRIVATEMAPANONYMOUS 1 0 0x7f22f9462000 write1 total 0n 8total 0 8 close1 0 munmap0x7f22f9462000 4096 0 close2 0 exitgroup0 exited with 0 OK so thats a lot of noise So lets see if we cant quieten it down a bit by using the count flag c strace c ls l total 0 time seconds usecscall calls errors syscall "},{"title":"Terminal Password Manager","tags":["shell","terminal","passwords"],"href":"/posts/terminal-password-manager","content":" Introduction1 Installation2 Example Usage3 Exporting Data4 Synchronisation5 Mobile and GUI Applications6 MultiFactor Authentication7 Conclusion8 Introduction Im guessing that you have an app like 1Passwordhttps1passwordcom or KeePassXhttpswwwkeepassxorg to manage your passwords and other login credentials and youre now looking for a cheaper alternative and also one that doesnt rely on a GUI although as youll see thats still possible If so then Passhttpswwwpasswordstoreorg might be something of interest to you as it offers you the ability to securely store information via the command line interface eg terminal shell and is a free solution to that particular problem But you need to be aware that its not as feature rich as far as something like 1Password is concerned For example 1Password has browser extensions and native apps that allow you to automatically pull your password from 1Password directly into the relevant login fields of the service youre visiting Pass is much simpler than that On the plus side Pass is based on standardised unix technology Specifically GPGhttpswwwgnupgorg which can give you confidence in the security mechanisms being utilised Note if you need a refresher on encryption and GPG then Ill refer you to an earlier blog post of minehttpwwwintegralistcoukpostssecuritybasics that covers the basics on this topic Installation Installation on macOS is easy with Homebrewhttpbrewsh and lets face it if youre using macOS then Homebrew has become a de facto standard brew install pass Once youve installed Pass youll probably want to install the shell autocomplete script as well This requires you sourcing it into your shell profile For me on Bash this looks like this echo source usrlocaletcbashcompletiondpasswordstore bashrc Note other distros available on the Pass websitehttpswwwpasswordstoreorgdownload At this point you need to initialize Pass pass init AB123C4D Just swap AB123C4D for your own GPG id You can find that information by executing the following GPG command to list out all your available keys gpg listkeys You should see something like the following output pub 1234AAB123C4D Your GPG id is the bit after the forward slash AB123C4D Example Usage So heres the super quick run down on how to use Pass pass displays the structure of your information pass generate Foobar 20 insert new record autogenerate password pass insert Foobar insert new record manually enter password pass insert Foobar m insert new record manually enter multiline data pass Foobar display first line of data in stdout pass c Foobar copy first line of data into clipboard pass rm Foobar remove the file bar to remove the whole directory pass rm rf Foo Once the last file in a directory is removed so is the directory So if you executed the second command generate and then executed the first command pass you would see something like the following Password Store Foo bargpg So we can see weve created an arbitrary data structure of Foo as the top level directory and inside of that a encrypted file called bargpg Where weve specified is an important point to be aware of because youre free to create any structure you like So the suggestion from Pass is to create a single file that contains the following information just as a guide YwZSNHz6ym9pI URL amazoncom Username AmazonianChickenexamplecom Secret Question 1 What is your childhood best friends most bizarre superhero fantasy Oh god Amazon its too awful to say Phone Support PIN 84719 Notice the first line is just the password which means you can easily copy it to your clipboard using the c flag eg pass c Foobar All the remaining information is considered secondary meta data Other types of data structuring you could do is to store data in subdirectories which would make it easier for copying into your clipboard For example Fooapp1password Fooapp1secretquestion Alternatively you could store the password in one file and have just a single additional metadata file like so Fooapp2 Fooapp2meta So app2 is the file containing the password and app2meta is the file that contains all the other related information such as secret questionanswer key pairs and contact numbersemails etc But to be honest that last style seems a bit pointless as having a meta file is still a manual process for copying out data unless your metadata consists of one additional secret question password which could be on the first line of that file for easy copying and then all other data is contact numbers and things like that Exporting Data If you want to automate the migration of data out of a GUI based app such as 1Password then the Pass community has you coveredhttpswwwpasswordstoreorgmigration Synchronisation I wanted to be able to backup my encrypted password store in case my laptop melted one day So the simplest solution was to move the directory passwordstore into a cloud provider space for synchronisation and then symlinking the directory into my home directory ln s YourCloudOrgpasswordstore passwordstore Yes this means that your encrypted data is now only as secure as the passphrase around your GPG private key But Im fairly confident in both my encryption key and the passphrase around it and so this is an acceptable compromise to make Security and Convenience these two always dance around each other Mobile and GUI Applications Ive no need for a desktoplaptop GUI as thats what the terminal shell is for and Im happy using that But if you check the introduction text on the Pass website youll find details on some different community built GUIs that are available There are also mobile applications which Ive yet to try out because they seem like quite a bit of faff to setup and this is the biggest downside to Pass so far and it doesnt work with synchronisation via a cloud provider Instead the Android app expects you to configure your Pass store to be a git repository something Ive not covered here But then that requires you to push the store into a publicprivate git repo Now theres no reason why I couldnt do this beause Ive already conceded that security aspect when I exposed the files by syncing them to a cloud provider a little less visible than a public git repo for some people but again if youre confident in your key encryption and its passphrase then this might work fine for you MultiFactor Authentication The purpose of multifactor authentication also known as 2FA two factor authentication is to add additional security to the process of accessing a service For example typically youll log into a service using a username and password But if your laptop becomes compromised and youve saved your login credentials for a particular service then without 2FA youve now given up access to that service and the data it holds For some services such as provided by Google you can enable 2FA What this means is that you associate with that service lets use Google as the example another device Most of the time the device is a mobile phone as that is one of the few devices that are usually safely held by the true owner of the Google account being accessed With 2FA youll be provided a token You then store this token in a 2FA application Google has its own Authenticator Android app for example Now every time you go to log into your Google account from a new machine youll be asked to consult the 2FA application which will give you a randomunique key back Youll then be expected to provide Google the 2FA key along with your username and password We can do a similar thing using Pass But instead of a mobile device as the associated device to the Google account you can associate your laptop running Pass and a desktop equivalent 2FA application remember it doesnt have to be your main laptop it could be another laptop or computer obviously Now this would normally be a bit of a concern for some people The idea being that 2FA is supposed to help when your device is compromised Hence people associate their mobile as the device for handling 2FA as its less likely to get lost or damaged If you have your laptop which has access to the service also being the associated 2FA token provider kinda defeats the point But because were using GPG and Pass in effect similar to my comments about syncing my Pass store onto a cloud provider if youre confident your generated GPG key and its passphrase are solid then you should be less concerned because your key will be near impossible to crack via automation and so if your laptop is compromised it wont matter as the 2FA application wont be able to pull the Google token from Pass in order to generate a unique key to access Google along with your username and password One way of achieving this was shared with me by Jake Championhttpstwittercomjakedchampion brew install oathtoolkit This will provide you with a oath command which will be used as your 2FA application The way it works is that when setting up 2FA on your Google account youll take the provided token and store it in Pass Now every time you need to access your account you can execute the following command and extract the Google token from Pass Which will generate the key needed to be provided to Google when logging in using your usernamepassword oathtool base32 totp pass 2FAAmazon The above snippet assumes you stored your Google token like so pass insert 2FAAmazon Thats all there is to it Note see this gisthttpsgistgithubcomNapoleonWils0n4005467 for more details on the Google 2FA setup process Conclusion Compared to the pricing of something like 1Password 3 per month forever 64 single licence not all devices either and not all features Then considering I spend the majority of my time working from a terminal shell It would seem that Pass is a good starting point But ultimately I think Im going to have to explore the git hosted option with a private repository for the mobile app setup just so that I can ensure a little less visibility into my data information structure "},{"title":"Computers 101: terminals, kernels and shells","tags":["builtins","executables","shell","terminal"],"href":"/posts/terminal-shell","content":" Kernel1 Program15 Executables5 Terminal2 Shell3 Shell Builtins4 Documentation6 Explicit Requests61 Locating programs7 Hashed Types8 List of all builtins vs executables9 Before we get started heres a diagram to give you an overview of what we will be covering in this post Kernel A computer has a kernel The kernel is responsible for managing the computers system The kernel has no user interface To interact with the kernel you use an intermediary program Program A programhttpssimplewikipediaorgwikiComputerprogram is a structured collection of instructions machine code that a computer can execute Your computer has many programs One such example would be the terminal emulator program Note see next section for explanation of what a terminal is Depending on the programming language used to create the program either the program is compiled down into binary so it can be understood by the computer or itll be interpreted by another program that then generates machine code out of the human readable program Executables Executables or executable binaries are programs More specifically an executable is a file that contains a program Note these are also often referred to as just binaries Executables are generally the result of a program being turned into something that can be executed by the computer Executables can be found in multiple locations eg look at the PATH environment variable in a terminal echo PATH usrlocalsbinusrlocalbinusrbinbinusrsbinsbin Note separate the output by and you see there are six directories Terminal A terminal is an inputoutput device Traditionally terminals would have been a real hardware device that you used to interact with a computer eg the computer would be a large box in a server room and the terminal would be a monitorkeyboard connected to the computer In modern computing we have electronic terminals The modern equivalent of a terminal is known as a terminal emulator Terminal emulators ironically or confusingly are part of the computer they would have previously been plugged into separately If you dont want to use a GUI graphical user interface to interact with your computer you can use a terminal emulator Shell A shell is a program which is accessed via a terminal emulator The terminal accepts input passes it to the shell and the shells output is sent back to the terminal to be displayed The shell accepts input as a set of commands The available commands vary depending on the shell eg different shells have different commands In order to fulfil your instructions commands are interpreted by the shell and the shell determines if it should either load another program or execute a builtin function Shell Builtins A builtin function is one that is provided by the shell If a command is provided and the shell has no corresponding builtin associated with the given command it will lookup the command via a separate list of available external executables A builtin command can affect the internal state of the shell This is why a command such as cd must be part of the shell ie a builtin because an external program cant change the current directory of the shell Other commands like echo might be and are in this case built into the shell for the sake of performance its quicker to call the builtin echo than it is to load and manage the external executable echo Documentation Most people are aware of manualspostsunderstandingmanpages eg man bash returns the documentation for the Bash shell Manuals do not cover shell builtins The exit command is a shell builtin so what happens when looking up a manual for it eg man exit returns a generic BUILTIN documentation page If you see that page then you know the command is a shell builtin Another way to tell if a command is a builtin vs an executable is to use the type command type exit exit is a shell builtin To read the documentation for a builtin you need to use the help command help exit exit exit n Exit the shell Exits the shell with a status of N If N is omitted the exit status is that of the last command executed The help command is itself a builtin hence it knows about builtins unlike man which isnt a builtin type help help is a shell builtin If we want to see the documentation for the type builtin use help type help type type type afptP name name Display information about command type For each NAME indicate how it would be interpreted if used as a command name Options a display all locations containing an executable named NAME includes aliases builtins and functions if and only if the p option is not also used f suppress shell function lookup P force a PATH search for each NAME even if it is an alias builtin or function and returns the name of the disk file that would be executed p returns either the name of the disk file that would be executed or nothing if type t NAME would not return file t output a single word which is one of alias keyword function builtin file or if NAME is an alias shell reserved word shell function shell builtin disk file or not found respectively Arguments NAME Command name to be interpreted Exit Status Returns success if all of the NAMEs are found fails if any are not found Explicit Requests When we used the type command earlier on the exit command it returned a single response exit is a shell builtin Lets try again with a different command echo type echo echo is a shell builtin But if we also apply the a flag we get more output type a echo echo is a shell builtin echo is binecho This indicates that the shell found a builtin first but that there was also an external executable called echo If you were to execute echo foo you would be calling the builtin echo command You could be explicit by executing it via the builtin command builtin echo foo foo You could also explicitly request the executable and not the builtin by using the command command command echo foo foo Locating programs To locate a program you use the which executable We know its an executable by using the type builtin to check it against type a which which is usrbinwhich If we use which to lookup the location of the echo command will it find the builtin or the external executable which echo binecho We can see it only found the external executable The which command isnt a builtin and so it has no idea of where to look for builtins Because by nature builtins are built into the shell itself Hashed Types If you open a fresh terminal screen and execute type man you would see the response man is usrbinman If you now execute the man command eg man echo and try type man again youll see man is hashed usrbinman The reason for this is because in order for the shell to locate the executable it needs to look it up from various locations These locations are defined in the PATH as we saw earlier To avoid having to do that lookup every time it caches the result in a hash table If we read the Bash manual man bash youll see the following comment Bash uses a hash table to remember the full pathnames of executable files see hash under SHELL BUILTIN COMMANDS below A full search of the directories in PATH is performed only if the command is not found in the hash table So it seems there is a hash builtin command lets take a look at that help hash hash hash lr p pathname dt name Remember or display program locations Determine and remember the full pathname of each command NAME If no arguments are given information about remembered commands is displayed Options d forget the remembered location of each NAME l display in a format that may be reused as input p pathname use PATHNAME as the full pathname of NAME r forget all remembered locations t print the remembered location of each NAME preceding each location with the corresponding NAME if multiple NAMEs are given Arguments NAME Each NAME is searched for in PATH and added to the list of remembered commands Exit Status Returns success unless NAME is not found or an invalid option is given So the documentation informs us of how we can look inside of the shells hash table by using the l flag hash l builtin hash p usrbinwhich which builtin hash p usrbinman man builtin hash p usrbinclear clear From this you can see Ive already executed the which man and clear executables hence theyre now cached Also in the Bash manual is the following comment BASHCMDS An associative array variable whose members correspond to the internal hash table of commands as maintained by the hash builtin Elements added to this array appear in the hash table however unsetting array elements currently does not cause command names to be removed from the hash table If BASHCMDS is unset it loses its special properties even if it is subsequently reset This informs us that there is another way to view the hash table contents In this case we can view the internal array the hash builtin appends to declare p BASHCMDS declare A BASHCMDSwhichusrbinwhich manusrbinman clearusrbinclear Note its not as clear to read as the hash output but this is probably more useful for interacting with programatically List of all builtins vs executables For a list of builtins you can use in the Bash shell at least enable a enable enable enable enable alias enable bg enable bind enable break enable builtin enable caller enable cd enable command enable compgen enable complete enable compopt enable continue enable declare enable dirs enable disown enable echo enable enable enable eval enable exec enable exit enable export enable false enable fc enable fg enable getopts enable hash enable help enable history enable jobs enable kill enable let enable local enable logout enable mapfile enable popd enable printf enable pushd enable pwd enable read enable readarray enable readonly enable return enable set enable shift enable shopt enable source enable suspend enable test enable times enable trap enable true enable type enable typeset enable ulimit enable umask enable unalias enable unset enable wait Note an online reference can be found herehttpswwwgnuorgsoftwarebashmanualhtmlnodeBashBuiltinshtmlBashBuiltins To list out all available executables is a little more tricky First you need to access only those directories youre interested in echo PATH tr n sort egrep usrbin bin usrbin usrlocalbin usrlocalsbin usrlocalsbin usrlocalsbin usrsbin Note tweak the regex as you see fit Then you need to list all the commands within those directories The following alias give you an idea of how you might approach doing that alias commandsdirecho PATH tr n sort egrep usrbin alias commandsfor i in commandsdir do eval ls l i done Does anyone know of a better way Id to hear about it "},{"title":"The Perfect Developer Qualities","tags":["quality","responsibility"],"href":"/posts/the-perfect-developer-qualities","content":" For me the perfect developer if there is such a person has these qualities Friendly is respected and liked by all they work with and are always approachable even in times of stress Humble has great humilty and is not driven by ego Calm doesnt get emotive within discussions including discussions that are both in their favour and those that arent Understanding appreciates that business requirements do change regularly and that there are no perfect scenarios so is able to adapt to problematic situations in the appropriate manner Agile recognises when they are potentially moving down a rabbit holetime sinkyak shave and will successfully reevaluate the situation and refocus their attention Patient appreciates that no dev is born equal and so varying softpractical skills will be encountered Experienced has a wide ranging skill set with relevant practical experience and most importantly realises the fundamentals of simple code design and recognised patterns Consistent by being consistent in their development approach they faciliate easier rotation of developers when resources are restrictedrestructured Wise knows when and where a particular technology is appropriate to use and when not appropriate Analytical is pragmatic and can break down complex problems into logical smaller tasks Articulate able to communicate clearly their ideas to people of all technical levels Open continuous documentation and visibility into design and development progress ensuring good communication Reasonable understands a problems sense of proportion whats important and what isnt eg when something is a big deal and worth pressing and when it isnt and so not prematurely raising red flags Passionate enjoys their craft always keenopen to new experiences and learning new things "},{"title":"Thread Safe Concurrency","tags":["clojure","concurrency","go","jruby","threads"],"href":"/posts/thread-safe-concurrency","content":" Introduction1 Shared Memory2 Message Passing3 Various options4 MutexesSemaphores5 Atomic operations51 STM6 Clojure example61 Quick Clojure Concurrency Detour611 JRuby example62 Actors7 Transactions and Actors71 Actors in Clojure72 Differences between Agents and Erlang Actors73 Limitations74 CSP8 Threads9 CPU vs IO91 Calculating the number of Threads92 Even workload distribution93 Conclusion10 Introduction Concurrency is a difficult concept Regardless of programming language or idiom that you use the practice of programming a threadsafe application can be harder than you think There are two fundamental models of concurrency 1 Shared Memory 2 Message Passing In the first we have concepts such as Threads Locks and Mutexes In the latter we have patterns such as Actors and CSP which rely on the mantra of dont communicate by sharing memory share memory by communicating Shared Memory The following diagram is an extremely simplistic view of how CPUs Processes and Threads interact but should help us to better understand why code can become NON threadsafe as far as the Shared Memory model is concerned well see shortly that the message passing model side steps this issue for those of you who cannot see the image effectively a process can spawn multiple threads and each thread belonging to a specific process shares the memory related to the process Meaning if your software process creates two threads then both threads have access to the same memory space and thus can manipulate the same chunk of memory and by memory Im specifically referring to data the application createshas access to This means if your software creates some mutable data eg in Ruby this could look something like foo bar a foo variable that holds the String value bar and you want to modify the value of the variable then multiple threads could manipulate the value in an unexpected order and subsequently cause a difficult to locate bug Any time you create a new Thread and within that Thread you modify a mutable piece of data you should be concerned about how threadsafe that data is Note if youre also utilising immutable data structures as found in more functional languages but also languages such as Go where they pass by value rather than pass by reference then this also makes code less prone to threadsafety concerns but thats a discussion for another day Message Passing The message passing model relies on no data being shared but rather communication between processes happening via either a message bus or by piping messages down a channel depending on which style is implemented in your programming language of choice As well as avoiding the issue of data being shared it also avoids the issue of trying to recover from failures which threadprocess is the correct one which is a hard problem to reason about Various options There are four main types of solutions to the problem of threadsafe concurrency 1 MutexesSemaphores 2 Software Transactional Memory STM 3 The Actors pattern 4 Communicating Sequential Processes CSP Lets investigate each of these in turn MutexesSemaphores Well be discussing specifically mutexes rather than semaphores they have very similar purposes in that they control access to specific data although a mutex offers some additional guarantees which we wont go into here When using a mutex you can lock a piece of data so only that specific Thread has access to the data When done manipulating the data you can unlock it thus allowing another Thread to use a mutex to lock the data so it can make its own changes Lets take a look at an example in Ruby imagine data is some shared state def update mutex Mutexnew Threadnew mutexsynchronize data 1 join end Note for the full Mutex API see httpwwwrubydocorgcore215Mutexhtmlhttpwwwrubydocorgcore215Mutexhtml This particular solution is the simplest of the three BUT it doesnt take into account any logic for handling unexpected changes to data well see what that means in the next section Atomic operations Locks and mutexes allow operations to become atomic meaning that the change happens as a whole Meaning it becomes thread safe because another thread cant accidentally read a piece of datastatememory that is half done eg in the above Ruby example would not be thread safe without the Mutex because a thread could read the value of data inbetween the read and the assignment that carries out STM Software Transactional Memory is known as being an optimistic process Compare this to the locksynchronisation mechanism of a Mutex which implies the onus of whether a write action will succeed should be on the writer ie the writer locks the data writes the data then releases the lock The STM on the other hand doesnt care what happens in another Thread it instead records all the changes in a log file and then just before it confirms the write referred to as a commit in STM terminology it verifies the data were modifying hasnt changed and if it hasnt we go ahead and commit the change made by our Thread If a change has been made to the shared data source then the transation will start over but this time using the latest copy of the data were modifying this can in some cases cause a deadlock to occur Because the STM retries transactions when they fail we should ensure that code within a transaction is idempotent and side effect free Otherwise if the code isnt idempotent then that code will be run again and might mean data is changed or recorded in ways you didnt expect eg a call to log some data within a transaction could be called multiple times Note the STM is best used for applications that have frequent reads and can allow for low to medium write collisions If you expect lots of write collisions then it may be best to opt for another pattern such as the Actors7 pattern discussed in the following section In the Clojure programming language youll also find that the STM facilitates embedded transactions which allows for greater atomicity What this means in pratice is that if there is a transaction that contains a subtransaction then in some implementations of the STM a failed transaction wont necessarily cause the subtransaction to fail But in Clojure it will Meaning that its definitely an atomic operation all or nothing Clojure example The following example uses the Clojure programming language to implement a theadsafe modification via the STM specifically the ensure function allows us to tell the STM what shared memory to watch for changes while our transaction is ongoing "},{"title":"Understanding Golang's Func Type","tags":["go","grpc","rpc"],"href":"/posts/understanding-golangs-func-type","content":" Introduction1 Four ways to skin a cat2 How does the adapter work3 Why is this interesting4 SummaryBreakdownsummarybreakdown Introduction Here is some code that demonstrates the typical hello world for a Go based web server package main import fmt nethttp func handlerw httpResponseWriter r httpRequest fmtFprintfw Hello s rURLPath1 func main httpHandleFuncWorld handler httpListenAndServe8080 nil Note httplocalhost8080World will return Hello World For most people setting up a web server to handle incoming HTTP requests is considered a quick and simple introduction to the Go programming languagehttpsgolangorg and looking at the above code its easy to see why that would be the case But further investigation can yield some nice learnings about Gos builtin func type and how it is used as an adapter layer In this blog post I will demonstrate a few different ways of creating a web server and then Ill clarify how some of the functionality specifically httpHandleFunc works What initially drove me to look into this was my curiosity as to why I would always insert nil to the httpListenAndServe function by default when setting up a basic web server see above code example It was never really that clear to me and so its just something I cargo culted and subsequently replicated every single time I needed a web server I realised I needed to know what its purpose was in order to feel like I wasnt going through the motions unnecessarily or missing out on additional functionality which it turns out I was Four ways to skin a cat There are currently four ways that I know of to create a web server with Go well actually only three the first two examples are effectively the same but we add a little more code to demonstrate different ways incoming requests can be handled Each of the variations ultimately revolve around what we send to httpListenAndServe as its second argument and this thing we send also ultimately should have a ServeHTTP method well see shortly how this is achieved in different ways So here are each of the variations 1 No request parsing serve same content regardless of request 2 Manual request parsing 3 Multiplexer 4 Global multiplexer No request parsing The most basic implementation and by basic I dont mean simplest more raw is demonstrated in the below code sample which calls ListenAndServe and passes in db as its second argument Note although I wrote this blog post back in October 2015 Ive rewritten the below examples based off inspiration from The Go Programming book Ive been reading recently This first section will give us enough background and grounding to build upon in the latter sections package main import fmt log nethttp type pounds float32 func p pounds String string return fmtSprintf2f p type database mapstringpounds func d database ServeHTTPw httpResponseWriter r httpRequest for item price range d fmtFprintfw s sn item price func main db database foo 1 bar 2 logFatalhttpListenAndServelocalhost8000 db We can see from the above code sample that db is an instance of our custom database type which states it should be a map data structure consisting of strings for keys and pounds for values We can also see that pounds is itself a type of float32 and has a custom String method attached allowing us to modify its output when converted into a string value Similarly the database type has a method attached but this time it is a ServeHTTP method The ServeHTTP is required in order to satisfy the ListenAndServe method signature which states the second argument should be a type of Handler func ListenAndServeaddr string handler Handler error Documentation godoc nethttp ListenAndServe less If we look at the source code for the Handler type below we can clearly see it requires a ServeHTTP method to be available hence why our database type associates its own ServeHTTP method type Handler interface ServeHTTPResponseWriter Request Documentation godoc nethttp Handler less The above sample web server code will always serve the same response regardless of the URL that was specified So for example httplocalhost8000 httplocalhost8000abc httplocalhost8000xyz will all serve back the response foo 100 bar 200 Manual request parsing OK so now weve got the above example written Lets enhance it by allowing our application to handle different routes as apposed to serving the same content all the time To do this well modify our ServeHTTP method to interrogate the incoming request object and parse out the URL as demonstrated in the below code sample package main import fmt log nethttp type pounds float32 func p pounds String string return fmtSprintf2f p type database mapstringpounds func d database ServeHTTPw httpResponseWriter r httpRequest switch rURLPath case foo fmtFprintfw foo sn dfoo case bar fmtFprintfw bar sn dbar default wWriteHeaderhttpStatusNotFound fmtFprintfw No page found for sn rURL func main db database foo 1 bar 2 logFatalhttpListenAndServelocalhost8000 db Nothing else to say about this other than weve implemented what we set out to do by utilising a simple switch statement that checks for known paths and writes to the httpResponseWriter a different response depending on the request If we cant match the URL then well instead send a 404 status code StatusNotFound followed by a message to notify the user we couldnt identify their request Documentation godoc src nethttp WriteHeader less Multiplexer So writing the above example demonstrates a bit of a code smell We could extract each cases block into separate functions but its still an ever growing switch statement Were also confined to using objects that implement the required interface eg if you dont provide an object that has a ServeHTTP method then youre not going to have much success Instead it would be nice if you could just pick an arbitrary function and allow it to be used as a handler Thats exactly what ServeMux provides to us via its HandleFunc function which is really just a convenience method on top of httpHandlerFunc Documentation godoc nethttp ServeMux less The following code sample demonstrates this in action by removing the ServeHTTP method from the database type and instead implementing individual methods for our defined routes to call package main import fmt log nethttp type pounds float32 func p pounds String string return fmtSprintf2f p type database mapstringpounds func d database foow httpResponseWriter r httpRequest fmtFprintfw foo sn dfoo func d database barw httpResponseWriter r httpRequest fmtFprintfw bar sn dbar func d database bazw httpResponseWriter r httpRequest fmtFprintfw baz sn dbaz func main db database foo 1 bar 2 baz 3 mux httpNewServeMux muxHandlefoo httpHandlerFuncdbfoo muxHandlebar httpHandlerFuncdbbar Convenience method for longer form muxHandle muxHandleFuncbaz dbbaz logFatalhttpListenAndServelocalhost8000 mux As we can see we create a new ServeMux instance using httpNewServeMux and then register our database methods as handlers for each of the routes we want to match them against The ServeMux instance is a multiplexer meaning we can pass it as the second argument to httpListenAndServe Note you can also see we demonstrate the shorthand muxHandleFunc which is really a convenience method over both muxHandle and httpHandlerFunc So how does httpHandlerFunc and muxHandleFunc allow us to use an arbitrary function as none of those database functions have access to a ServeHTTP function as required by ListenAndServe Well come back to the answer in a little bit Lets quickly review the last variation of how to run a web server first Global multiplexer Typically youll have your code split up into separate packages So in order to setup your routing handlers you would need to pass around your ServeMux instance to each of these packages Instead you can just utilise Gos global DefaultServeMux To do that you pass nil as the second argument to httpListenAndServe Documentation godoc src nethttp DefaultServeMux less The following code sample demonstrates this package main import fmt log nethttp type pounds float32 func p pounds String string return fmtSprintf2f p type database mapstringpounds func d database foow httpResponseWriter r httpRequest fmtFprintfw foo sn dfoo func d database barw httpResponseWriter r httpRequest fmtFprintfw bar sn dbar func d database bazw httpResponseWriter r httpRequest fmtFprintfw baz sn dbaz func main db database foo 1 bar 2 baz 3 httpHandleFuncfoo dbfoo httpHandleFuncbar dbbar httpHandleFuncbaz dbbaz logFatalhttpListenAndServelocalhost8000 nil Again we have a convenience method HandleFunc which allows an arbitrary function to be adapted so it fits the interface requirements that ListenAndServes second argument enforces How does the adapter work The adapter here being the httpHandleFunc function How does it take an arbitrary function and enable it to support the relevant interface so it can be passed to ListenAndServe The way httpHandleFunc solves this requirement is by internally calling its other function httpHandle and passing it the required type ie it passes a type that satisfies the interface requirement that the Handle function has OK lets look back at the two functions and their respective signatures to refresh our memory as to whats required func Handlepattern string handler Handler func HandleFuncpattern string handler funcResponseWriter Request We can see the Handle signature requires a type that satisfies the Handler interface which is defined as follows type Handler interface ServeHTTPResponseWriter Request In other words as long as you pass in a type that has a ServeHTTP method then the Handle function will be happy So HandleFunc facilitates this requirement by taking your user defined function and converting it into a type that happens to have ServeHTTP available So how does it do that conversion Firstly it defines a func type called httpHandlerFunc like so type HandlerFunc funcResponseWriter Request This says that for a function to match this type it should have the same signature eg ResponseWriter Request Inside the HandleFunc function youll see it actually calls this func type and passes it your user defined function This will look something like the following in the Go implementation source code func mux ServeMux HandleFuncpattern string handler funcResponseWriter Request muxHandlepattern HandlerFunchandler Notice the call of HandlerFunchandler where handler is your user defined function you passed into HandleFunc from your application code This is the conversion of your function into the HandlerFunc type Youre now effectively passing a HandlerFunc into the internal function muxHandle So how does that help us How does passing in a function that looks like a HandlerFunc type into muxHandle help us solve the problem that were still passing in a function that has no ServeHTTP method available and so should fail the interface requirement that muxHandle has Well once you convert your user defined function into a HandlerFunc youll find it now does have a ServeHTTP method available If we look at the Go source code just after the definition of the HandlerFunc func type youll also find the following snippet of code func f HandlerFunc ServeHTTPw ResponseWriter r Request fw r This associates the required ServeHTTP function with the HandlerFunc type So when you convert your function to a HandlerFunc it will indeed gain access to a ServeHTTP function Also remember that when you associate a method with a typeobject the receiver is also available to you So in this case we can see the f is actually your user defined function you passed in to be converted So when you convert that user defined function into a HandlerFunc you get the ServeHTTP method which internally is calling your original user defined function Lets now take a quick look at that muxHandle function to see what it expects func mux ServeMux Handlepattern string handler Handler As we can see it expects a type of Handler to be provided What is Handler Well remember from earlier this is an interface which states there should be a ServeHTTP function available type Handler interface ServeHTTPResponseWriter Request We know now that weve utilised Gos func type to adapttransform our incoming function into a type that has the required method ServeHTTP associated with it thus allowing it to pass the Handler interface requirement Why is this interesting Really understanding what initially looked to be a simple web server abstraction ended up being a complex mix of types and interfaces that work together to allow seemingly incompatible types to be adapted to fit Demonstrating how flexible and dynamic your code can be when working in an idiomatic way with the Go principles I now have a much better appreciation of why lots of long time Gophers will routinely recommend sifting through the official Go source code as it can indeed be quite enlightening SummaryBreakdown Here is a useful summary for you httpHandler interface you support httpHandler if you have a ServeHTTPw httpResponseWriter r httpRequest method available httpHandle eg an object with a ServeHTTP method httpHandleFunc eg a function that accepts the arguments w httpResponseWriter r httpRequest httpHandlerFunc func type used internally by httpHandleFunc eg it adapts the given function to the httpHandlerFunc type which has an associated ServeHTTP method that is able to call your original incompatible function "},{"title":"Understanding Man Pages","tags":["manpage","shell"],"href":"/posts/understanding-man-pages","content":" Introduction1 Sections2 Sub sections3 Accessing different sections4 Introduction Your operating system provides manual pages that explain what specific commands do and where they can be located on your computer Most of us are at least familiar with opening a terminal and typing man But you might be confused when you see a manual page informing you to check out 2 For example you might be shown the message at the end of a man page that says See also chmod2 If you try that it wont work man chmod2 So lets understand why that is and what it means Sections So the manual pages are separated into sections This is what the number within the parentheses represents ie chmod2 represents the chmod command from section 2 of the OS manual To find where your manual pages are stored cd usrshareman A simple ls will show that on my OS macOS I have nine manuals man1 man2 man3 man4 man5 man6 man7 man8 man9 Each manual section has an introduction page that explains what the section covers These taken from my OS are as follows man1 introduction to general commands tools and utilities man2 introduction to system calls and error numbers man3 introduction to the C libraries man4 contains documentation on special files and sockets man5 introduction to file formats man6 contains documentation about games and other miscellaneous fun programs man7 miscellaneous information pages man8 introduction to system maintenance and operation commands man9 introduction to system kernel interfaces except for manual sections 4 and 6 I had to go to the online reference for macOShttpsdeveloperapplecomlegacylibrarydocumentationDarwinReferenceManPages in order to find out what they contained If you wanted to know at a glance what commands were available then you could install the tree command and execute it at the current directory usrshareman and this would show you something like the following cut short for brevity man1 ab1 accesstool1 addftinfo1 afconvert1 afhash1 afida1 afinfo1 man2 accept2 access2 acct2 adjtime2 aiocancel2 aioerror2 aioread2 Sub sections Youll notice that most page files within each manual section have an extension that matches the section theyre contained within eg ab1 for ab command inside manual 1 or aioread2 for aioread command inside manual 2 Whats interesting is that there are some files such as httpdstatd1m which is inside manual section 1 that have a different extension d1m or asn1parse1ssl 1ssl By looking back at the online macOS manual pages referencehttpsdeveloperapplecomlegacylibrarydocumentationDarwinReferenceManPages I noticed there were additional sub sections Section 1m contains documentation for tools built on top of DTrace Section 1ssl contains documentation on tools that are part of OpenSSL Section 1tcl contains documentation on tools that are part of Tcl Section 3cc contains documentation on the Common Crypto API Section 3pcap contains documentation on the packet capture library libpcap Section 3pm contains documentation on Perl modules Section 3ssl contains documentation on OpenSSL C library routines Section 3tcl contains documentation on TclTk C library routines Section 3x contains documentation on cursesrelated C library routines Section 5ssl contains documentation on SSLspecific configuration file formats Section 7ssl miscellaneous SSL documentation section Section n contains documentation about TclTk Section ntcl contains documentation about TclTk Accessing different sections Typically when we type man the man command will search all the sections looking for the specified command It stops at the first match it finds So there could be a scenario where you search for a command and you get the user space toplevel entry command and not the system call version that actually interacts with your OS The chmod command is a good example of that scenario If you want to see the actual system call manual page for chmod you need to search inside the specific section which in this case is 2 man 2 chmod Alternatively you can search multiple specific sections man S 12 chmod The above command will search through only sections 1 and 2 but it will still stop at the first match it finds so the above command is no different in outcome than just man chmod But if you know for sure that the command youre looking for is somewhere within either the system call or C library manual pages then man S 23 would prevent a command from manual section 1 getting matched first Note to see the intro page for a section use man intro "}]